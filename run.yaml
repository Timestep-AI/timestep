apis:
  - agents
  - datasetio
  - eval
  - inference
  - memory
  - safety
  - scoring
  - telemetry
# conda_env: ollama
conda_env: tgi
datasets: []
docker_image: null
eval_tasks: []
image_name: ollama
image_name: tgi
memory_banks: []
metadata_store:
  # db_path: ${env.SQLITE_STORE_DIR:~/.llama/distributions/ollama}/registry.db
  db_path: ${env.SQLITE_STORE_DIR:~/.llama/distributions/tgi}/registry.db
  namespace: null
  type: sqlite
models:
  # - metadata: {}
  #   # model_id: ${env.INFERENCE_MODEL:llama3.2:1b}
  #   model_id: ${env.INFERENCE_MODEL:tinyllama:latest}
  #   model_type: llm
  #   provider_id: ollama
  #   provider_model_id: null
  - metadata: {}
    model_id: ${env.INFERENCE_MODEL:TinyLlama/TinyLlama-1.1B-Chat-v1.0}
    model_type: llm
    provider_id: tgi-inference
    provider_model_id: null
  # - metadata:
  #     embedding_dimension: 384
  #   model_id: all-MiniLM-L6-v2
  #   model_type: embedding
  #   provider_id: sentence-transformers
  #   provider_model_id: null
providers:
  agents:
    - config:
        persistence_store:
          type: sqlite
          namespace: null
          # db_path: ${env.SQLITE_STORE_DIR:~/.llama/distributions/ollama}/agents_store.db
          db_path: ${env.SQLITE_STORE_DIR:~/.llama/distributions/tgi}/agents_store.db
      provider_id: meta-reference
      provider_type: inline::meta-reference
  datasetio:
    - config: {}
      provider_id: huggingface
      provider_type: remote::huggingface
    - config: {}
      provider_id: localfs
      provider_type: inline::localfs
  eval:
    - config: {}
      provider_id: meta-reference
      provider_type: inline::meta-reference
  inference:
    # - config:
    #     url: ${env.OLLAMA_URL:http://localhost:11434}
    #   provider_id: ollama
    #   provider_type: remote::ollama
    - config:
        url: ${env.TGI_URL:https://localhost:3000}
      provider_id: tgi-inference
      provider_type: remote::tgi
    # - config: {}
    #   provider_id: sentence-transformers
    #   provider_type: inline::sentence-transformers
  memory:
    - config:
        kvstore:
          # db_path: ${env.SQLITE_STORE_DIR:~/.llama/distributions/ollama}/faiss_store.db
          db_path: ${env.SQLITE_STORE_DIR:~/.llama/distributions/tgi}/faiss_store.db
          namespace: null
          type: sqlite
      provider_id: faiss
      provider_type: inline::faiss
  safety:
    - config: {}
      provider_id: llama-guard
      provider_type: inline::llama-guard
  scoring:
    - config: {}
      provider_id: basic
      provider_type: inline::basic
    # - config:
    #     openai_api_key: ${env.OPENAI_API_KEY:}
    #   provider_id: braintrust
    #   provider_type: inline::braintrust
    - config: {}
      provider_id: llm-as-judge
      provider_type: inline::llm-as-judge
  telemetry:
    - config:
        service_name: ${env.OTEL_SERVICE_NAME:llama-stack}
        sinks: ${env.TELEMETRY_SINKS:console,sqlite}
        # sqlite_db_path: ${env.SQLITE_DB_PATH:~/.llama/distributions/ollama/trace_store.db}
        sqlite_db_path: ${env.SQLITE_DB_PATH:~/.llama/distributions/tgi/trace_store.db}
      provider_id: meta-reference
      provider_type: inline::meta-reference
scoring_fns: []
shields: []
version: "2"
