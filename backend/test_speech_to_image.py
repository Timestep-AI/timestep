#!/usr/bin/env python3
"""
Test script for speech-to-image transcription using VLM.
This demonstrates converting speech to spectrogram images and using VLM for transcription.
"""

import asyncio
import io
import tempfile
from pathlib import Path

import numpy as np
import soundfile as sf
from PIL import Image

from src.backend.services.speech_to_image_service import get_speech_to_image_service


async def create_test_audio():
    """Create a simple test audio file with spoken content."""
    # Generate a simple sine wave as a placeholder
    # In practice, you'd use real speech audio
    sample_rate = 22050
    duration = 3.0  # 3 seconds
    t = np.linspace(0, duration, int(sample_rate * duration), False)
    
    # Create a simple tone (this is just for demonstration)
    # Real implementation would use actual speech
    frequency = 440  # A4 note
    audio = 0.3 * np.sin(2 * np.pi * frequency * t)
    
    # Add some variation to make it more interesting
    audio += 0.1 * np.sin(2 * np.pi * 880 * t)  # Higher frequency
    
    # Create a temporary file
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
        sf.write(tmp.name, audio, sample_rate)
        return tmp.name


async def test_speech_to_image_transcription():
    """Test the speech-to-image transcription approach."""
    print("üé§ Testing Speech-to-Image Transcription with VLM")
    print("=" * 60)
    
    # Create test audio
    audio_file_path = await create_test_audio()
    print(f"‚úÖ Created test audio file: {audio_file_path}")
    
    try:
        # Initialize the service
        service = get_speech_to_image_service()
        
        # Create a mock UploadFile from the audio file
        class MockUploadFile:
            def __init__(self, file_path):
                self.file_path = file_path
                self._content = None
            
            async def read(self):
                if self._content is None:
                    with open(self.file_path, 'rb') as f:
                        self._content = f.read()
                return self._content
            
            async def seek(self, position):
                pass
        
        mock_audio_file = MockUploadFile(audio_file_path)
        
        print("\nüîÑ Testing different audio visualizations...")
        
        # Test different image types
        image_types = ["spectrogram", "waveform", "mfcc"]
        
        for image_type in image_types:
            print(f"\nüìä Testing {image_type} visualization...")
            
            # Reset file pointer
            await mock_audio_file.seek(0)
            
            try:
                # Test the transcription
                result = await service.transcribe_with_vlm(
                    mock_audio_file,
                    image_type=image_type,
                    prompt=f"Please transcribe the speech shown in this audio {image_type} image. Extract all spoken words and convert them to text."
                )
                
                print(f"‚úÖ {image_type.capitalize()} transcription result:")
                print(f"   {result}")
                
            except Exception as e:
                print(f"‚ùå {image_type.capitalize()} transcription failed: {e}")
        
        print("\nüîÑ Testing comparison with traditional STT...")
        
        # Reset file pointer
        await mock_audio_file.seek(0)
        
        try:
            comparison = await service.compare_transcription_methods(
                mock_audio_file,
                vlm_prompt="Please transcribe the speech shown in this audio spectrogram image. Extract all spoken words and convert them to text."
            )
            
            print("‚úÖ Comparison results:")
            print(f"   Traditional STT: {comparison['traditional_stt']}")
            print(f"   VLM Transcription: {comparison['vlm_transcription']}")
            print(f"   Methods compared: {comparison['methods_compared']}")
            
        except Exception as e:
            print(f"‚ùå Comparison failed: {e}")
    
    finally:
        # Clean up
        Path(audio_file_path).unlink(missing_ok=True)
        print(f"\nüßπ Cleaned up test file: {audio_file_path}")


def demonstrate_concept():
    """Demonstrate the concept and potential benefits."""
    print("\n" + "=" * 60)
    print("üéØ SPEECH-TO-IMAGE TRANSCRIPTION CONCEPT")
    print("=" * 60)
    
    print("""
This approach converts speech to image representations and uses Vision Language Models (VLM) 
for transcription instead of traditional Speech-to-Text models.

üîç HOW IT WORKS:
1. Audio file ‚Üí Spectrogram/Waveform/MFCC image
2. Image ‚Üí VLM processing with transcription prompt
3. VLM output ‚Üí Transcribed text

üéØ POTENTIAL BENEFITS:
‚Ä¢ Visual context: VLM can "see" audio patterns, noise, and characteristics
‚Ä¢ Robustness: May handle different accents, background noise, or audio quality better
‚Ä¢ Interpretability: Visual representation makes the process more transparent
‚Ä¢ Flexibility: Can use different image types (spectrogram, waveform, MFCC)
‚Ä¢ Leverage: Uses powerful vision-language models for audio processing

üî¨ DIFFERENT AUDIO VISUALIZATIONS:
‚Ä¢ Spectrogram: Shows frequency content over time (most common)
‚Ä¢ Waveform: Shows amplitude over time (simpler)
‚Ä¢ MFCC: Shows mel-frequency cepstral coefficients (speech-specific features)

‚ö†Ô∏è  LIMITATIONS:
‚Ä¢ May be slower than direct STT
‚Ä¢ Requires more computational resources
‚Ä¢ VLM models may not be optimized for audio transcription
‚Ä¢ Quality depends on the VLM model's understanding of audio visualizations

üöÄ USE CASES:
‚Ä¢ Research and experimentation
‚Ä¢ Audio quality analysis
‚Ä¢ Educational demonstrations
‚Ä¢ Alternative transcription methods
""")


if __name__ == "__main__":
    print("üöÄ Starting Speech-to-Image Transcription Test")
    
    # Demonstrate the concept
    demonstrate_concept()
    
    # Run the test
    asyncio.run(test_speech_to_image_transcription())
    
    print("\n‚úÖ Test completed!")