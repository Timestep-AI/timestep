# Speech-to-Image Transcription with VLM

This project implements an innovative approach to speech transcription by converting audio to image representations and using Vision Language Models (VLM) for transcription instead of traditional Speech-to-Text models.

## üéØ Concept Overview

Instead of directly processing audio with STT models like Whisper, this approach:

1. **Converts audio to visual representations** (spectrograms, waveforms, MFCC)
2. **Uses VLM models** to "read" the audio images
3. **Extracts text** from the VLM's understanding of the audio visualization

## üî¨ How It Works

### Audio Visualization Types

#### 1. Spectrogram
- **What**: Shows frequency content over time
- **Best for**: Most speech transcription tasks
- **Visual**: Color-coded frequency intensity over time

#### 2. Waveform
- **What**: Shows amplitude over time
- **Best for**: Simple audio analysis
- **Visual**: Wave-like pattern showing audio intensity

#### 3. MFCC (Mel-frequency Cepstral Coefficients)
- **What**: Speech-specific frequency features
- **Best for**: Speech recognition and analysis
- **Visual**: Compact representation of speech characteristics

### Processing Pipeline

```
Audio File ‚Üí Audio Visualization ‚Üí VLM Processing ‚Üí Transcribed Text
     ‚Üì              ‚Üì                    ‚Üì              ‚Üì
   Input        Spectrogram         VLM Model      Output Text
```

## üöÄ API Endpoints

### VLM Transcription
```http
POST /audio/transcriptions/vlm
```

**Parameters:**
- `file`: Audio file (multipart/form-data)
- `image_type`: "spectrogram", "waveform", or "mfcc" (default: "spectrogram")
- `vlm_model`: VLM model name (default: "HuggingFaceTB/SmolVLM2-256M-Video-Instruct")
- `prompt`: Custom prompt for VLM (default: transcription prompt)

### Compare Methods
```http
POST /audio/transcriptions/compare
```

**Parameters:**
- `file`: Audio file (multipart/form-data)
- `vlm_prompt`: Custom prompt for VLM comparison

**Returns:**
```json
{
  "traditional_stt": "Whisper transcription result",
  "vlm_transcription": "VLM transcription result",
  "methods_compared": ["whisper", "vlm_spectrogram"]
}
```

## üéØ Potential Benefits

### 1. Visual Context Understanding
- VLM can "see" audio patterns, noise, and characteristics
- May handle different accents or speech patterns better
- Visual representation provides additional context

### 2. Robustness
- Could be more robust to background noise
- May handle audio quality variations better
- Visual processing might be more interpretable

### 3. Flexibility
- Multiple visualization types (spectrogram, waveform, MFCC)
- Different VLM models can be used
- Customizable prompts for different use cases

### 4. Research Value
- Novel approach to audio processing
- Educational for understanding audio-visual relationships
- Potential for new research directions

## ‚ö†Ô∏è Limitations

### 1. Performance
- Slower than direct STT processing
- Requires more computational resources
- Additional processing steps (audio ‚Üí image ‚Üí VLM)

### 2. Model Limitations
- VLM models not specifically trained for audio transcription
- Quality depends on VLM's understanding of audio visualizations
- May not handle all audio types equally well

### 3. Accuracy
- Experimental approach with unproven accuracy
- May not match traditional STT performance
- Depends heavily on VLM model capabilities

## üõ†Ô∏è Technical Implementation

### Dependencies Added
```toml
librosa>=0.10.1    # Audio processing
matplotlib>=3.7.0   # Image generation
```

### Key Components

#### SpeechToImageService
- `_audio_to_spectrogram_image()`: Converts audio to spectrogram
- `_audio_to_waveform_image()`: Converts audio to waveform
- `_audio_to_mfcc_image()`: Converts audio to MFCC
- `transcribe_with_vlm()`: Main transcription method
- `compare_transcription_methods()`: Compare with traditional STT

#### API Integration
- New endpoints in `audio_api.py`
- Compatible with existing audio service structure
- Async processing for better performance

## üß™ Testing

### Run the Test Script
```bash
cd backend
python test_speech_to_image.py
```

### Test Different Visualizations
The test script demonstrates:
- Spectrogram transcription
- Waveform transcription
- MFCC transcription
- Comparison with traditional STT

## üî¨ Use Cases

### 1. Research and Experimentation
- Novel approach to audio processing
- Understanding audio-visual relationships
- Testing VLM capabilities on audio data

### 2. Educational Demonstrations
- Visual representation of audio
- Understanding how audio can be "seen"
- Teaching audio processing concepts

### 3. Audio Quality Analysis
- Visual inspection of audio characteristics
- Identifying audio patterns and features
- Quality assessment through visualization

### 4. Alternative Transcription Methods
- When traditional STT fails
- For specific audio types or conditions
- As a backup or verification method

## üöÄ Future Enhancements

### 1. Model Optimization
- Train VLM models specifically for audio transcription
- Fine-tune on audio visualization datasets
- Optimize prompts for better transcription accuracy

### 2. Advanced Visualizations
- Multi-scale spectrograms
- Advanced audio feature representations
- Custom visualization techniques

### 3. Hybrid Approaches
- Combine traditional STT with VLM results
- Ensemble methods for better accuracy
- Adaptive method selection based on audio characteristics

### 4. Real-time Processing
- Optimize for real-time audio transcription
- Streaming audio visualization
- Low-latency VLM processing

## üìä Performance Considerations

### Memory Usage
- Audio visualization requires additional memory
- VLM models are typically larger than STT models
- Image processing adds computational overhead

### Processing Time
- Audio ‚Üí Image conversion: ~100-500ms
- VLM processing: ~1-5 seconds (depending on model)
- Total time: 2-10x slower than traditional STT

### Accuracy Trade-offs
- Experimental approach with unproven accuracy
- May work better for certain audio types
- Requires careful evaluation and comparison

## üîç Monitoring and Debugging

### Logging
- Audio processing steps are logged
- VLM processing details are captured
- Error handling with detailed messages

### Visualization Debugging
- Generated images can be saved for inspection
- Different visualization types can be compared
- Quality assessment through visual inspection

## üìö References

- [Librosa Documentation](https://librosa.org/)
- [Matplotlib Documentation](https://matplotlib.org/)
- [Vision Language Models](https://huggingface.co/models?pipeline_tag=image-to-text)
- [Audio Visualization Techniques](https://en.wikipedia.org/wiki/Spectrogram)

## ü§ù Contributing

This is an experimental approach. Contributions are welcome for:
- Improving visualization techniques
- Optimizing VLM prompts
- Adding new audio visualization types
- Performance optimizations
- Accuracy improvements

## ‚öñÔ∏è License

This implementation follows the same license as the main project.