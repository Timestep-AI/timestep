{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Introduction to TorchRL\n",
        "This demo was presented at ICML 2022 on the industry demo day.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It gives a good overview of TorchRL functionalities. Feel free to reach out\n",
        "to vmoens@fb.com or submit issues if you have questions or comments about\n",
        "it.\n",
        "\n",
        "TorchRL is an open-source Reinforcement Learning (RL) library for PyTorch.\n",
        "\n",
        "https://github.com/pytorch/rl\n",
        "\n",
        "The PyTorch ecosystem team (Meta) has decided to invest in that library to\n",
        "provide a leading platform to develop RL solutions in research settings.\n",
        "\n",
        "It provides pytorch and **python-first**, low and high level\n",
        "**abstractions** # for RL that are intended to be efficient, documented and\n",
        "properly tested.\n",
        "The code is aimed at supporting research in RL. Most of it is written in\n",
        "python in a highly modular way, such that researchers can easily swap\n",
        "components, transform them or write new ones with little effort.\n",
        "\n",
        "This repo attempts to align with the existing pytorch ecosystem libraries\n",
        "in that it has a dataset pillar (torchrl/envs), transforms, models, data\n",
        "utilities (e.g. collectors and containers), etc. TorchRL aims at having as\n",
        "few dependencies as possible (python standard library, numpy and pytorch).\n",
        "Common environment libraries (e.g. OpenAI gym) are only optional.\n",
        "\n",
        "**Content**:\n",
        "   .. aafig::\n",
        "\n",
        "     \"torchrl\"\n",
        "     │\n",
        "     ├── \"collectors\"\n",
        "     │   └── \"collectors.py\"\n",
        "     ├── \"data\"\n",
        "     │   ├── \"tensor_specs.py\"\n",
        "     │   ├── \"postprocs\"\n",
        "     │   │  └── \"postprocs.py\"\n",
        "     │   └── \"replay_buffers\"\n",
        "     │      ├── \"replay_buffers.py\"\n",
        "     │      └── \"storages.py\"\n",
        "     ├── \"envs\"\n",
        "     │   ├── \"common.py\"\n",
        "     │   ├── \"env_creator.py\"\n",
        "     │   ├── \"gym_like.py\"\n",
        "     │   ├── \"vec_env.py\"\n",
        "     │   ├── \"libs\"\n",
        "     │   │  ├── \"dm_control.py\"\n",
        "     │   │  └── \"gym.py\"\n",
        "     │   └── \"transforms\"\n",
        "     │      ├── \"functional.py\"\n",
        "     │      └── \"transforms.py\"\n",
        "     ├── \"modules\"\n",
        "     │   ├── \"distributions\"\n",
        "     │   │  ├── \"continuous.py\"\n",
        "     │   │  └── \"discrete.py\"\n",
        "     │   ├── \"models\"\n",
        "     │   │  ├── \"models.py\"\n",
        "     │   │  └── \"exploration.py\"\n",
        "     │   └── \"tensordict_module\"\n",
        "     │      ├── \"actors.py\"\n",
        "     │      ├── \"common.py\"\n",
        "     │      ├── \"exploration.py\"\n",
        "     │      ├── \"probabilistic.py\"\n",
        "     │      └── \"sequence.py\"\n",
        "     ├── \"objectives\"\n",
        "     │   ├── \"common.py\"\n",
        "     │   ├── \"ddpg.py\"\n",
        "     │   ├── \"dqn.py\"\n",
        "     │   ├── \"functional.py\"\n",
        "     │   ├── \"ppo.py\"\n",
        "     │   ├── \"redq.py\"\n",
        "     │   ├── \"reinforce.py\"\n",
        "     │   ├── \"sac.py\"\n",
        "     │   ├── \"utils.py\"\n",
        "     │   └── \"value\"\n",
        "     │      ├── \"advantages.py\"\n",
        "     │      ├── \"functional.py\"\n",
        "     │      ├── \"pg.py\"\n",
        "     │      ├── \"utils.py\"\n",
        "     │      └── \"vtrace.py\"\n",
        "     ├── \"record\"\n",
        "     │   └── \"recorder.py\"\n",
        "     └── \"trainers\"\n",
        "         ├── \"loggers\"\n",
        "         │  ├── \"common.py\"\n",
        "         │  ├── \"csv.py\"\n",
        "         │  ├── \"mlflow.py\"\n",
        "         │  ├── \"tensorboard.py\"\n",
        "         │  └── \"wandb.py\"\n",
        "         ├── \"trainers.py\"\n",
        "         └── \"helpers\"\n",
        "            ├── \"collectors.py\"\n",
        "            ├── \"envs.py\"\n",
        "            ├── \"loggers.py\"\n",
        "            ├── \"losses.py\"\n",
        "            ├── \"models.py\"\n",
        "            ├── \"replay_buffer.py\"\n",
        "            └── \"trainers.py\"\n",
        "\n",
        "Unlike other domains, RL is less about media than *algorithms*. As such, it\n",
        "is harder to make truly independent components.\n",
        "\n",
        "What TorchRL is not:\n",
        "\n",
        "* a collection of algorithms: we do not intend to provide SOTA implementations of RL algorithms,\n",
        "  but we provide these algorithms only as examples of how to use the library.\n",
        "\n",
        "* a research framework: modularity in TorchRL comes in two flavours. First, we try\n",
        "  to build re-usable components, such that they can be easily swapped with each other.\n",
        "  Second, we make our best such that components can be used independently of the rest\n",
        "  of the library.\n",
        "\n",
        "TorchRL has very few core dependencies, predominantly PyTorch and numpy. All\n",
        "other dependencies (gym, torchvision, wandb / tensorboard) are optional.\n",
        "\n",
        "## Data\n",
        "\n",
        "### TensorDict\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tensordict import TensorDict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's create a TensorDict.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorDict(\n",
            "    fields={\n",
            "        key 1: Tensor(shape=torch.Size([5, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
            "        key 2: Tensor(shape=torch.Size([5, 5, 6]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
            "    batch_size=torch.Size([5]),\n",
            "    device=None,\n",
            "    is_shared=False)\n"
          ]
        }
      ],
      "source": [
        "batch_size = 5\n",
        "tensordict = TensorDict(\n",
        "    source={\n",
        "        \"key 1\": torch.zeros(batch_size, 3),\n",
        "        \"key 2\": torch.zeros(batch_size, 5, 6, dtype=torch.bool),\n",
        "    },\n",
        "    batch_size=[batch_size],\n",
        ")\n",
        "print(tensordict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can index a TensorDict as well as query keys.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorDict(\n",
            "    fields={\n",
            "        key 1: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
            "        key 2: Tensor(shape=torch.Size([5, 6]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
            "    batch_size=torch.Size([]),\n",
            "    device=None,\n",
            "    is_shared=False)\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "print(tensordict[2])\n",
        "print(tensordict[\"key 1\"] is tensordict.get(\"key 1\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following shows how to stack multiple TensorDicts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict1 = TensorDict(\n",
        "    source={\n",
        "        \"key 1\": torch.zeros(batch_size, 1),\n",
        "        \"key 2\": torch.zeros(batch_size, 5, 6, dtype=torch.bool),\n",
        "    },\n",
        "    batch_size=[batch_size],\n",
        ")\n",
        "\n",
        "tensordict2 = TensorDict(\n",
        "    source={\n",
        "        \"key 1\": torch.ones(batch_size, 1),\n",
        "        \"key 2\": torch.ones(batch_size, 5, 6, dtype=torch.bool),\n",
        "    },\n",
        "    batch_size=[batch_size],\n",
        ")\n",
        "\n",
        "tensordict = torch.stack([tensordict1, tensordict2], 0)\n",
        "tensordict.batch_size, tensordict[\"key 1\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here are some other functionalities of TensorDict.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\n",
        "    \"view(-1): \",\n",
        "    tensordict.view(-1).batch_size,\n",
        "    tensordict.view(-1).get(\"key 1\").shape,\n",
        ")\n",
        "\n",
        "print(\"to device: \", tensordict.to(\"cpu\"))\n",
        "\n",
        "# print(\"pin_memory: \", tensordict.pin_memory())\n",
        "\n",
        "print(\"share memory: \", tensordict.share_memory_())\n",
        "\n",
        "print(\n",
        "    \"permute(1, 0): \",\n",
        "    tensordict.permute(1, 0).batch_size,\n",
        "    tensordict.permute(1, 0).get(\"key 1\").shape,\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"expand: \",\n",
        "    tensordict.expand(3, *tensordict.batch_size).batch_size,\n",
        "    tensordict.expand(3, *tensordict.batch_size).get(\"key 1\").shape,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can create a **nested TensorDict** as well.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict = TensorDict(\n",
        "    source={\n",
        "        \"key 1\": torch.zeros(batch_size, 3),\n",
        "        \"key 2\": TensorDict(\n",
        "            source={\"sub-key 1\": torch.zeros(batch_size, 2, 1)},\n",
        "            batch_size=[batch_size, 2],\n",
        "        ),\n",
        "    },\n",
        "    batch_size=[batch_size],\n",
        ")\n",
        "tensordict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Replay buffers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.data import PrioritizedReplayBuffer, ReplayBuffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rb = ReplayBuffer(collate_fn=lambda x: x)\n",
        "rb.add(1)\n",
        "rb.sample(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rb.extend([2, 3])\n",
        "rb.sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rb = PrioritizedReplayBuffer(alpha=0.7, beta=1.1, collate_fn=lambda x: x)\n",
        "rb.add(1)\n",
        "rb.sample(1)\n",
        "rb.update_priority(1, 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here are examples of using a replaybuffer with tensordicts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "collate_fn = torch.stack\n",
        "rb = ReplayBuffer(collate_fn=collate_fn)\n",
        "rb.add(TensorDict({\"a\": torch.randn(3)}, batch_size=[]))\n",
        "len(rb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rb.extend(TensorDict({\"a\": torch.randn(2, 3)}, batch_size=[2]))\n",
        "print(len(rb))\n",
        "print(rb.sample(10))\n",
        "print(rb.sample(2).contiguous())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "from torchrl.data import TensorDictPrioritizedReplayBuffer\n",
        "\n",
        "rb = TensorDictPrioritizedReplayBuffer(alpha=0.7, beta=1.1, priority_key=\"td_error\")\n",
        "rb.extend(TensorDict({\"a\": torch.randn(2, 3)}, batch_size=[2]))\n",
        "tensordict_sample = rb.sample(2).contiguous()\n",
        "tensordict_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict_sample[\"index\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict_sample[\"td_error\"] = torch.rand(2)\n",
        "rb.update_tensordict_priority(tensordict_sample)\n",
        "\n",
        "for i, val in enumerate(rb._sampler._sum_tree):\n",
        "    print(i, val)\n",
        "    if i == len(rb):\n",
        "        break\n",
        "\n",
        "try:\n",
        "    import gymnasium as gym\n",
        "except ModuleNotFoundError:\n",
        "    import gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Envs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.envs.libs.gym import GymEnv, GymWrapper\n",
        "\n",
        "gym_env = gym.make(\"Pendulum-v1\")\n",
        "env = GymWrapper(gym_env)\n",
        "env = GymEnv(\"Pendulum-v1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict = env.reset()\n",
        "env.rand_step(tensordict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Changing environments config\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env = GymEnv(\"Pendulum-v1\", frame_skip=3, from_pixels=True, pixels_only=False)\n",
        "env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env.close()\n",
        "del env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.envs import (\n",
        "    Compose,\n",
        "    NoopResetEnv,\n",
        "    ObservationNorm,\n",
        "    ToTensorImage,\n",
        "    TransformedEnv,\n",
        ")\n",
        "\n",
        "base_env = GymEnv(\"Pendulum-v1\", frame_skip=3, from_pixels=True, pixels_only=False)\n",
        "env = TransformedEnv(base_env, Compose(NoopResetEnv(3), ToTensorImage()))\n",
        "env.append_transform(ObservationNorm(in_keys=[\"pixels\"], loc=2, scale=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transforms\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.envs import (\n",
        "    Compose,\n",
        "    NoopResetEnv,\n",
        "    ObservationNorm,\n",
        "    StepCounter,\n",
        "    ToTensorImage,\n",
        "    TransformedEnv,\n",
        ")\n",
        "\n",
        "base_env = GymEnv(\"Pendulum-v1\", frame_skip=3, from_pixels=True, pixels_only=False)\n",
        "env = TransformedEnv(base_env, Compose(NoopResetEnv(3), ToTensorImage()))\n",
        "env.append_transform(ObservationNorm(in_keys=[\"pixels\"], loc=2, scale=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"env: \", env)\n",
        "print(\"last transform parent: \", env.transform[2].parent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Vectorized Environments\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.envs import ParallelEnv\n",
        "\n",
        "base_env = ParallelEnv(\n",
        "    4,\n",
        "    lambda: GymEnv(\"Pendulum-v1\", frame_skip=3, from_pixels=True, pixels_only=False),\n",
        ")\n",
        "env = TransformedEnv(\n",
        "    base_env, Compose(StepCounter(), ToTensorImage())\n",
        ")  # applies transforms on batch of envs\n",
        "env.append_transform(ObservationNorm(in_keys=[\"pixels\"], loc=2, scale=1))\n",
        "env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(env.action_spec)\n",
        "\n",
        "env.close()\n",
        "del env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modules\n",
        "\n",
        "### Models\n",
        "\n",
        "Example of a MLP model:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.modules import ConvNet, MLP\n",
        "from torchrl.modules.models.utils import SquashDims\n",
        "\n",
        "net = MLP(num_cells=[32, 64], out_features=4, activation_class=nn.ELU)\n",
        "print(net)\n",
        "print(net(torch.randn(10, 3)).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Example of a CNN model:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cnn = ConvNet(\n",
        "    num_cells=[32, 64],\n",
        "    kernel_sizes=[8, 4],\n",
        "    strides=[2, 1],\n",
        "    aggregator_class=SquashDims,\n",
        ")\n",
        "print(cnn)\n",
        "print(cnn(torch.randn(10, 3, 32, 32)).shape)  # last tensor is squashed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TensorDictModules\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tensordict.nn import TensorDictModule\n",
        "\n",
        "tensordict = TensorDict({\"key 1\": torch.randn(10, 3)}, batch_size=[10])\n",
        "module = nn.Linear(3, 4)\n",
        "td_module = TensorDictModule(module, in_keys=[\"key 1\"], out_keys=[\"key 2\"])\n",
        "td_module(tensordict)\n",
        "print(tensordict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sequences of Modules\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tensordict.nn import TensorDictSequential\n",
        "\n",
        "backbone_module = nn.Linear(5, 3)\n",
        "backbone = TensorDictModule(\n",
        "    backbone_module, in_keys=[\"observation\"], out_keys=[\"hidden\"]\n",
        ")\n",
        "actor_module = nn.Linear(3, 4)\n",
        "actor = TensorDictModule(actor_module, in_keys=[\"hidden\"], out_keys=[\"action\"])\n",
        "value_module = MLP(out_features=1, num_cells=[4, 5])\n",
        "value = TensorDictModule(value_module, in_keys=[\"hidden\", \"action\"], out_keys=[\"value\"])\n",
        "\n",
        "sequence = TensorDictSequential(backbone, actor, value)\n",
        "print(sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(sequence.in_keys, sequence.out_keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict = TensorDict(\n",
        "    {\"observation\": torch.randn(3, 5)},\n",
        "    [3],\n",
        ")\n",
        "backbone(tensordict)\n",
        "actor(tensordict)\n",
        "value(tensordict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict = TensorDict(\n",
        "    {\"observation\": torch.randn(3, 5)},\n",
        "    [3],\n",
        ")\n",
        "sequence(tensordict)\n",
        "print(tensordict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Functional Programming (Ensembling / Meta-RL)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tensordict.nn import make_functional\n",
        "\n",
        "params = make_functional(sequence)\n",
        "len(list(sequence.parameters()))  # functional modules have no parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sequence(tensordict, params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch import vmap\n",
        "\n",
        "params_expand = params.expand(4)\n",
        "tensordict_exp = vmap(sequence, (None, 0))(tensordict, params_expand)\n",
        "print(tensordict_exp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Specialized Classes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "from torchrl.data import BoundedTensorSpec\n",
        "from torchrl.modules import SafeModule\n",
        "\n",
        "spec = BoundedTensorSpec(-torch.ones(3), torch.ones(3))\n",
        "base_module = nn.Linear(5, 3)\n",
        "module = SafeModule(\n",
        "    module=base_module, spec=spec, in_keys=[\"obs\"], out_keys=[\"action\"], safe=True\n",
        ")\n",
        "tensordict = TensorDict({\"obs\": torch.randn(5)}, batch_size=[])\n",
        "module(tensordict)[\"action\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict = TensorDict({\"obs\": torch.randn(5) * 100}, batch_size=[])\n",
        "module(tensordict)[\"action\"]  # safe=True projects the result within the set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.modules import Actor\n",
        "\n",
        "base_module = nn.Linear(5, 3)\n",
        "actor = Actor(base_module, in_keys=[\"obs\"])\n",
        "tensordict = TensorDict({\"obs\": torch.randn(5)}, batch_size=[])\n",
        "actor(tensordict)  # action is the default value\n",
        "\n",
        "from tensordict.nn import (\n",
        "    ProbabilisticTensorDictModule,\n",
        "    ProbabilisticTensorDictSequential,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Probabilistic modules\n",
        "from torchrl.modules import NormalParamWrapper, TanhNormal\n",
        "\n",
        "td = TensorDict({\"input\": torch.randn(3, 5)}, [3])\n",
        "net = NormalParamWrapper(nn.Linear(5, 4))  # splits the output in loc and scale\n",
        "module = TensorDictModule(net, in_keys=[\"input\"], out_keys=[\"loc\", \"scale\"])\n",
        "td_module = ProbabilisticTensorDictSequential(\n",
        "    module,\n",
        "    ProbabilisticTensorDictModule(\n",
        "        in_keys=[\"loc\", \"scale\"],\n",
        "        out_keys=[\"action\"],\n",
        "        distribution_class=TanhNormal,\n",
        "        return_log_prob=False,\n",
        "    ),\n",
        ")\n",
        "td_module(td)\n",
        "print(td)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# returning the log-probability\n",
        "td = TensorDict({\"input\": torch.randn(3, 5)}, [3])\n",
        "td_module = ProbabilisticTensorDictSequential(\n",
        "    module,\n",
        "    ProbabilisticTensorDictModule(\n",
        "        in_keys=[\"loc\", \"scale\"],\n",
        "        out_keys=[\"action\"],\n",
        "        distribution_class=TanhNormal,\n",
        "        return_log_prob=True,\n",
        "    ),\n",
        ")\n",
        "td_module(td)\n",
        "print(td)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Sampling vs mode / mean\n",
        "from torchrl.envs.utils import ExplorationType, set_exploration_type\n",
        "\n",
        "td = TensorDict({\"input\": torch.randn(3, 5)}, [3])\n",
        "\n",
        "torch.manual_seed(0)\n",
        "with set_exploration_type(ExplorationType.RANDOM):\n",
        "    td_module(td)\n",
        "    print(\"random:\", td[\"action\"])\n",
        "\n",
        "with set_exploration_type(ExplorationType.MODE):\n",
        "    td_module(td)\n",
        "    print(\"mode:\", td[\"action\"])\n",
        "\n",
        "with set_exploration_type(ExplorationType.MODE):\n",
        "    td_module(td)\n",
        "    print(\"mean:\", td[\"action\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using Environments and Modules\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.envs.utils import step_mdp\n",
        "\n",
        "env = GymEnv(\"Pendulum-v1\")\n",
        "\n",
        "action_spec = env.action_spec\n",
        "actor_module = nn.Linear(3, 1)\n",
        "actor = SafeModule(\n",
        "    actor_module, spec=action_spec, in_keys=[\"observation\"], out_keys=[\"action\"]\n",
        ")\n",
        "\n",
        "torch.manual_seed(0)\n",
        "env.set_seed(0)\n",
        "\n",
        "max_steps = 100\n",
        "tensordict = env.reset()\n",
        "tensordicts = TensorDict({}, [max_steps])\n",
        "for i in range(max_steps):\n",
        "    actor(tensordict)\n",
        "    tensordicts[i] = env.step(tensordict)\n",
        "    if tensordict[\"done\"].any():\n",
        "        break\n",
        "    tensordict = step_mdp(tensordict)  # roughly equivalent to obs = next_obs\n",
        "\n",
        "tensordicts_prealloc = tensordicts.clone()\n",
        "print(\"total steps:\", i)\n",
        "print(tensordicts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# equivalent\n",
        "torch.manual_seed(0)\n",
        "env.set_seed(0)\n",
        "\n",
        "max_steps = 100\n",
        "tensordict = env.reset()\n",
        "tensordicts = []\n",
        "for _ in range(max_steps):\n",
        "    actor(tensordict)\n",
        "    tensordicts.append(env.step(tensordict))\n",
        "    if tensordict[\"done\"].any():\n",
        "        break\n",
        "    tensordict = step_mdp(tensordict)  # roughly equivalent to obs = next_obs\n",
        "tensordicts_stack = torch.stack(tensordicts, 0)\n",
        "print(\"total steps:\", i)\n",
        "print(tensordicts_stack)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "(tensordicts_stack == tensordicts_prealloc).all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "env.set_seed(0)\n",
        "tensordict_rollout = env.rollout(policy=actor, max_steps=max_steps)\n",
        "tensordict_rollout\n",
        "\n",
        "\n",
        "(tensordict_rollout == tensordicts_prealloc).all()\n",
        "\n",
        "from tensordict.nn import TensorDictModule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Collectors\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchrl.collectors import MultiaSyncDataCollector, MultiSyncDataCollector\n",
        "\n",
        "from torchrl.envs import EnvCreator, ParallelEnv\n",
        "from torchrl.envs.libs.gym import GymEnv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "EnvCreator makes sure that we can send a lambda function from process to process\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "parallel_env = ParallelEnv(3, EnvCreator(lambda: GymEnv(\"Pendulum-v1\")))\n",
        "create_env_fn = [parallel_env, parallel_env]\n",
        "\n",
        "actor_module = nn.Linear(3, 1)\n",
        "actor = TensorDictModule(actor_module, in_keys=[\"observation\"], out_keys=[\"action\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sync data collector\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "devices = [\"cpu\", \"cpu\"]\n",
        "\n",
        "collector = MultiSyncDataCollector(\n",
        "    create_env_fn=create_env_fn,  # either a list of functions or a ParallelEnv\n",
        "    policy=actor,\n",
        "    total_frames=240,\n",
        "    max_frames_per_traj=-1,  # envs are terminating, we don't need to stop them early\n",
        "    frames_per_batch=60,  # we want 60 frames at a time (we have 3 envs per sub-collector)\n",
        "    storing_devices=devices,  # len must match len of env created\n",
        "    devices=devices,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for i, d in enumerate(collector):\n",
        "    if i == 0:\n",
        "        print(d)  # trajectories are split automatically in [6 workers x 10 steps]\n",
        "    collector.update_policy_weights_()  # make sure that our policies have the latest weights if working on multiple devices\n",
        "print(i)\n",
        "collector.shutdown()\n",
        "del collector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# async data collector: keeps working while you update your model\n",
        "collector = MultiaSyncDataCollector(\n",
        "    create_env_fn=create_env_fn,  # either a list of functions or a ParallelEnv\n",
        "    policy=actor,\n",
        "    total_frames=240,\n",
        "    max_frames_per_traj=-1,  # envs are terminating, we don't need to stop them early\n",
        "    frames_per_batch=60,  # we want 60 frames at a time (we have 3 envs per sub-collector)\n",
        "    storing_devices=devices,  # len must match len of env created\n",
        "    devices=devices,\n",
        ")\n",
        "\n",
        "for i, d in enumerate(collector):\n",
        "    if i == 0:\n",
        "        print(d)  # trajectories are split automatically in [6 workers x 10 steps]\n",
        "    collector.update_policy_weights_()  # make sure that our policies have the latest weights if working on multiple devices\n",
        "print(i)\n",
        "collector.shutdown()\n",
        "del collector\n",
        "del create_env_fn\n",
        "del parallel_env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Objectives\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# TorchRL delivers meta-RL compatible loss functions\n",
        "# Disclaimer: This APi may change in the future\n",
        "from torchrl.objectives import DDPGLoss\n",
        "\n",
        "actor_module = nn.Linear(3, 1)\n",
        "actor = TensorDictModule(actor_module, in_keys=[\"observation\"], out_keys=[\"action\"])\n",
        "\n",
        "\n",
        "class ConcatModule(nn.Linear):\n",
        "    def forward(self, obs, action):\n",
        "        return super().forward(torch.cat([obs, action], -1))\n",
        "\n",
        "\n",
        "value_module = ConcatModule(4, 1)\n",
        "value = TensorDictModule(\n",
        "    value_module, in_keys=[\"observation\", \"action\"], out_keys=[\"state_action_value\"]\n",
        ")\n",
        "\n",
        "loss_fn = DDPGLoss(actor, value, gamma=0.99)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensordict = TensorDict(\n",
        "    {\n",
        "        \"observation\": torch.randn(10, 3),\n",
        "        \"next\": {\n",
        "            \"observation\": torch.randn(10, 3),\n",
        "            \"reward\": torch.randn(10, 1),\n",
        "            \"done\": torch.zeros(10, 1, dtype=torch.bool),\n",
        "        },\n",
        "        \"action\": torch.randn(10, 1),\n",
        "    },\n",
        "    batch_size=[10],\n",
        "    device=\"cpu\",\n",
        ")\n",
        "loss_td = loss_fn(tensordict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(loss_td)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(tensordict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## State of the Library\n",
        "\n",
        "TorchRL is currently an **alpha-release**: there may be bugs and there is no\n",
        "guarantee about BC-breaking changes. We should be able to move to a beta-release\n",
        "by the end of the year. Our roadmap to get there comprises:\n",
        "\n",
        "- Distributed solutions\n",
        "- Offline RL\n",
        "- Greater support for meta-RL\n",
        "- Multi-task and hierarchical RL\n",
        "\n",
        "## Contributing\n",
        "\n",
        "We are actively looking for contributors and early users. If you're working in\n",
        "RL (or just curious), try it! Give us feedback: what will make the success of\n",
        "TorchRL is how well it covers researchers needs. To do that, we need their input!\n",
        "Since the library is nascent, it is a great time for you to shape it the way you want!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installing the Library\n",
        "\n",
        "The library is on PyPI: *pip install torchrl*\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
