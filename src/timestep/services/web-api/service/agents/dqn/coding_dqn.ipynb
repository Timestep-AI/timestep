{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# TorchRL trainer: A DQN example\n",
        "**Author**: [Vincent Moens](https://github.com/vmoens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TorchRL provides a generic :class:`~torchrl.trainers.Trainer` class to handle\n",
        "your training loop. The trainer executes a nested loop where the outer loop\n",
        "is the data collection and the inner loop consumes this data or some data\n",
        "retrieved from the replay buffer to train the model.\n",
        "At various points in this training loop, hooks can be attached and executed at\n",
        "given intervals.\n",
        "\n",
        "In this tutorial, we will be using the trainer class to train a DQN algorithm\n",
        "to solve the CartPole task from scratch.\n",
        "\n",
        "Main takeaways:\n",
        "\n",
        "- Building a trainer with its essential components: data collector, loss\n",
        "  module, replay buffer and optimizer.\n",
        "- Adding hooks to a trainer, such as loggers, target network updaters and such.\n",
        "\n",
        "The trainer is fully customisable and offers a large set of functionalities.\n",
        "The tutorial is organised around its construction.\n",
        "We will be detailing how to build each of the components of the library first,\n",
        "and then put the pieces together using the :class:`~torchrl.trainers.Trainer`\n",
        "class.\n",
        "\n",
        "Along the road, we will also focus on some other aspects of the library:\n",
        "\n",
        "- how to build an environment in TorchRL, including transforms (e.g. data\n",
        "  normalization, frame concatenation, resizing and turning to grayscale)\n",
        "  and parallel execution. Unlike what we did in the\n",
        "  [DDPG tutorial](https://pytorch.org/rl/tutorials/coding_ddpg.html), we\n",
        "  will normalize the pixels and not the state vector.\n",
        "- how to design a :class:`~torchrl.modules.QValueActor` object, i.e. an actor\n",
        "  that estimates the action values and picks up the action with the highest\n",
        "  estimated return;\n",
        "- how to collect data from your environment efficiently and store them\n",
        "  in a replay buffer;\n",
        "- how to use multi-step, a simple preprocessing step for off-policy algorithms;\n",
        "- and finally how to evaluate your model.\n",
        "\n",
        "**Prerequisites**: We encourage you to get familiar with torchrl through the\n",
        "[PPO tutorial](https://pytorch.org/rl/tutorials/coding_ppo.html) first.\n",
        "\n",
        "## DQN\n",
        "\n",
        "DQN ([Deep Q-Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)) was\n",
        "the founding work in deep reinforcement learning.\n",
        "\n",
        "On a high level, the algorithm is quite simple: Q-learning consists in\n",
        "learning a table of state-action values in such a way that, when\n",
        "encountering any particular state, we know which action to pick just by\n",
        "searching for the one with the highest value. This simple setting\n",
        "requires the actions and states to be\n",
        "discrete, otherwise a lookup table cannot be built.\n",
        "\n",
        "DQN uses a neural network that encodes a map from the state-action space to\n",
        "a value (scalar) space, which amortizes the cost of storing and exploring all\n",
        "the possible state-action combinations: if a state has not been seen in the\n",
        "past, we can still pass it in conjunction with the various actions available\n",
        "through our neural network and get an interpolated value for each of the\n",
        "actions available.\n",
        "\n",
        "We will solve the classic control problem of the cart pole. From the\n",
        "Gymnasium doc from where this environment is retrieved:\n",
        "\n",
        "| A pole is attached by an un-actuated joint to a cart, which moves along a\n",
        "| frictionless track. The pendulum is placed upright on the cart and the goal\n",
        "| is to balance the pole by applying forces in the left and right direction\n",
        "| on the cart.\n",
        "\n",
        ".. figure:: /_static/img/cartpole_demo.gif\n",
        "   :alt: Cart Pole\n",
        "\n",
        "We do not aim at giving a SOTA implementation of the algorithm, but rather\n",
        "to provide a high-level illustration of TorchRL features in the context\n",
        "of this algorithm.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import uuid\n",
        "import tempfile\n",
        "import warnings\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchrl.collectors import MultiaSyncDataCollector\n",
        "from torchrl.data import LazyMemmapStorage, MultiStep, TensorDictReplayBuffer\n",
        "from torchrl.envs import (\n",
        "    EnvCreator,\n",
        "    ExplorationType,\n",
        "    ParallelEnv,\n",
        "    RewardScaling,\n",
        "    StepCounter,\n",
        ")\n",
        "from torchrl.envs.libs.gym import GymEnv\n",
        "from torchrl.envs.transforms import (\n",
        "    CatFrames,\n",
        "    Compose,\n",
        "    GrayScale,\n",
        "    ObservationNorm,\n",
        "    Resize,\n",
        "    ToTensorImage,\n",
        "    TransformedEnv,\n",
        ")\n",
        "from torchrl.modules import DuelingCnnDQNet, EGreedyWrapper, QValueActor\n",
        "\n",
        "from torchrl.objectives import DQNLoss, SoftUpdate\n",
        "from torchrl.record.loggers.csv import CSVLogger\n",
        "from torchrl.trainers import (\n",
        "    LogReward,\n",
        "    Recorder,\n",
        "    ReplayBufferTrainer,\n",
        "    Trainer,\n",
        "    UpdateWeights,\n",
        ")\n",
        "\n",
        "\n",
        "def is_notebook() -> bool:\n",
        "    try:\n",
        "        shell = get_ipython().__class__.__name__\n",
        "        if shell == \"ZMQInteractiveShell\":\n",
        "            return True  # Jupyter notebook or qtconsole\n",
        "        elif shell == \"TerminalInteractiveShell\":\n",
        "            return False  # Terminal running IPython\n",
        "        else:\n",
        "            return False  # Other type (?)\n",
        "    except NameError:\n",
        "        return False  # Probably standard Python interpreter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's get started with the various pieces we need for our algorithm:\n",
        "\n",
        "- An environment;\n",
        "- A policy (and related modules that we group under the \"model\" umbrella);\n",
        "- A data collector, which makes the policy play in the environment and\n",
        "  delivers training data;\n",
        "- A replay buffer to store the training data;\n",
        "- A loss module, which computes the objective function to train our policy\n",
        "  to maximise the return;\n",
        "- An optimizer, which performs parameter updates based on our loss.\n",
        "\n",
        "Additional modules include a logger, a recorder (executes the policy in\n",
        "\"eval\" mode) and a target network updater. With all these components into\n",
        "place, it is easy to see how one could misplace or misuse one component in\n",
        "the training script. The trainer is there to orchestrate everything for you!\n",
        "\n",
        "## Building the environment\n",
        "\n",
        "First let's write a helper function that will output an environment. As usual,\n",
        "the \"raw\" environment may be too simple to be used in practice and we'll need\n",
        "some data transformation to expose its output to the policy.\n",
        "\n",
        "We will be using five transforms:\n",
        "\n",
        "- :class:`~torchrl.envs.StepCounter` to count the number of steps in each trajectory;\n",
        "- :class:`~torchrl.envs.transforms.ToTensorImage` will convert a ``[W, H, C]`` uint8\n",
        "  tensor in a floating point tensor in the ``[0, 1]`` space with shape\n",
        "  ``[C, W, H]``;\n",
        "- :class:`~torchrl.envs.transforms.RewardScaling` to reduce the scale of the return;\n",
        "- :class:`~torchrl.envs.transforms.GrayScale` will turn our image into grayscale;\n",
        "- :class:`~torchrl.envs.transforms.Resize` will resize the image in a 64x64 format;\n",
        "- :class:`~torchrl.envs.transforms.CatFrames` will concatenate an arbitrary number of\n",
        "  successive frames (``N=4``) in a single tensor along the channel dimension.\n",
        "  This is useful as a single image does not carry information about the\n",
        "  motion of the cartpole. Some memory about past observations and actions\n",
        "  is needed, either via a recurrent neural network or using a stack of\n",
        "  frames.\n",
        "- :class:`~torchrl.envs.transforms.ObservationNorm` which will normalize our observations\n",
        "  given some custom summary statistics.\n",
        "\n",
        "In practice, our environment builder has two arguments:\n",
        "\n",
        "- ``parallel``: determines whether multiple environments have to be run in\n",
        "  parallel. We stack the transforms after the\n",
        "  :class:`~torchrl.envs.ParallelEnv` to take advantage\n",
        "  of vectorization of the operations on device, although this would\n",
        "  technically work with every single environment attached to its own set of\n",
        "  transforms.\n",
        "- ``obs_norm_sd`` will contain the normalizing constants for\n",
        "  the :class:`~torchrl.envs.ObservationNorm` transform.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def make_env(\n",
        "    parallel=False,\n",
        "    obs_norm_sd=None,\n",
        "):\n",
        "    if obs_norm_sd is None:\n",
        "        obs_norm_sd = {\"standard_normal\": True}\n",
        "    if parallel:\n",
        "        base_env = ParallelEnv(\n",
        "            num_workers,\n",
        "            EnvCreator(\n",
        "                lambda: GymEnv(\n",
        "                    \"CartPole-v1\",\n",
        "                    from_pixels=True,\n",
        "                    pixels_only=True,\n",
        "                    device=device,\n",
        "                )\n",
        "            ),\n",
        "        )\n",
        "    else:\n",
        "        base_env = GymEnv(\n",
        "            \"CartPole-v1\",\n",
        "            from_pixels=True,\n",
        "            pixels_only=True,\n",
        "            device=device,\n",
        "        )\n",
        "\n",
        "    env = TransformedEnv(\n",
        "        base_env,\n",
        "        Compose(\n",
        "            StepCounter(),  # to count the steps of each trajectory\n",
        "            ToTensorImage(),\n",
        "            RewardScaling(loc=0.0, scale=0.1),\n",
        "            GrayScale(),\n",
        "            Resize(64, 64),\n",
        "            CatFrames(4, in_keys=[\"pixels\"], dim=-3),\n",
        "            ObservationNorm(in_keys=[\"pixels\"], **obs_norm_sd),\n",
        "        ),\n",
        "    )\n",
        "    return env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compute normalizing constants\n",
        "\n",
        "To normalize images, we don't want to normalize each pixel independently\n",
        "with a full ``[C, W, H]`` normalizing mask, but with simpler ``[C, 1, 1]``\n",
        "shaped set of normalizing constants (loc and scale parameters).\n",
        "We will be using the ``reduce_dim`` argument\n",
        "of :meth:`~torchrl.envs.ObservationNorm.init_stats` to instruct which\n",
        "dimensions must be reduced, and the ``keep_dims`` parameter to ensure that\n",
        "not all dimensions disappear in the process:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_norm_stats():\n",
        "    test_env = make_env()\n",
        "    test_env.transform[-1].init_stats(\n",
        "        num_iter=1000, cat_dim=0, reduce_dim=[-1, -2, -4], keep_dims=(-1, -2)\n",
        "    )\n",
        "    obs_norm_sd = test_env.transform[-1].state_dict()\n",
        "    # let's check that normalizing constants have a size of ``[C, 1, 1]`` where\n",
        "    # ``C=4`` (because of :class:`~torchrl.envs.CatFrames`).\n",
        "    print(\"state dict of the observation norm:\", obs_norm_sd)\n",
        "    return obs_norm_sd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building the model (Deep Q-network)\n",
        "\n",
        "The following function builds a :class:`~torchrl.modules.DuelingCnnDQNet`\n",
        "object which is a simple CNN followed by a two-layer MLP. The only trick used\n",
        "here is that the action values (i.e. left and right action value) are\n",
        "computed using\n",
        "\n",
        "\\begin{align}\\mathbb{v} = b(obs) + v(obs) - \\mathbb{E}[v(obs)]\\end{align}\n",
        "\n",
        "where $\\mathbb{v}$ is our vector of action values,\n",
        "$b$ is a $\\mathbb{R}^n \\rightarrow 1$ function and $v$ is a\n",
        "$\\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ function, for\n",
        "$n = \\# obs$ and $m = \\# actions$.\n",
        "\n",
        "Our network is wrapped in a :class:`~torchrl.modules.QValueActor`,\n",
        "which will read the state-action\n",
        "values, pick up the one with the maximum value and write all those results\n",
        "in the input :class:`tensordict.TensorDict`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def make_model(dummy_env):\n",
        "    cnn_kwargs = {\n",
        "        \"num_cells\": [32, 64, 64],\n",
        "        \"kernel_sizes\": [6, 4, 3],\n",
        "        \"strides\": [2, 2, 1],\n",
        "        \"activation_class\": nn.ELU,\n",
        "        # This can be used to reduce the size of the last layer of the CNN\n",
        "        # \"squeeze_output\": True,\n",
        "        # \"aggregator_class\": nn.AdaptiveAvgPool2d,\n",
        "        # \"aggregator_kwargs\": {\"output_size\": (1, 1)},\n",
        "    }\n",
        "    mlp_kwargs = {\n",
        "        \"depth\": 2,\n",
        "        \"num_cells\": [\n",
        "            64,\n",
        "            64,\n",
        "        ],\n",
        "        \"activation_class\": nn.ELU,\n",
        "    }\n",
        "    net = DuelingCnnDQNet(\n",
        "        dummy_env.action_spec.shape[-1], 1, cnn_kwargs, mlp_kwargs\n",
        "    ).to(device)\n",
        "    net.value[-1].bias.data.fill_(init_bias)\n",
        "\n",
        "    actor = QValueActor(net, in_keys=[\"pixels\"], spec=dummy_env.action_spec).to(device)\n",
        "    # init actor: because the model is composed of lazy conv/linear layers,\n",
        "    # we must pass a fake batch of data through it to instantiate them.\n",
        "    tensordict = dummy_env.fake_tensordict()\n",
        "    actor(tensordict)\n",
        "\n",
        "    # we wrap our actor in an EGreedyWrapper for data collection\n",
        "    actor_explore = EGreedyWrapper(\n",
        "        actor,\n",
        "        annealing_num_steps=total_frames,\n",
        "        eps_init=eps_greedy_val,\n",
        "        eps_end=eps_greedy_val_env,\n",
        "    )\n",
        "\n",
        "    return actor, actor_explore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Collecting and storing data\n",
        "\n",
        "### Replay buffers\n",
        "\n",
        "Replay buffers play a central role in off-policy RL algorithms such as DQN.\n",
        "They constitute the dataset we will be sampling from during training.\n",
        "\n",
        "Here, we will use a regular sampling strategy, although a prioritized RB\n",
        "could improve the performance significantly.\n",
        "\n",
        "We place the storage on disk using\n",
        ":class:`~torchrl.data.replay_buffers.storages.LazyMemmapStorage` class. This\n",
        "storage is created in a lazy manner: it will only be instantiated once the\n",
        "first batch of data is passed to it.\n",
        "\n",
        "The only requirement of this storage is that the data passed to it at write\n",
        "time must always have the same shape.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_replay_buffer(buffer_size, n_optim, batch_size):\n",
        "    replay_buffer = TensorDictReplayBuffer(\n",
        "        batch_size=batch_size,\n",
        "        storage=LazyMemmapStorage(buffer_size),\n",
        "        prefetch=n_optim,\n",
        "    )\n",
        "    return replay_buffer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data collector\n",
        "\n",
        "As in [PPO](https://pytorch.org/rl/tutorials/coding_ppo.html) and\n",
        "[DDPG](https://pytorch.org/rl/tutorials/coding_ddpg.html), we will be using\n",
        "a data collector as a dataloader in the outer loop.\n",
        "\n",
        "We choose the following configuration: we will be running a series of\n",
        "parallel environments synchronously in parallel in different collectors,\n",
        "themselves running in parallel but asynchronously.\n",
        "The advantage of this configuration is that we can balance the amount of\n",
        "compute that is executed in batch with what we want to be executed\n",
        "asynchronously. We encourage the reader to experiment how the collection\n",
        "speed is impacted by modifying the number of collectors (ie the number of\n",
        "environment constructors passed to the collector) and the number of\n",
        "environment executed in parallel in each collector (controlled by the\n",
        "``num_workers`` hyperparameter).\n",
        "\n",
        "When building the collector, we can choose on which device we want the\n",
        "environment and policy to execute the operations through the ``device``\n",
        "keyword argument. The ``storing_devices`` argument will modify the\n",
        "location of the data being collected: if the batches that we are gathering\n",
        "have a considerable size, we may want to store them on a different location\n",
        "than the device where the computation is happening. For asynchronous data\n",
        "collectors such as ours, different storing devices mean that the data that\n",
        "we collect won't sit on the same device each time, which is something that\n",
        "out training loop must account for. For simplicity, we set the devices to\n",
        "the same value for all sub-collectors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_collector(\n",
        "    obs_norm_sd,\n",
        "    num_collectors,\n",
        "    actor_explore,\n",
        "    frames_per_batch,\n",
        "    total_frames,\n",
        "    device,\n",
        "):\n",
        "    data_collector = MultiaSyncDataCollector(\n",
        "        [\n",
        "            make_env(parallel=True, obs_norm_sd=obs_norm_sd),\n",
        "        ]\n",
        "        * num_collectors,\n",
        "        policy=actor_explore,\n",
        "        frames_per_batch=frames_per_batch,\n",
        "        total_frames=total_frames,\n",
        "        # this is the default behaviour: the collector runs in ``\"random\"`` (or explorative) mode\n",
        "        exploration_type=ExplorationType.RANDOM,\n",
        "        # We set the all the devices to be identical. Below is an example of\n",
        "        # heterogeneous devices\n",
        "        device=device,\n",
        "        storing_device=device,\n",
        "        split_trajs=False,\n",
        "        postproc=MultiStep(gamma=gamma, n_steps=5),\n",
        "    )\n",
        "    return data_collector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loss function\n",
        "\n",
        "Building our loss function is straightforward: we only need to provide\n",
        "the model and a bunch of hyperparameters to the DQNLoss class.\n",
        "\n",
        "### Target parameters\n",
        "\n",
        "Many off-policy RL algorithms use the concept of \"target parameters\" when it\n",
        "comes to estimate the value of the next state or state-action pair.\n",
        "The target parameters are lagged copies of the model parameters. Because\n",
        "their predictions mismatch those of the current model configuration, they\n",
        "help learning by putting a pessimistic bound on the value being estimated.\n",
        "This is a powerful trick (known as \"Double Q-Learning\") that is ubiquitous\n",
        "in similar algorithms.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_loss_module(actor, gamma):\n",
        "    loss_module = DQNLoss(actor, delay_value=True)\n",
        "    loss_module.make_value_estimator(gamma=gamma)\n",
        "    target_updater = SoftUpdate(loss_module, eps=0.995)\n",
        "    return loss_module, target_updater"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameters\n",
        "\n",
        "Let's start with our hyperparameters. The following setting should work well\n",
        "in practice, and the performance of the algorithm should hopefully not be\n",
        "too sensitive to slight variations of these.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "device = \"cuda:0\" if torch.cuda.device_count() > 0 else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# the learning rate of the optimizer\n",
        "lr = 2e-3\n",
        "# weight decay\n",
        "wd = 1e-5\n",
        "# the beta parameters of Adam\n",
        "betas = (0.9, 0.999)\n",
        "# Optimization steps per batch collected (aka UPD or updates per data)\n",
        "n_optim = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DQN parameters\n",
        "gamma decay factor\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "gamma = 0.99"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Smooth target network update decay parameter.\n",
        "This loosely corresponds to a 1/tau interval with hard target network\n",
        "update\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tau = 0.02"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data collection and replay buffer\n",
        "\n",
        "<div class=\"alert alert-info\"><h4>Note</h4><p>Values to be used for proper training have been commented.</p></div>\n",
        "\n",
        "Total frames collected in the environment. In other implementations, the\n",
        "user defines a maximum number of episodes.\n",
        "This is harder to do with our data collectors since they return batches\n",
        "of N collected frames, where N is a constant.\n",
        "However, one can easily get the same restriction on number of episodes by\n",
        "breaking the training loop when a certain number\n",
        "episodes has been collected.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "total_frames = 5_000  # 500000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Random frames used to initialize the replay buffer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "init_random_frames = 100  # 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Frames in each batch collected.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "frames_per_batch = 32  # 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Frames sampled from the replay buffer at each optimization step\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_size = 32  # 256"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Size of the replay buffer in terms of frames\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "buffer_size = min(total_frames, 100000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Number of environments run in parallel in each data collector\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_workers = 2  # 8\n",
        "num_collectors = 2  # 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Environment and exploration\n",
        "\n",
        "We set the initial and final value of the epsilon factor in Epsilon-greedy\n",
        "exploration.\n",
        "Since our policy is deterministic, exploration is crucial: without it, the\n",
        "only source of randomness would be the environment reset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "eps_greedy_val = 0.1\n",
        "eps_greedy_val_env = 0.005"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To speed up learning, we set the bias of the last layer of our value network\n",
        "to a predefined value (this is not mandatory)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "init_bias = 2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>For fast rendering of the tutorial ``total_frames`` hyperparameter\n",
        "  was set to a very low number. To get a reasonable performance, use a greater\n",
        "  value e.g. 500000</p></div>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building a Trainer\n",
        "\n",
        "TorchRL's :class:`~torchrl.trainers.Trainer` class constructor takes the\n",
        "following keyword-only arguments:\n",
        "\n",
        "- ``collector``\n",
        "- ``loss_module``\n",
        "- ``optimizer``\n",
        "- ``logger``: A logger can be\n",
        "- ``total_frames``: this parameter defines the lifespan of the trainer.\n",
        "- ``frame_skip``: when a frame-skip is used, the collector must be made\n",
        "  aware of it in order to accurately count the number of frames\n",
        "  collected etc. Making the trainer aware of this parameter is not\n",
        "  mandatory but helps to have a fairer comparison between settings where\n",
        "  the total number of frames (budget) is fixed but the frame-skip is\n",
        "  variable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mjschock/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_envs to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_envs` for environment variables or `env.get_wrapper_attr('num_envs')` that will search the reminding wrappers.\u001b[0m\n",
            "  logger.warn(\n",
            "/home/mjschock/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward_space` for environment variables or `env.get_wrapper_attr('reward_space')` that will search the reminding wrappers.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "state dict of the observation norm: OrderedDict([('standard_normal', tensor(True, device='cuda:0')), ('loc', tensor([[[0.9895]],\n",
            "\n",
            "        [[0.9895]],\n",
            "\n",
            "        [[0.9895]],\n",
            "\n",
            "        [[0.9895]]], device='cuda:0')), ('scale', tensor([[[0.0737]],\n",
            "\n",
            "        [[0.0737]],\n",
            "\n",
            "        [[0.0737]],\n",
            "\n",
            "        [[0.0737]]], device='cuda:0'))])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mjschock/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_envs to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_envs` for environment variables or `env.get_wrapper_attr('num_envs')` that will search the reminding wrappers.\u001b[0m\n",
            "  logger.warn(\n",
            "/home/mjschock/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward_space` for environment variables or `env.get_wrapper_attr('reward_space')` that will search the reminding wrappers.\u001b[0m\n",
            "  logger.warn(\n",
            "/home/mjschock/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
            "/home/mjschock/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_envs to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_envs` for environment variables or `env.get_wrapper_attr('num_envs')` that will search the reminding wrappers.\u001b[0m\n",
            "  logger.warn(\n",
            "/home/mjschock/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.reward_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.reward_space` for environment variables or `env.get_wrapper_attr('reward_space')` that will search the reminding wrappers.\u001b[0m\n",
            "  logger.warn(\n",
            "/home/mjschock/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/torchrl/collectors/collectors.py:1239: UserWarning: total_frames (5000) is not exactly divisible by frames_per_batch (32).This means 24 additional frames will be collected.To silence this message, set the environment variable RL_WARNINGS to False.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "self.log_dir: /tmp/tmptr690zuz/dqn_exp_9b77f602-653a-11ee-8af5-b5981fb12a61\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1246818/3134880558.py:16: UserWarning: log dir: /tmp/tmptr690zuz/dqn_exp_9b77f602-653a-11ee-8af5-b5981fb12a61\n",
            "  warnings.warn(f\"log dir: {logger.experiment.log_dir}\")\n"
          ]
        }
      ],
      "source": [
        "stats = get_norm_stats()\n",
        "test_env = make_env(parallel=False, obs_norm_sd=stats)\n",
        "# Get model\n",
        "actor, actor_explore = make_model(test_env)\n",
        "loss_module, target_net_updater = get_loss_module(actor, gamma)\n",
        "\n",
        "collector = get_collector(\n",
        "    stats, num_collectors, actor_explore, frames_per_batch, total_frames, device\n",
        ")\n",
        "optimizer = torch.optim.Adam(\n",
        "    loss_module.parameters(), lr=lr, weight_decay=wd, betas=betas\n",
        ")\n",
        "exp_name = f\"dqn_exp_{uuid.uuid1()}\"\n",
        "tmpdir = tempfile.TemporaryDirectory()\n",
        "logger = CSVLogger(exp_name=exp_name, log_dir=tmpdir.name)\n",
        "warnings.warn(f\"log dir: {logger.experiment.log_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can control how often the scalars should be logged. Here we set this\n",
        "to a low value as our training loop is short:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "log_interval = 500\n",
        "\n",
        "trainer = Trainer(\n",
        "    collector=collector,\n",
        "    total_frames=total_frames,\n",
        "    frame_skip=1,\n",
        "    loss_module=loss_module,\n",
        "    optimizer=optimizer,\n",
        "    logger=logger,\n",
        "    optim_steps_per_batch=n_optim,\n",
        "    log_interval=log_interval,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Registering hooks\n",
        "\n",
        "Registering hooks can be achieved in two separate ways:\n",
        "\n",
        "- If the hook has it, the :meth:`~torchrl.trainers.TrainerHookBase.register`\n",
        "  method is the first choice. One just needs to provide the trainer as input\n",
        "  and the hook will be registered with a default name at a default location.\n",
        "  For some hooks, the registration can be quite complex: :class:`~torchrl.trainers.ReplayBufferTrainer`\n",
        "  requires 3 hooks (``extend``, ``sample`` and ``update_priority``) which\n",
        "  can be cumbersome to implement.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "buffer_hook = ReplayBufferTrainer(\n",
        "    get_replay_buffer(buffer_size, n_optim, batch_size=batch_size),\n",
        "    flatten_tensordicts=True,\n",
        ")\n",
        "buffer_hook.register(trainer)\n",
        "weight_updater = UpdateWeights(collector, update_weights_interval=1)\n",
        "weight_updater.register(trainer)\n",
        "recorder = Recorder(\n",
        "    record_interval=100,  # log every 100 optimization steps\n",
        "    record_frames=1000,  # maximum number of frames in the record\n",
        "    frame_skip=1,\n",
        "    policy_exploration=actor_explore,\n",
        "    environment=test_env,\n",
        "    exploration_type=ExplorationType.MODE,\n",
        "    log_keys=[(\"next\", \"reward\")],\n",
        "    out_keys={(\"next\", \"reward\"): \"rewards\"},\n",
        "    log_pbar=True,\n",
        ")\n",
        "recorder.register(trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Any callable (including :class:`~torchrl.trainers.TrainerHookBase`\n",
        "  subclasses) can be registered using :meth:`~torchrl.trainers.Trainer.register_op`.\n",
        "  In this case, a location must be explicitly passed (). This method gives\n",
        "  more control over the location of the hook but it also requires more\n",
        "  understanding of the Trainer mechanism.\n",
        "  Check the [trainer documentation](https://pytorch.org/rl/reference/trainers.html)\n",
        "  for a detailed description of the trainer hooks.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trainer.register_op(\"post_optim\", target_net_updater.step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can log the training rewards too. Note that this is of limited interest\n",
        "with CartPole, as rewards are always 1. The discounted sum of rewards is\n",
        "maximised not by getting higher rewards but by keeping the cart-pole alive\n",
        "for longer.\n",
        "This will be reflected by the `total_rewards` value displayed in the\n",
        "progress bar.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "log_reward = LogReward(log_pbar=True)\n",
        "log_reward.register(trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>It is possible to link multiple optimizers to the trainer if needed.\n",
        "  In this case, each optimizer will be tied to a field in the loss\n",
        "  dictionary.\n",
        "  Check the :class:`~torchrl.trainers.OptimizerHook` to learn more.</p></div>\n",
        "\n",
        "Here we are, ready to train our algorithm! A simple call to\n",
        "``trainer.train()`` and we'll be getting our results logged in.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/5000 [00:00<?, ?it/s]"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "TensorDictModule failed with operation\n    DuelingCnnDQNet(\n      (features): ConvNet(\n        (0): Conv2d(4, 32, kernel_size=(6, 6), stride=(2, 2))\n        (1): ELU(alpha=1.0)\n        (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n        (3): ELU(alpha=1.0)\n        (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n        (5): ELU(alpha=1.0)\n        (6): SquashDims()\n      )\n      (advantage): MLP(\n        (0): Linear(in_features=9216, out_features=64, bias=True)\n        (1): ELU(alpha=1.0)\n        (2): Linear(in_features=64, out_features=64, bias=True)\n        (3): ELU(alpha=1.0)\n        (4): Linear(in_features=64, out_features=2, bias=True)\n      )\n      (value): MLP(\n        (0): Linear(in_features=9216, out_features=64, bias=True)\n        (1): ELU(alpha=1.0)\n        (2): Linear(in_features=64, out_features=64, bias=True)\n        (3): ELU(alpha=1.0)\n        (4): Linear(in_features=64, out_features=1, bias=True)\n      )\n    )\n    in_keys=['pixels']\n    out_keys=['action_value'].",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/tensordict/nn/common.py:1178\u001b[0m, in \u001b[0;36mTensorDictModule.forward\u001b[0;34m(self, tensordict, tensordict_out, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m         \u001b[39mraise\u001b[39;00m err\n\u001b[1;32m   1179\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(tensors, (\u001b[39mdict\u001b[39m, TensorDictBase)):\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/tensordict/nn/common.py:1164\u001b[0m, in \u001b[0;36mTensorDictModule.forward\u001b[0;34m(self, tensordict, tensordict_out, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1164\u001b[0m     tensors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_module(tensors, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1165\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/tensordict/nn/common.py:1121\u001b[0m, in \u001b[0;36mTensorDictModule._call_module\u001b[0;34m(self, tensors, **kwargs)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_module\u001b[39m(\n\u001b[1;32m   1119\u001b[0m     \u001b[39mself\u001b[39m, tensors: Sequence[Tensor], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any\n\u001b[1;32m   1120\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor \u001b[39m|\u001b[39m Sequence[Tensor]:\n\u001b[0;32m-> 1121\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodule(\u001b[39m*\u001b[39;49mtensors, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1122\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1519\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1528\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1527\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1528\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1530\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/tensordict/nn/functional_modules.py:588\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 588\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mtype\u001b[39;49m(\u001b[39mself\u001b[39;49m), fun_name)(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/torchrl/modules/models/models.py:865\u001b[0m, in \u001b[0;36mDuelingCnnDQNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 865\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures(x)\n\u001b[1;32m    866\u001b[0m     advantage \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvantage(x)\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1519\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1528\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1527\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1528\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1530\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/tensordict/nn/functional_modules.py:588\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 588\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mtype\u001b[39;49m(\u001b[39mself\u001b[39;49m), fun_name)(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/torchrl/modules/models/models.py:484\u001b[0m, in \u001b[0;36mConvNet.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    483\u001b[0m     inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mflatten(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(batch) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m--> 484\u001b[0m out \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m(ConvNet, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mforward(inputs)\n\u001b[1;32m    485\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(batch) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    216\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1519\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1528\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1527\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1528\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1530\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/tensordict/nn/functional_modules.py:588\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 588\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mtype\u001b[39;49m(\u001b[39mself\u001b[39;49m), fun_name)(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/tensordict/memmap.py:431\u001b[0m, in \u001b[0;36mMemmapTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m args \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(a\u001b[39m.\u001b[39m_tensor \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(a, \u001b[39m\"\u001b[39m\u001b[39m_tensor\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m a \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m args)\n\u001b[0;32m--> 431\u001b[0m ret \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    432\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m/home/mjschock/Projects/timestep-ai/timestep/src/timestep/platform/agents/dqn/coding_dqn.ipynb Cell 52\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/mjschock/Projects/timestep-ai/timestep/src/timestep/platform/agents/dqn/coding_dqn.ipynb#Y102sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/torchrl/trainers/trainers.py:451\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pre_steps_log_hook(batch)\n\u001b[1;32m    450\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcollected_frames \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcollector\u001b[39m.\u001b[39minit_random_frames:\n\u001b[0;32m--> 451\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptim_steps(batch)\n\u001b[1;32m    452\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_steps_hook()\n\u001b[1;32m    454\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_steps_log_hook(batch)\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/torchrl/trainers/trainers.py:484\u001b[0m, in \u001b[0;36mTrainer.optim_steps\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optim_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    483\u001b[0m sub_batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_optim_batch_hook(batch)\n\u001b[0;32m--> 484\u001b[0m losses_td \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss_module(sub_batch)\n\u001b[1;32m    485\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_loss_hook(sub_batch)\n\u001b[1;32m    487\u001b[0m losses_detached \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_hook(losses_td)\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1518\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1519\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1569\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1566\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1567\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1569\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1570\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1571\u001b[0m     \u001b[39mfor\u001b[39;00m hook_id, hook \u001b[39min\u001b[39;00m (\n\u001b[1;32m   1572\u001b[0m         \u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1573\u001b[0m         \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1574\u001b[0m     ):\n\u001b[1;32m   1575\u001b[0m         \u001b[39m# mark that always called hook is run\u001b[39;00m\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/tensordict/_contextlib.py:126\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    125\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 126\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/tensordict/nn/common.py:282\u001b[0m, in \u001b[0;36mdispatch.__call__.<locals>.wrapper\u001b[0;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(out[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m dest)\n\u001b[1;32m    281\u001b[0m     \u001b[39mreturn\u001b[39;00m out[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m out\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/torchrl/objectives/dqn.py:282\u001b[0m, in \u001b[0;36mDQNLoss.forward\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Computes the DQN loss given a tensordict sampled from the replay buffer.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \n\u001b[1;32m    270\u001b[0m \u001b[39mThis function will also write a \"td_error\" key that can be used by prioritized replay buffers to assign\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m \n\u001b[1;32m    280\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    281\u001b[0m td_copy \u001b[39m=\u001b[39m tensordict\u001b[39m.\u001b[39mclone(\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 282\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalue_network(\n\u001b[1;32m    283\u001b[0m     td_copy,\n\u001b[1;32m    284\u001b[0m     params\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalue_network_params,\n\u001b[1;32m    285\u001b[0m )\n\u001b[1;32m    287\u001b[0m action \u001b[39m=\u001b[39m tensordict\u001b[39m.\u001b[39mget(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensor_keys\u001b[39m.\u001b[39maction)\n\u001b[1;32m    288\u001b[0m pred_val \u001b[39m=\u001b[39m td_copy\u001b[39m.\u001b[39mget(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensor_keys\u001b[39m.\u001b[39maction_value)\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1518\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1519\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1528\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1525\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1527\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1528\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1530\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1531\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/tensordict/nn/functional_modules.py:576\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m old_params \u001b[39m=\u001b[39m _assign_params(\n\u001b[1;32m    573\u001b[0m     \u001b[39mself\u001b[39m, params, make_stateless\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, return_old_tensordict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    574\u001b[0m )\n\u001b[1;32m    575\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 576\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(\u001b[39mtype\u001b[39;49m(\u001b[39mself\u001b[39;49m), fun_name)(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    577\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    578\u001b[0m     \u001b[39m# reset the previous params, and tell the submodules to look for params\u001b[39;00m\n\u001b[1;32m    579\u001b[0m     _assign_params(\n\u001b[1;32m    580\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    581\u001b[0m         old_params,\n\u001b[1;32m    582\u001b[0m         make_stateless\u001b[39m=\u001b[39m_is_stateless,\n\u001b[1;32m    583\u001b[0m         return_old_tensordict\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    584\u001b[0m     )\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/tensordict/nn/common.py:282\u001b[0m, in \u001b[0;36mdispatch.__call__.<locals>.wrapper\u001b[0;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(out[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m dest)\n\u001b[1;32m    281\u001b[0m     \u001b[39mreturn\u001b[39;00m out[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m out\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/tensordict/_contextlib.py:126\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    125\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 126\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/tensordict/nn/utils.py:254\u001b[0m, in \u001b[0;36mset_skip_existing.__call__.<locals>.wrapper\u001b[0;34m(_self, tensordict, *args, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    249\u001b[0m     skip_existing()\n\u001b[1;32m    250\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(key \u001b[39min\u001b[39;00m tensordict\u001b[39m.\u001b[39mkeys(\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m out_keys)\n\u001b[1;32m    251\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39many\u001b[39m(key \u001b[39min\u001b[39;00m out_keys \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m in_keys)\n\u001b[1;32m    252\u001b[0m ):\n\u001b[1;32m    253\u001b[0m     \u001b[39mreturn\u001b[39;00m tensordict\n\u001b[0;32m--> 254\u001b[0m \u001b[39mreturn\u001b[39;00m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/tensordict/nn/sequence.py:426\u001b[0m, in \u001b[0;36mTensorDictSequential.forward\u001b[0;34m(self, tensordict, tensordict_out, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(kwargs):\n\u001b[1;32m    425\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule:\n\u001b[0;32m--> 426\u001b[0m         tensordict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_module(module, tensordict, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    427\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    429\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTensorDictSequential does not support keyword arguments other than \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtensordict_out\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or in_keys: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_keys\u001b[39m}\u001b[39;00m\u001b[39m. Got \u001b[39m\u001b[39m{\u001b[39;00mkwargs\u001b[39m.\u001b[39mkeys()\u001b[39m}\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m     )\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/tensordict/nn/sequence.py:407\u001b[0m, in \u001b[0;36mTensorDictSequential._run_module\u001b[0;34m(self, module, tensordict, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_module\u001b[39m(\n\u001b[1;32m    399\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    400\u001b[0m     module: TensorDictModule,\n\u001b[1;32m    401\u001b[0m     tensordict: TensorDictBase,\n\u001b[1;32m    402\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    403\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    404\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpartial_tolerant \u001b[39mor\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[1;32m    405\u001b[0m         key \u001b[39min\u001b[39;00m tensordict\u001b[39m.\u001b[39mkeys(include_nested\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39min_keys\n\u001b[1;32m    406\u001b[0m     ):\n\u001b[0;32m--> 407\u001b[0m         tensordict \u001b[39m=\u001b[39m module(tensordict, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    408\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpartial_tolerant \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(tensordict, LazyStackedTensorDict):\n\u001b[1;32m    409\u001b[0m         \u001b[39mfor\u001b[39;00m sub_td \u001b[39min\u001b[39;00m tensordict\u001b[39m.\u001b[39mtensordicts:\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1518\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1519\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1528\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1525\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1527\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1528\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1530\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1531\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/tensordict/nn/functional_modules.py:588\u001b[0m, in \u001b[0;36m_make_decorator.<locals>.new_fun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    587\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 588\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mtype\u001b[39;49m(\u001b[39mself\u001b[39;49m), fun_name)(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    589\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    590\u001b[0m         pattern \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.*takes \u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ positional arguments but \u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ were given|got multiple values for argument\u001b[39m\u001b[39m\"\u001b[39m\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/tensordict/nn/common.py:282\u001b[0m, in \u001b[0;36mdispatch.__call__.<locals>.wrapper\u001b[0;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(out[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m dest)\n\u001b[1;32m    281\u001b[0m     \u001b[39mreturn\u001b[39;00m out[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m out\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/tensordict/_contextlib.py:126\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    125\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 126\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/tensordict/nn/utils.py:254\u001b[0m, in \u001b[0;36mset_skip_existing.__call__.<locals>.wrapper\u001b[0;34m(_self, tensordict, *args, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    249\u001b[0m     skip_existing()\n\u001b[1;32m    250\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(key \u001b[39min\u001b[39;00m tensordict\u001b[39m.\u001b[39mkeys(\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m out_keys)\n\u001b[1;32m    251\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39many\u001b[39m(key \u001b[39min\u001b[39;00m out_keys \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m in_keys)\n\u001b[1;32m    252\u001b[0m ):\n\u001b[1;32m    253\u001b[0m     \u001b[39mreturn\u001b[39;00m tensordict\n\u001b[0;32m--> 254\u001b[0m \u001b[39mreturn\u001b[39;00m func(_self, tensordict, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Projects/timestep-ai/timestep/.venv/lib/python3.11/site-packages/tensordict/nn/common.py:1204\u001b[0m, in \u001b[0;36mTensorDictModule.forward\u001b[0;34m(self, tensordict, tensordict_out, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1202\u001b[0m in_keys \u001b[39m=\u001b[39m indent(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39min_keys=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_keys\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m4\u001b[39m \u001b[39m*\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1203\u001b[0m out_keys \u001b[39m=\u001b[39m indent(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mout_keys=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_keys\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m4\u001b[39m \u001b[39m*\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1204\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1205\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTensorDictModule failed with operation\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmodule\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00min_keys\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mout_keys\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1206\u001b[0m ) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: TensorDictModule failed with operation\n    DuelingCnnDQNet(\n      (features): ConvNet(\n        (0): Conv2d(4, 32, kernel_size=(6, 6), stride=(2, 2))\n        (1): ELU(alpha=1.0)\n        (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n        (3): ELU(alpha=1.0)\n        (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n        (5): ELU(alpha=1.0)\n        (6): SquashDims()\n      )\n      (advantage): MLP(\n        (0): Linear(in_features=9216, out_features=64, bias=True)\n        (1): ELU(alpha=1.0)\n        (2): Linear(in_features=64, out_features=64, bias=True)\n        (3): ELU(alpha=1.0)\n        (4): Linear(in_features=64, out_features=2, bias=True)\n      )\n      (value): MLP(\n        (0): Linear(in_features=9216, out_features=64, bias=True)\n        (1): ELU(alpha=1.0)\n        (2): Linear(in_features=64, out_features=64, bias=True)\n        (3): ELU(alpha=1.0)\n        (4): Linear(in_features=64, out_features=1, bias=True)\n      )\n    )\n    in_keys=['pixels']\n    out_keys=['action_value']."
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now quickly check the CSVs with the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def print_csv_files_in_folder(folder_path):\n",
        "    \"\"\"\n",
        "    Find all CSV files in a folder and prints the first 10 lines of each file.\n",
        "\n",
        "    Args:\n",
        "        folder_path (str): The relative path to the folder.\n",
        "\n",
        "    \"\"\"\n",
        "    csv_files = []\n",
        "    output_str = \"\"\n",
        "    for dirpath, _, filenames in os.walk(folder_path):\n",
        "        for file in filenames:\n",
        "            if file.endswith(\".csv\"):\n",
        "                csv_files.append(os.path.join(dirpath, file))\n",
        "    for csv_file in csv_files:\n",
        "        output_str += f\"File: {csv_file}\\n\"\n",
        "        with open(csv_file, \"r\") as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if i == 10:\n",
        "                    break\n",
        "                output_str += line.strip() + \"\\n\"\n",
        "        output_str += \"\\n\"\n",
        "    print(output_str)\n",
        "\n",
        "\n",
        "print_csv_files_in_folder(logger.experiment.log_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion and possible improvements\n",
        "\n",
        "In this tutorial we have learned:\n",
        "\n",
        "- How to write a Trainer, including building its components and registering\n",
        "  them in the trainer;\n",
        "- How to code a DQN algorithm, including how to create a policy that picks\n",
        "  up the action with the highest value with\n",
        "  :class:`~torchrl.modules.QValueNetwork`;\n",
        "- How to build a multiprocessed data collector;\n",
        "\n",
        "Possible improvements to this tutorial could include:\n",
        "\n",
        "- A prioritized replay buffer could also be used. This will give a\n",
        "  higher priority to samples that have the worst value accuracy.\n",
        "  Learn more on the\n",
        "  [replay buffer section](https://pytorch.org/rl/reference/data.html#composable-replay-buffers)\n",
        "  of the documentation.\n",
        "- A distributional loss (see :class:`~torchrl.objectives.DistributionalDQNLoss`\n",
        "  for more information).\n",
        "- More fancy exploration techniques, such as :class:`~torchrl.modules.NoisyLinear` layers and such.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
