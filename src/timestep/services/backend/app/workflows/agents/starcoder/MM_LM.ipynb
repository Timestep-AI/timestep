{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Language Model:\n",
    "\n",
    "Use the VisionTextDualEncoder as the encoder and for the decoder use GPT2 and ImageGPT.\n",
    "\n",
    "# References:\n",
    "# https://huggingface.co/docs/transformers/model_doc/vision-text-dual-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import (\n",
    "    EncoderDecoderModel,\n",
    "    VisionTextDualEncoderModel,\n",
    "    VisionTextDualEncoderProcessor,\n",
    "    AutoImageProcessor,\n",
    "    AutoTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The projection layer and logit scale weights `['visual_projection.weight', 'text_projection.weight', 'logit_scale']` are newly initialized. You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "processor = VisionTextDualEncoderProcessor(image_processor, tokenizer)\n",
    "encoder_model = VisionTextDualEncoderModel.from_vision_text_pretrained(\n",
    "    \"google/vit-base-patch16-224\", \"bert-base-uncased\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrastive training\n",
    "urls = [\n",
    "    \"http://images.cocodataset.org/val2017/000000039769.jpg\",\n",
    "    \"https://farm3.staticflickr.com/2674/5850229113_4fe05d5265_z.jpg\",\n",
    "]\n",
    "images = [Image.open(requests.get(url, stream=True).raw) for url in urls]\n",
    "inputs = processor(\n",
    "    text=[\"a photo of a cat\", \"a photo of a dog\"], images=images, return_tensors=\"pt\", padding=True\n",
    ")\n",
    "outputs = encoder_model(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    pixel_values=inputs.pixel_values,\n",
    "    return_loss=True,\n",
    ")\n",
    "loss, logits_per_image = outputs.loss, outputs.logits_per_image  # this is the image-text similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and load from pretrained\n",
    "encoder_model.save_pretrained(\"vit-bert\")\n",
    "encoder_model = VisionTextDualEncoderModel.from_pretrained(\"./vit-bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "outputs = encoder_model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.4.crossattention.q_attn.weight', 'h.9.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.11.crossattention.c_attn.weight', 'h.7.ln_cross_attn.weight', 'h.10.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_proj.weight', 'h.9.ln_cross_attn.bias', 'h.2.crossattention.c_proj.weight', 'h.5.crossattention.c_attn.bias', 'h.3.ln_cross_attn.weight', 'h.0.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.2.crossattention.c_attn.bias', 'h.4.crossattention.c_proj.weight', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.0.crossattention.q_attn.bias', 'h.11.ln_cross_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.weight', 'h.8.crossattention.c_proj.bias', 'h.6.ln_cross_attn.weight', 'h.1.ln_cross_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.2.crossattention.q_attn.weight', 'h.11.crossattention.q_attn.weight', 'h.0.crossattention.c_proj.weight', 'h.1.ln_cross_attn.weight', 'h.10.ln_cross_attn.bias', 'h.7.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.weight', 'h.4.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.bias', 'h.5.ln_cross_attn.bias', 'h.7.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.bias', 'h.3.crossattention.c_proj.bias', 'h.8.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.bias', 'h.3.ln_cross_attn.bias', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.c_proj.bias', 'h.0.ln_cross_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.4.ln_cross_attn.bias', 'h.6.crossattention.c_attn.bias', 'h.9.ln_cross_attn.weight', 'h.6.crossattention.c_proj.weight', 'h.5.ln_cross_attn.weight', 'h.5.crossattention.q_attn.weight', 'h.6.crossattention.c_attn.weight', 'h.4.crossattention.c_attn.weight', 'h.8.crossattention.q_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.7.crossattention.c_attn.bias', 'h.8.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.2.crossattention.c_attn.weight', 'h.0.ln_cross_attn.weight', 'h.11.ln_cross_attn.weight', 'h.5.crossattention.c_proj.weight', 'h.9.crossattention.c_proj.bias', 'h.3.crossattention.q_attn.weight', 'h.8.ln_cross_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.7.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.8.crossattention.q_attn.bias', 'h.7.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.10.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.0.crossattention.c_attn.bias', 'h.3.crossattention.q_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.0.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'VisionTextDualEncoderConfig' object has no attribute 'hidden_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mEncoderDecoderModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_encoder_decoder_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./vit-bert\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/mjschock/timestep/src/timestep/services/backend/app/workflows/agents/starcoder/.venv/lib/python3.11/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:537\u001b[0m, in \u001b[0;36mEncoderDecoderModel.from_encoder_decoder_pretrained\u001b[0;34m(cls, encoder_pretrained_model_name_or_path, decoder_pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;66;03m# instantiate config with corresponding kwargs\u001b[39;00m\n\u001b[1;32m    536\u001b[0m config \u001b[38;5;241m=\u001b[39m EncoderDecoderConfig\u001b[38;5;241m.\u001b[39mfrom_encoder_decoder_configs(encoder\u001b[38;5;241m.\u001b[39mconfig, decoder\u001b[38;5;241m.\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 537\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/mjschock/timestep/src/timestep/services/backend/app/workflows/agents/starcoder/.venv/lib/python3.11/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:239\u001b[0m, in \u001b[0;36mEncoderDecoderModel.__init__\u001b[0;34m(self, config, encoder, decoder)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# encoder outputs might need to be projected to different dimension for decoder\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_size\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mcross_attention_hidden_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    241\u001b[0m ):\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc_to_dec_proj \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_size)\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mget_output_embeddings() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Projects/mjschock/timestep/src/timestep/services/backend/app/workflows/agents/starcoder/.venv/lib/python3.11/site-packages/transformers/configuration_utils.py:261\u001b[0m, in \u001b[0;36mPretrainedConfig.__getattribute__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    260\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m)[key]\n\u001b[0;32m--> 261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'VisionTextDualEncoderConfig' object has no attribute 'hidden_size'"
     ]
    }
   ],
   "source": [
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"./vit-bert\", \"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
