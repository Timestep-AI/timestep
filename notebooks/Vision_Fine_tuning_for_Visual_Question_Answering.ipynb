{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copied and modified from https://github.com/openai/openai-cookbook/blob/010e24382c22fd3edb4f887b2c2d7639baffcc1b/examples/multimodal/Vision_Fine_tuning_on_GPT4o_for_Visual_Question_Answering.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Fine-tuning on GPT-4o for Visual Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're excited to announce the launch of [Vision Fine-Tuning on GPT-4o](https://openai.com/index/introducing-vision-to-the-fine-tuning-api/), a cutting-edge multimodal fine-tuning capability that empowers developers to fine-tune GPT-4o using both **images** and **text**. With this new feature, you can customize models to have stronger image understanding capabilities, unlocking possibilities across various industries and applications.\n",
    "\n",
    "From **advanced visual search** to **improved object detection** for autonomous vehicles or smart cities, vision fine-tuning enables you to craft solutions tailored to your specific needs. By combining text and image inputs, this product is uniquely positioned for tasks like **visual question answering**, where detailed, context-aware answers are derived from analyzing images. In general, this seems to be most effective when the model is presented with questions and images that resemble the training data as we are able to teach the model how to search and identify relevant parts of the image to answer the question correctly. Similarly to fine-tuning on text inputs, vision fine-tuning is not as useful for teaching the model new information.\n",
    "\n",
    "In this guide, we’ll walk you through the steps to fine-tune GPT-4o with multimodal inputs. Specifically, we’ll demonstrate how to train a model for answering questions related to **images of books**, but the potential applications span countless domains—from **web design** and **education** to **healthcare** and **research**.\n",
    "\n",
    "Whether you're looking to build smarter defect detection models for manufacturing, enhance complex document processing and diagram understanding, or develop applications with better visual comprehension for a variety of other use cases, this guide will show you just how fast and easy it is to get started.\n",
    "\n",
    "For more information, check out the full [Documentation](https://platform.openai.com/docs/guides/fine-tuning/vision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI, ChatCompletion\n",
    "import json\n",
    "import os\n",
    "\n",
    "# client = OpenAI()\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as an env var>\"), base_url=\"http://localhost:8000/v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with a dataset of question-answer pairs on images of books from the [OCR-VQA dataset](https://ocr-vqa.github.io/), accessible through HuggingFace. This dataset contains 207,572 images of books with associated question-answer pairs inquiring about title, author, edition, year and genre of the book. In total, the dataset contains ~1M QA pairs. For the purposes of this guide, we will only use a small subset of the dataset to train, validate and test our model.\n",
    "\n",
    "We believe that this dataset will be well suited for fine-tuning on multimodal inputs as it requires the model to not only accurately identify relevant bounding boxes to extract key information, but also reason about the content of the image to answer the question correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# load dataset\u001b[39;00m\n\u001b[1;32m      4\u001b[0m ds \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhoward-hou/OCR-VQA\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# load dataset\n",
    "ds = load_dataset(\"howard-hou/OCR-VQA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll begin by sampling 150 training examples, 50 validation examples and 100 test examples. We will also explode the `questions` and `answers` columns to create a single QA pair for each row. Additionally, since our images are stored as byte strings, we'll convert them to images for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "# sample 150 training examples, 50 validation examples and 100 test examples\n",
    "ds_train = ds['train'].shuffle(seed=42).select(range(150))\n",
    "ds_val = ds['validation'].shuffle(seed=42).select(range(50))\n",
    "ds_test = ds['test'].shuffle(seed=42).select(range(100))\n",
    "\n",
    "# convert to pandas dataframe\n",
    "ds_train = ds_train.to_pandas()\n",
    "ds_val = ds_val.to_pandas()\n",
    "ds_test = ds_test.to_pandas()\n",
    "\n",
    "# convert byte strings to images\n",
    "ds_train['image'] = ds_train['image'].apply(lambda x: Image.open(BytesIO(x['bytes'])))\n",
    "ds_val['image'] = ds_val['image'].apply(lambda x: Image.open(BytesIO(x['bytes'])))\n",
    "ds_test['image'] = ds_test['image'].apply(lambda x: Image.open(BytesIO(x['bytes'])))\n",
    "\n",
    "# explode the 'questions' and 'answers' columns\n",
    "ds_train = ds_train.explode(['questions', 'answers'])\n",
    "ds_val = ds_val.explode(['questions', 'answers'])\n",
    "ds_test = ds_test.explode(['questions', 'answers'])\n",
    "\n",
    "# rename columns\n",
    "ds_train = ds_train.rename(columns={'questions': 'question', 'answers': 'answer'})\n",
    "ds_val = ds_val.rename(columns={'questions': 'question', 'answers': 'answer'})\n",
    "ds_test = ds_test.rename(columns={'questions': 'question', 'answers': 'answer'})\n",
    "\n",
    "# create unique ids for each example\n",
    "ds_train = ds_train.reset_index(drop=True)\n",
    "ds_val = ds_val.reset_index(drop=True)\n",
    "ds_test = ds_test.reset_index(drop=True)\n",
    "\n",
    "# select columns\n",
    "ds_train = ds_train[['question', 'answer', 'image']]\n",
    "ds_val = ds_val[['question', 'answer', 'image']]\n",
    "ds_test = ds_test[['question', 'answer', 'image']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect a random sample from the training set.\n",
    "\n",
    "In this example, the question prompts the model to determine the title of the book. In this case, the answer is quite ambiguous as there is the main title \"Patty's Patterns - Advanced Series Vol. 1 & 2\" as well as the subtitle \"100 Full-Page Patterns Value Bundle\" which are found in different parts of the image. Also, the name of the author here is not an individual, but a group called \"Penny Farthing Graphics\" which could be mistaken as part of the title.\n",
    "\n",
    "This type of task is typical in visual question answering, where the model must interpret complex images and provide accurate, context-specific responses. By training on these kinds of questions, we can enhance the model's ability to perform detailed image analysis across a variety of domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "# display a random training example\n",
    "print('QUESTION:', ds_train.iloc[198]['question'])\n",
    "display(ds_train.iloc[198]['image'])\n",
    "print('ANSWER:', ds_train.iloc[198]['answer'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure successful fine-tuning of our model, it’s crucial to properly structure the training data. Correctly formatting the data helps avoid validation errors during training and ensures the model can effectively learn from both text and image inputs. The good news is, this process is quite straightforward.\n",
    "\n",
    "Each example in the training dataset should be a conversation in the same format as the **Chat Completions API**. Specifically, this means structuring the data as a series of **messages**, where each message includes a `role` (such as \"user\" or \"assistant\") and the `content` of the message.\n",
    "\n",
    "Since we are working with both text and images for vision fine-tuning, we’ll construct these messages to include both content types. For each training sample, the question about the image is presented as a user message, and the corresponding answer is provided as an assistant message.\n",
    "\n",
    "Images can be included in one of two ways:\n",
    "* As **HTTP URLs**, referencing the location of the image.\n",
    "* As **data URLs** containing the image encoded in **base64**.\n",
    "\n",
    "Here’s an example of how the message format should look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"messages\": \n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Use the image to answer the question.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"What is the title of this book?\"},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": \"data:image/jpeg;base64,<encoded_image>\"}}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining the **system instructions** for our model. These instructions provide the model with important context, guiding how it should behave when processing the training data. Clear and concise system instructions are particularly useful to make sure the model reasons well on both text and images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "Generate an answer to the question based on the image of the book provided.\n",
    "Questions will include both open-ended questions and binary \"yes/no\" questions.\n",
    "The questions will inquire about the title, author, edition, year and genre of the book in the image.\n",
    "\n",
    "You will read the question and examine the corresponding image to provide an accurate answer.\n",
    "\n",
    "# Steps\n",
    "\n",
    "1. **Read the Question:** Carefully analyze the question to understand what information is being asked.\n",
    "2. **Examine the Image:**\n",
    "   - **Identify Relevant Bounding Boxes (if applicable):** For questions requiring specific details like the title or author, focus on the relevant areas or bounding boxes within the image to extract the necessary text. There may be multiple relevant bounding boxes in the image, so be sure to consider all relevant areas.\n",
    "   - **Analyze the Whole Image:** For questions that need general reasoning (e.g., \"Is this book related to Children's Books?\"), consider the entire image, including title, graphics, colors, and overall design elements.\n",
    "3. **Formulate a Reasoned Answer:**\n",
    "   - For binary questions (yes/no), use evidence from the image to support your answer.\n",
    "   - For open-ended questions, provide the exact text from the image or a concise phrase that best describes the requested information.\n",
    "\n",
    "# Output Format\n",
    "\n",
    "- Provide your answer in a concise and clear manner. Always return the final conclusion only, no additional text or reasoning.\n",
    "- If the question is binary, answer with \"Yes\" or \"No.\"\n",
    "- For open-ended questions requesting specific details (e.g., title, author), return the exact text from the image.\n",
    "- For questions about general attributes like \"genre,\" return a single word or phrase that best describes it.\n",
    "\n",
    "# Notes\n",
    "\n",
    "- Always prioritize accuracy and clarity in your responses.\n",
    "- If multiple authors are listed, return the first author listed.\n",
    "- If the information is not present in the image, try to reason about the question using the information you can gather from the image e.g. if the author is not listed, use the title and genre to find the author.\n",
    "- Ensure reasoning steps logically lead to the conclusions before stating your final answer.\n",
    "\n",
    "# Examples\n",
    "You will be provided with examples of questions and corresponding images of book covers, along with the reasoning and conclusion for each example. Use these examples to guide your reasoning process.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure our images are properly formatted for vision fine-tuning, they must be in **base64 format** and either **RGB or RGBA**. This ensures the model can accurately process the images during training. Below is a function that handles the encoding of images, while also converting them to the correct format if necessary.\n",
    "\n",
    "This function allows us to control the quality of the image encoding, which can be useful if we want to reduce the size of the file. 100 is the highest quality, and 1 is the lowest. The maximum file size for a fine-tuning job is 1GB, but we are unlikely to see improvements with a very large amount of training data. Nevertheless, we can use the `quality` parameter to reduce the size of the file if needed to accomodate file size limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def encode_image(image, quality=100):\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')  # Convert to RGB\n",
    "    buffered = BytesIO()\n",
    "    image.save(buffered, format=\"JPEG\", quality=quality) \n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also include **Few-Shot examples** from the training set as user and assistant messages to help guide the model's reasoning process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEW_SHOT_EXAMPLES = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"**Example 1:**\\n\\n**Question:** Who wrote this book?\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encode_image(ds_train.iloc[286]['image'], quality=50)}\"}}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"**Reasoning:** The cover clearly displays two authors' names, 'Evelyn M. Thomson' and 'Orlen N. Johnson,' at the bottom of the cover, with Evelyn M. Thomson listed first. Typically, the first-listed author is considered the primary author or main contributor.\\n\\n**Conclusion:** Evelyn Thomson\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"**Example 2:**\\n\\n**Question:** What is the title of this book?\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encode_image(ds_train.iloc[22]['image'], quality=50)}\"}}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"**Answer:**\\n\\n**Reasoning:** The cover prominently displays the title across the top and center of the image. The full title reads, 'Computer Systems: An Integrated Approach to Architecture and Operating Systems,' with each component of the title clearly separated and formatted to stand out.\\n\\n**Conclusion:** Computer Systems: An Integrated Approach to Architecture and Operating Systems\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"**Example 3:**\\n\\n**Question:** Is this book related to Children's Books?\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encode_image(ds_train.iloc[492]['image'], quality=50)}\"}}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"**Answer:**\\n\\n**Reasoning:** The cover illustration features a whimsical mermaid holding a red shoe, with gentle, child-friendly artwork that suggests it is targeted toward a young audience. Additionally, the style and imagery are typical of children's literature.\\n\\n**Conclusion:** Yes\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"**Example 4:**\\n\\n**Question:** Is this book related to History?\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encode_image(ds_train.iloc[68]['image'], quality=50)}\"}}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"**Answer:**\\n\\n**Reasoning:** The title 'Oliver Wendell Holmes, Jr.: Civil War Soldier, Supreme Court Justice' clearly indicates that this book focuses on the life of Oliver Wendell Holmes, Jr., providing a biographical account rather than a general historical analysis. Although it references historical elements (Civil War, Supreme Court), the primary focus is on the individual rather than historical events as a whole.\\n\\n**Conclusion:** No\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"**Example 5:**\\n\\n**Question:** What is the genre of this book?\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encode_image(ds_train.iloc[42]['image'], quality=50)}\"}}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"**Answer:**\\n\\n**Reasoning:** The cover prominently features an image of a train station and the title 'Railway Depots, Stations & Terminals,' which directly suggests a focus on railway infrastructure. This points to the book being related to topics within Engineering & Transportation.\\n\\n**Conclusion:** Engineering & Transportation\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"**Example 6:**\\n\\n**Question:** What type of book is this?\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encode_image(ds_train.iloc[334]['image'], quality=50)}\"}}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"**Answer:**\\n\\n**Reasoning:** The title 'Principles and Practice of Modern Chromatographic Methods' suggests a focus on chromatography, a scientific technique used in chemistry and biology. This aligns with the academic and technical nature typical of books in the 'Science & Math' category.\\n\\n**Conclusion:** Science & Math\"}\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our system instructions, few-shot examples, and the image encoding function in place, the next step is to iterate through the training set and construct the messages required for fine-tuning. As a reminder, each training example must be formatted as a conversation and must include both the image (in base64 format) and the corresponding question and answer.\n",
    "\n",
    "To fine-tune GPT-4o, we recommend providing at least **10 examples**, but you’ll typically see noticeable improvements with **50 to 100** training examples. In this case, we'll go all-in and fine-tune the model using our larger training sample of **150 images, and 721 QA pairs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# constructing the training set\n",
    "json_data = []\n",
    "\n",
    "for idx, example in tqdm(ds_train.iterrows()):\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": SYSTEM_PROMPT}]\n",
    "    }\n",
    "    \n",
    "    user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": f\"Question [{idx}]: {example['question']}\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encode_image(example['image'], quality=50)}\"}}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    assistant_message = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": example[\"answer\"]}]\n",
    "    }\n",
    "\n",
    "    all_messages = [system_message] + FEW_SHOT_EXAMPLES + [user_message, assistant_message]\n",
    "    \n",
    "    json_data.append({\"messages\": all_messages})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save our final training set in a `.jsonl` file where each line in the file represents a single example in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the JSON data to a file\n",
    "with open(\"ocr-vqa-train.jsonl\", \"w\") as f:\n",
    "    for message in json_data:\n",
    "        json.dump(message, f)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the training set, we need to structure our validation and test sets in the same message format. However, for the test set, there's a key difference: since the test set is used for evaluation, we do not include the assistant's message (i.e., the answer). This ensures the model generates its own answers, which we can later compare to the ground truth for performance evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing the validation set\n",
    "json_data = []\n",
    "\n",
    "for idx, example in tqdm(ds_val.iterrows()):\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": SYSTEM_PROMPT}]\n",
    "    }\n",
    "    \n",
    "    user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": f\"Question [{idx}]: {example['question']}\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encode_image(example['image'], quality=50)}\"}}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    assistant_message = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": example[\"answer\"]}]\n",
    "    }\n",
    "\n",
    "    all_messages = [system_message] + FEW_SHOT_EXAMPLES + [user_message, assistant_message]\n",
    "    \n",
    "    json_data.append({\"messages\": all_messages})\n",
    "\n",
    "# save the JSON data to a file\n",
    "with open(\"ocr-vqa-validation.jsonl\", \"w\") as f:\n",
    "    for message in json_data:\n",
    "        json.dump(message, f)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing the test set\n",
    "json_data = []\n",
    "\n",
    "for idx, example in tqdm(ds_test.iterrows()):\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": SYSTEM_PROMPT}]\n",
    "    }\n",
    "    \n",
    "    user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": f\"Question [{idx}]: {example['question']}\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encode_image(example['image'], quality=50)}\"}}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    all_messages = [system_message] + FEW_SHOT_EXAMPLES + [user_message]\n",
    "    \n",
    "    json_data.append({\"messages\": all_messages})\n",
    "\n",
    "# save the JSON data to a file\n",
    "with open(\"ocr-vqa-test.jsonl\", \"w\") as f:\n",
    "    for message in json_data:\n",
    "        json.dump(message, f)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have prepared our training and validation datasets in the right format, we can upload them using the [Files API](https://platform.openai.com/docs/api-reference/files/create) for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload training file\n",
    "train_file = client.files.create(\n",
    "  file=open(\"ocr-vqa-train.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "# upload validation file\n",
    "val_file = client.files.create(\n",
    "  file=open(\"ocr-vqa-validation.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the files are uploaded, we're ready to proceed to the next step: starting the fine-tuning job.\n",
    "\n",
    "To create a fine-tuning job, we use the fine-tuning API. This may take some time to complete, but you can track the progress of the fine-tuning job in the [Platform UI](https://platform.openai.com/finetune/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create fine tuning job\n",
    "file_train = train_file.id\n",
    "file_val = val_file.id\n",
    "\n",
    "client.fine_tuning.jobs.create(\n",
    "  training_file=file_train,\n",
    "  # note: validation file is optional\n",
    "  validation_file=file_val,\n",
    "  model=\"gpt-4o-2024-08-06\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the fine-tuning job is complete, it’s time to evaluate the performance of our model by running inference on the test set. This step involves using the fine-tuned model to generate responses to the questions in the test set and comparing its predictions to the ground truth answers for evaluation. We will also run inference on the test set using the non-fine-tuned GPT-4o model for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import re\n",
    "\n",
    "# load the test data from JSONL file\n",
    "test_data = []\n",
    "with open(\"ocr-vqa-test.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        test_data.append(json.loads(line))\n",
    "\n",
    "def process_example(example, model):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=example[\"messages\"],\n",
    "        store=True,\n",
    "        metadata={'dataset': 'ocr-vqa-test'}\n",
    "    )\n",
    "    predicted_answer = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # regex to get the question ID\n",
    "    match = re.search(r'\\[(\\d+)\\]', example[\"messages\"][-1][\"content\"][0][\"text\"])\n",
    "    if match:\n",
    "        example_id = int(match.group(1))\n",
    "    else:\n",
    "        example_id = -1\n",
    "    \n",
    "    actual_answer = ds_test.iloc[example_id]['answer']\n",
    "\n",
    "    return {\n",
    "        \"example_id\": example_id,\n",
    "        \"predicted_answer\": predicted_answer,\n",
    "        \"actual_answer\": actual_answer\n",
    "    }\n",
    "\n",
    "# run the prompts through the finetuned model and store the results\n",
    "model = \"ft:gpt-4o-2024-08-06:openai::AOY1M8VG\"\n",
    "results = []\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = {executor.submit(process_example, example, model): example for example in test_data}\n",
    "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "        results.append(future.result())\n",
    "\n",
    "# save the results to a file\n",
    "with open(\"ocr-vqa-ft-results.jsonl\", \"w\") as f:\n",
    "    for result in results:\n",
    "        json.dump(result, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# run the prompts through the non-fine-tuned model and store the results\n",
    "model = \"gpt-4o\"\n",
    "results = []\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = {executor.submit(process_example, example, model): example for example in test_data}\n",
    "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "        results.append(future.result())\n",
    "\n",
    "# save the results to a file\n",
    "with open(\"ocr-vqa-4o-results.jsonl\", \"w\") as f:\n",
    "    for result in results:\n",
    "        json.dump(result, f)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we’ve run inference using our fine-tuned model, let’s inspect a few specific examples to understand how well the model performed compared to the actual answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: What is the title of this book?\n",
    "{\"example_id\": 6, \"predicted_answer\": \"A Wrinkle in Time\", \"actual_answer\": \"A Wrinkle in Time (Time Quintet)\"}\n",
    "# Q: Who wrote this book?\n",
    "{\"example_id\": 10, \"predicted_answer\": \"DK Travel\", \"actual_answer\": \"DK Publishing\"}\n",
    "# Q: What is the title of this book?\n",
    "{\"example_id\": 11, \"predicted_answer\": \"DK Eyewitness Travel Guide: Peru\", \"actual_answer\": \"DK Eyewitness Travel Guide: Peru\"}\n",
    "# Q: What type of book is this?\n",
    "{\"example_id\": 12, \"predicted_answer\": \"Travel\", \"actual_answer\": \"Travel\"}\n",
    "# Q: Who wrote this book?\n",
    "{\"example_id\": 437, \"predicted_answer\": \"Cookshack, Inc.\", \"actual_answer\": \"Cookshack\"}\n",
    "# Q: What type of book is this?\n",
    "{\"example_id\": 482, \"predicted_answer\": \"Christian Books & Bibles\", \"actual_answer\": \"Religion & Spirituality\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the fine-tuned model does a great job at answering the questions, with many responses being exactly correct. \n",
    "\n",
    "However, there are also cases where the model’s **predicted answers** are close to the **ground truth**, while not matching exactly, particularly in open-ended questions where phrasing or details may differ. To assess the quality of these predictions, we will use GPT-4o to evaluate the similarity between the predicted responses and the ground truth labels from the dataset.\n",
    "\n",
    "In order to evaluate our model responses, we will use GPT-4o to determine the similarity between the ground truth and our predicted responses. We will rank our predicted answers based on the following criteria:\n",
    "* **Very Similar**: The predicted answer exactly matches the ground truth and there is no important information omitted, although there may be some minor ommissions or discrepancies in punctuation.\n",
    "\n",
    "* **Mostly Similar**: The predicted answer closely aligns with the ground truth, perhaps with some missing words or phrases.\n",
    "\n",
    "* **Somewhat Similar**: Although the predicted answer has noticeable differences to the ground truth, the core content is accurate and semantically similar, perhaps with some missing information.\n",
    "\n",
    "* **Incorrect**: The predicted answer is completely incorrect, irrelevant, or contains critical errors or omissions from the ground truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# define output schema\n",
    "class Result(BaseModel):\n",
    "    example_id: int = Field(description=\"The unique ID of the question\")\n",
    "    rating: str = Field(description=\"The assigned similarity rating. One of [Very Similar | Mostly Similar | Somewhat Similar | Incorrect]\")\n",
    "    type: str = Field(description=\"The type of question. Open if the question is binary yes/no, otherwise Closed. One of [Open | Closed]\")\n",
    "\n",
    "EVAL_PROMPT = \"\"\"\n",
    "Evaluate the closeness between the predicted answer and the ground truth for each provided result.\n",
    "Rank the predicted answer based on the following criteria:\n",
    "\n",
    "1. **Very Similar**: The predicted answer exactly matches the ground truth and there is no important information omitted, although there may be some minor ommissions or discrepancies in punctuation.\n",
    "2. **Mostly Similar**: The predicted answer closely aligns with the ground truth, perhaps with some missing words or phrases.\n",
    "3. **Somewhat Similar**: Although the predicted answer has noticeable differences to the ground truth, the core content is accurate and semantically similar, perhaps with some missing information.\n",
    "4. **Incorrect**: The predicted answer is completely incorrect, irrelevant, or contains critical errors or omissions from the ground truth.\n",
    "\n",
    "Ensure to consider both open-ended and yes/no questions.\n",
    "\n",
    "# Steps\n",
    "1. **Analyze the Answers**: Read the predicted answer, and ground truth carefully.\n",
    "2. **Evaluate Similarity**:\n",
    "    - Check if the predicted answer contains the same core information and correctness as the ground truth.\n",
    "    - Determine if there are any important omissions or errors.\n",
    "3. **Assign a Rating**: Based on your evaluation, assign the appropriate rating: Very Similar, Mostly Similar, Somewhat Similar, or Incorrect.\n",
    "\n",
    "# Output Format\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"example_id\": [example_id],\n",
    "        \"rating\": \"[Very Similar | Mostly Similar | Somewhat Similar | Incorrect]\",\n",
    "        \"type\": \"[Open | Closed]\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "# Examples\n",
    "\n",
    "**Input:**\n",
    "```json\n",
    "{\"example_id\": 6, \"predicted_answer\": \"A Wrinkle in Time\", \"actual_answer\": \"A Wrinkle in Time (Time Quintet)\"}\n",
    "```\n",
    "**Reasoning:**\n",
    "The predicted answer \"A Wrinkle in Time\" is a very close match to the ground truth \"A Wrinkle in Time (Time Quintet)\" with a missing tagline or subtitle.\n",
    "**Output:**\n",
    "```json\n",
    "{ \"example_id\": 6, \"rating\": \"Mostly Similar\", \"type\": \"Open\" }\n",
    "```\n",
    "\n",
    "**Input:**\n",
    "```json\n",
    "{\"example_id\": 437, \"predicted_answer\": \"Cookshack, Inc.\", \"actual_answer\": \"Cookshack\"}\n",
    "```\n",
    "**Reasoning:**\n",
    "The predicted answer \"Cookshack, Inc.\" is exactly the same as the ground truth \"Cookshack\", with only a difference in punctuation.\n",
    "**Output:**\n",
    "```json\n",
    "{ \"example_id\": 437, \"rating\": \"Very Similar\", \"type\": \"Open\" }\n",
    "```\n",
    "\n",
    "**Input:**\n",
    "```json\n",
    "{\"example_id\": 482, \"predicted_answer\": \"Christian Books & Bibles\", \"actual_answer\": \"Religion & Spirituality\"}\n",
    "```\n",
    "**Reasoning:**\n",
    "The predicted answer \"Christian Books & Bibles\" is semantically similar to the ground truth \"Religion & Spirituality\", however there is a key difference in the predicted answer.\n",
    "**Output:**\n",
    "```json\n",
    "{ \"example_id\": 482, \"rating\": \"Somewhat Similar\", \"type\": \"Open\" }\n",
    "```\n",
    "\n",
    "**Input:**\n",
    "```json\n",
    "{ \"example_id\": 417, \"predicted_answer\": \"yes\", \"actual_answer\": \"no\" }\n",
    "```\n",
    "**Reasoning:**\n",
    "The predicted answer \"yes\" is completely incorrect compared to the actual answer \"no.\"\n",
    "**Output:**\n",
    "```json\n",
    "{ \"example_id\": 417, \"rating\": \"Incorrect\", \"type\": \"Closed\" }\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "def process_result(result):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": EVAL_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": str(result)\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model='gpt-4o',\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "        response_format=Result\n",
    "    )\n",
    "\n",
    "    return json.loads(response.choices[0].message.content)\n",
    "\n",
    "# fine-tuned model results with scores\n",
    "results = []\n",
    "with open(\"ocr-vqa-ft-results.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        results.append(json.loads(line))\n",
    "\n",
    "results_w_scores = []\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = {executor.submit(process_result, result): result for result in results}\n",
    "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "        results_w_scores.append(future.result())\n",
    "\n",
    "# Save the results to a file\n",
    "with open(\"ocr-vqa-ft-similarity.jsonl\", \"w\") as f:\n",
    "    for score in results_w_scores:\n",
    "        json.dump(score, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# non-fine-tuned model results with scores\n",
    "results = []\n",
    "with open(\"ocr-vqa-4o-results.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        results.append(json.loads(line))\n",
    "\n",
    "results_w_scores_4o = []\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = {executor.submit(process_result, result): result for result in results}\n",
    "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "        results_w_scores_4o.append(future.result())\n",
    "\n",
    "# Save the results to a file\n",
    "with open(\"ocr-vqa-4o-similarity.jsonl\", \"w\") as f:\n",
    "    for score in results_w_scores_4o:\n",
    "        json.dump(score, f)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fully understand the impact of fine-tuning, we also evaluated the same set of test questions using the **non-fine-tuned GPT-4o** model.\n",
    "\n",
    "Let's start by comparing the performance of the fine-tuned model vs the non-fine-tuned model for **Closed** form (Yes/No) questions.\n",
    "\n",
    "Note that with the fine-tuned model, we can check for exact matches between the predicted and actual answers because the model has learned to produce consistent answers that follow the response format specified in the system prompt. However, for the non-fine-tuned model, we need to account for variations in phrasing and wording in the predicted answers. Below is an example of a non-fine-tuned model output. As we can see, the final answer is correct but the response format is inconsistent and outputs reasoning in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of non-fine-tuned model output\n",
    "{\"example_id\": 14, \"predicted_answer\": \"**Answer:**\\n\\nNo. \\n\\n**Reasoning:** The cover shows \\\"Eyewitness Travel\\\" and \\\"Peru,\\\" indicating it is a travel guide focused on the country, rather than a pharmaceutical book.\", \"actual_answer\": \"No\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in results\n",
    "results_ft = []\n",
    "with open(\"ocr-vqa-ft-results.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        results_ft.append(json.loads(line))\n",
    "\n",
    "results_4o = []\n",
    "with open(\"ocr-vqa-4o-results.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        results_4o.append(json.loads(line))\n",
    "\n",
    "# filter results for yes/no questions\n",
    "results_ft_closed = [result for result in results_ft if result['actual_answer'] in ['Yes', 'No']]\n",
    "results_4o_closed = [result for result in results_4o if result['actual_answer'] in ['Yes', 'No']]\n",
    "\n",
    "# check for correct predictions\n",
    "correct_ft_closed = [result for result in results_ft_closed if result['predicted_answer'] == result['actual_answer']]\n",
    "correct_4o_closed = [\n",
    "    result for result in results_4o_closed \n",
    "    if result['predicted_answer'].lower() == result['actual_answer'].lower() \n",
    "    or result['actual_answer'].lower() in result['predicted_answer'].lower()\n",
    "]\n",
    "print(f\"Fine-tuned model accuracy: {round(100*len(correct_ft_closed) / len(results_ft_closed), 2)}%\")\n",
    "print(f\"Non-fine-tuned model accuracy: {round(100*len(correct_4o_closed) / len(results_4o_closed), 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a generous allowance for variations in phrasing and wording for the non-fine-tuned model including ignoring case and allowing for partial matches, the fine-tuned model still outperforms the non-fine-tuned model by a margin of **2.64%** on this set of questions.\n",
    "\n",
    "Now, let's compare the performance of the fine-tuned model vs the non-fine-tuned model over all the open-ended questions. First, we'll check for exact matches between the predicted and actual answers, again allowing for general variations in phrasing and wording for the non-fine-tuned model, but maintaining a strict standard for the fine-tuned model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter results for open-ended questions\n",
    "results_ft_open = [result for result in results_ft if result['actual_answer'] not in ['Yes', 'No']]\n",
    "results_4o_open = [result for result in results_4o if result['actual_answer'] not in ['Yes', 'No']]\n",
    "\n",
    "# check for correct predictions\n",
    "correct_ft_open = [result for result in results_ft_open if result['predicted_answer'] == result['actual_answer']]\n",
    "correct_4o_open = [\n",
    "    result for result in results_4o_open \n",
    "    if result['predicted_answer'].lower() == result['actual_answer'].lower() \n",
    "    or result['actual_answer'].lower() in result['predicted_answer'].lower()\n",
    "]\n",
    "print(f\"Fine-tuned model accuracy: {round(100*len(correct_ft_open) / len(results_ft_open), 2)}%\")\n",
    "print(f\"Non-fine-tuned model accuracy: {round(100*len(correct_4o_open) / len(results_4o_open), 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The improvement in accuracy here is much more pronounced, with the fine-tuned model outperforming the non-fine-tuned model by a substantial margin of **17.97%**, even with very generous allowances for variations in phrasing and wording for the non-fine-tuned model!\n",
    "\n",
    "If we were to afford the same leniency to the fine-tuned model, we would see an additional 4.1% increase in accuracy, bringing the total margin of improvement to **22.07%**.\n",
    "\n",
    "To dig a little deeper, we can also look at the accuracy by question type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# seperate by question type\n",
    "def get_question_type(question):\n",
    "    if question in [\"What is the title of this book?\"]:\n",
    "        return \"Title\"\n",
    "    elif question in [\"What is the genre of this book?\", \"What type of book is this?\"]:\n",
    "        return \"Genre\"\n",
    "    elif question in [\"Who wrote this book?\", \"Who is the author of this book?\"]:\n",
    "        return \"Author\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "# get index numbers for each question type\n",
    "question_type_indexes = {\n",
    "    \"Title\": [],\n",
    "    \"Genre\": [],\n",
    "    \"Author\": [],\n",
    "    \"Other\": []\n",
    "}\n",
    "\n",
    "for idx, row in ds_test.iterrows():\n",
    "    question_type = get_question_type(row['question'])\n",
    "    question_type_indexes[question_type].append(idx)\n",
    "\n",
    "# plot accuracy by question type]\n",
    "accuracy_by_type_ft = {}\n",
    "accuracy_by_type_4o = {}\n",
    "\n",
    "for question_type, indexes in question_type_indexes.items():\n",
    "    correct_predictions_ft = [\n",
    "        result for result in results_ft if result['example_id'] in indexes and (\n",
    "            result['predicted_answer'].lower() == result['actual_answer'].lower() or\n",
    "            result['actual_answer'].lower() in result['predicted_answer'].lower()\n",
    "        )\n",
    "    ]\n",
    "    correct_predictions_4o = [\n",
    "        result for result in results_4o if result['example_id'] in indexes and (\n",
    "            result['predicted_answer'].lower() == result['actual_answer'].lower() or\n",
    "            result['actual_answer'].lower() in result['predicted_answer'].lower()\n",
    "        )\n",
    "    ]\n",
    "    accuracy_ft = len(correct_predictions_ft) / len(indexes) if indexes else 0\n",
    "    accuracy_4o = len(correct_predictions_4o) / len(indexes) if indexes else 0\n",
    "    accuracy_by_type_ft[question_type] = accuracy_ft * 100 \n",
    "    accuracy_by_type_4o[question_type] = accuracy_4o * 100\n",
    "\n",
    "# prepare data for plotting\n",
    "question_types = list(accuracy_by_type_ft.keys())\n",
    "accuracies_ft = list(accuracy_by_type_ft.values())\n",
    "accuracies_4o = list(accuracy_by_type_4o.values())\n",
    "\n",
    "# plot grouped bar chart\n",
    "bar_width = 0.35\n",
    "index = range(len(question_types))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bar1 = plt.bar(index, accuracies_ft, bar_width, label='Fine-tuned GPT-4o', color='skyblue')\n",
    "bar2 = plt.bar([i + bar_width for i in index], accuracies_4o, bar_width, label='Non-fine-tuned GPT-4o', color='lightcoral')\n",
    "\n",
    "plt.xlabel('Question Type')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Accuracy by Question Type')\n",
    "plt.ylim(0, 100)\n",
    "plt.xticks([i + bar_width / 2 for i in index], question_types, rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the largest performance gains for the fine-tuned model are for questions in the **Genre** category e.g. \"What type of book is this?\" or \"What is the genre of this book?\". This might be indicative of the benefits of fine-tuning in general in that we teach the model to classify genres based on the categories present in the training data. However, it also highlights the model's strong visual undserstanding capabilties, since we are able to identify the genre based on the visual content of the book cover alone.\n",
    "\n",
    "Additionally, we see significant lift in the **Title** category, which suggests that fine-tuning has boosted the model's OCR capbilities and its ability to understand the layout and structure of the book cover to extract the relevant information.\n",
    "\n",
    "Finally, let's compare the distribution of similarity ratings between the fine-tuned model and the non-fine-tuned model to allow for variations in phrasing and wording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# extract ratings\n",
    "ratings_ft = [result['rating'] for result in results_w_scores if result['type'] == 'Open']\n",
    "ratings_4o = [result['rating'] for result in results_w_scores_4o if result['type'] == 'Open']\n",
    "\n",
    "# count occurrences of each rating\n",
    "rating_counts_ft = Counter(ratings_ft)\n",
    "rating_counts_4o = Counter(ratings_4o)\n",
    "\n",
    "# define the order of ratings\n",
    "rating_order = [\"Very Similar\", \"Mostly Similar\", \"Somewhat Similar\", \"Incorrect\"]\n",
    "\n",
    "# create bar chart\n",
    "bar_width = 0.35\n",
    "index = range(len(rating_order))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bar1 = ax.bar(index, [rating_counts_ft.get(rating, 0) for rating in rating_order], bar_width, label='FT GPT-4o')\n",
    "bar2 = ax.bar([i + bar_width for i in index], [rating_counts_4o.get(rating, 0) for rating in rating_order], bar_width, label='GPT-4o')\n",
    "\n",
    "ax.set_xlabel('Ratings')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Ratings Distribution')\n",
    "ax.set_xticks([i + bar_width / 2 for i in index])\n",
    "ax.set_xticklabels(rating_order)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results provide a clear picture of the benefits gained through fine-tuning, without any other modifications.\n",
    "Comparing the distribution of ratings between the **fine-tuned GPT-4o** model and **GPT-4o without fine-tuning**, we see that the fine-tuned model gets many more responses exactly correct, with a comparable amount of incorrect responses.\n",
    "### Key Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Improved Precision**: Fine-tuning helped the model produce more precise answers that matched the ground truth, especially in highly domain-specific tasks like OCR on book covers.\n",
    "* **Better Generalization**: While the non-fine-tuned GPT-4o was able to get at least somewhat to the ground truth for many questions, it was less consistent. The fine-tuned model exhibited better generalization across a variety of test questions, thanks to the exposure to multimodal data during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the results from vision fine-tuning are promising, there are still opportunities for improvement. Much like fine-tuning on text, the effectiveness of vision fine-tuning depends heavily on the **quality, diversity, and representativeness** of the training data. In particular, models benefit from focusing on cases where errors occur most frequently, allowing for targeted improvements.\n",
    "\n",
    "Upon reviewing the incorrect results, many of the \"Incorrect\" responses from the fine-tuned model are in fact due to inconsistencies in the labels from the dataset. For example, some ground truth answers provide only the first and last name of the author, whereas the image actually shows the middle initial as well. Similarly, some ground truth labels for the title include subheadings and taglines, whereas others do not. \n",
    "\n",
    "Another common theme was miscategorization of genres. Although the model was almost always able to produce a semantically similar genre to the ground truth, the answer sometimes deviated. This is likely due to the lack of presence of these genres in the training data. Providing the model with more diverse training examples to cover these genres, or clearer instructions for dealing with edge cases can help to guide the model’s understanding.\n",
    "\n",
    "### Next Steps:\n",
    "* **Expand the Training Dataset**: Adding more varied examples that cover the model’s weaker areas, such as identifying genres, could significantly enhance performance.\n",
    "\n",
    "* **Expert-Informed Prompts**: Incorporating domain-specific instructions into the training prompts may further refine the model’s ability to accurately interpret and respond in complex cases.\n",
    "\n",
    "Although there is still some progress to be made on this particular task, the initial results are highly encouraging. With minimal setup and effort, we’ve already observed a substantial uplift in overall accuracy with vision fine-tuning, indicating that this approach holds great potential. Vision fine-tuning opens up possibilities for improvement across a wide range of visual question answering tasks, as well as other tasks that rely on strong visual understanding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
