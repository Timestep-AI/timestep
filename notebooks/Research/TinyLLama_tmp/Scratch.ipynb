{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tinygrad and CUDA device.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "from os import getenv\n",
    "\n",
    "from tqdm import tqdm\n",
    "import mlflow\n",
    "import openai\n",
    "import pandas as pd\n",
    "# from sentencepiece import SentencePieceProcessor\n",
    "from tinygrad import Device, Tensor, nn, Variable, GlobalCounters\n",
    "from tinygrad.helpers import Context, Timing, Profiling, DEBUG, JIT, getenv, colored\n",
    "from tinygrad.nn.state import get_state_dict, safe_load, torch_load, load_state_dict, get_parameters, safe_save\n",
    "from transformers import AutoTokenizer, AutoConfig, DataCollatorWithPadding, pipeline, Trainer, TrainingArguments\n",
    "\n",
    "# from notebooks.Research.lib.transformers.src.transformers.models.llama.modeling_tinygrad_llama import TinygradLlamaForCausalLM\n",
    "# from notebooks.Research.tinygrad.train_llama import function_calling_template\n",
    "from timestep.config import settings\n",
    "\n",
    "from src.transformers.models.llama.modeling_tinygrad_llama import TinygradLlamaForCausalLM\n",
    "\n",
    "device = Device.DEFAULT\n",
    "print(f'Using Tinygrad and {device} device.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bos_token=\"<|begin_of_text|>\"\n",
    "bos_token=\"<s>\"\n",
    "bos_token_id=1\n",
    "# eos_token=\"<|eot_id|>\"\n",
    "eos_token=\"</s>\"\n",
    "eos_token_id=2\n",
    "pad_token=eos_token\n",
    "pad_token_id=eos_token_id\n",
    "# unk_token=\"<|unk_id|>\"\n",
    "unk_token=\"<unk>\"\n",
    "unk_token_id=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_repo_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_name_or_path\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 2048,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 5632,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 22,\n",
       "  \"num_key_value_heads\": 4,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.44.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(hf_repo_id, bos_token_id=bos_token_id, eos_token_id=eos_token_id)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ram used:  2.20 GB, freqs_cis                                         : 100%|â–ˆ| \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded weights in 2206.74 ms, 2.20 GB loaded at 1.00 GB/s\n"
     ]
    }
   ],
   "source": [
    "# model_path = os.path.join(settings.app_dir, f\"models/{hf_repo_id}/model.safetensors\")\n",
    "model_path: Path = Path(settings.app_dir) / f\"models/{hf_repo_id}/model.safetensors\"\n",
    "# tokenizer_path = os.path.join(settings.app_dir, f\"models/{hf_repo_id}/tokenizer.model\")\n",
    "\n",
    "# model = TinygradLlamaForCausalLM.from_pretrained(hf_repo_id)\n",
    "model = TinygradLlamaForCausalLM(\n",
    "    config=config,\n",
    "    device=device,\n",
    "    model_path=model_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bos_token_id: 1\n",
      "eos_token_id: 2\n",
      "pad_token_id: None\n"
     ]
    }
   ],
   "source": [
    "print('bos_token_id:', model.config.bos_token_id)\n",
    "print('eos_token_id:', model.config.eos_token_id)\n",
    "print('pad_token_id:', model.config.pad_token_id)\n",
    "# print('unk_token_id:', model.config.unk_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bos_token: <s>\n",
      "eos_token: </s>\n",
      "pad_token: </s>\n",
      "unk_token: <unk>\n",
      "bos_token_id: 1\n",
      "eos_token_id: 2\n",
      "pad_token_id: 2\n",
      "unk_token_id: 0\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    hf_repo_id,\n",
    "    # bos_token=\"<|begin_of_text|>\",\n",
    "    bos_token=bos_token,\n",
    "    clean_up_tokenization_spaces=True,\n",
    "    eos_token=eos_token,\n",
    "    # pad_token='<|im_end|>',\n",
    "    # pad_token=pad_token,\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print('bos_token:', tokenizer.bos_token)\n",
    "print('eos_token:', tokenizer.eos_token)\n",
    "print('pad_token:', tokenizer.pad_token)\n",
    "print('unk_token:', tokenizer.unk_token)\n",
    "\n",
    "print('bos_token_id:', tokenizer.bos_token_id)\n",
    "print('eos_token_id:', tokenizer.eos_token_id)\n",
    "print('pad_token_id:', tokenizer.pad_token_id)\n",
    "print('unk_token_id:', tokenizer.unk_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokenizer.bos_token_id == model.config.bos_token_id, f'{tokenizer.bos_token_id} != {model.config.bos_token_id}'\n",
    "assert tokenizer.eos_token_id == model.config.eos_token_id, f'{tokenizer.eos_token_id} != {model.config.eos_token_id}'\n",
    "# assert tokenizer.pad_token_id == model.config.pad_token_id, f'{tokenizer.pad_token_id} != {model.config.pad_token_id}'\n",
    "# assert tokenizer.unk_token_id == model.config.unk_token_id, f'{tokenizer.unk_token_id} != {model.config.unk_token_id}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tiktoken \n",
    "\n",
    "# cl100k_base = tiktoken.get_encoding(\"cl100k_base\") \n",
    "\n",
    "# enc = tiktoken.Encoding( \n",
    "#     name=\"gpt-35-turbo\",  \n",
    "#     pat_str=cl100k_base._pat_str, \n",
    "#     mergeable_ranks=cl100k_base._mergeable_ranks, \n",
    "#     special_tokens={ \n",
    "#         **cl100k_base._special_tokens, \n",
    "#         \"<|im_start|>\": 100264, \n",
    "#         \"<|im_end|>\": 100265\n",
    "#     } \n",
    "# ) \n",
    "\n",
    "# tokens = enc.encode( \n",
    "#     \"<|im_start|>user\\nHello<|im_end|><|im_start|>assistant\",  \n",
    "#     allowed_special={\"<|im_start|>\", \"<|im_end|>\"} \n",
    "# ) \n",
    "\n",
    "# assert len(tokens) == 7 \n",
    "# assert tokens == [100264, 882, 198, 9906, 100265, 100264, 78191]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{%- for message in messages %}<|im_start|>{{- message.role }}\n",
      "{% if message.role == 'system' %}{{ message.content }}{% if tool_calls %}\n",
      "\n",
      "You have access to the following functions:\n",
      "{% for tool in tools %}\n",
      "functions.{{ tool.function.name }}:\n",
      "{{ tool.function.parameters | tojson }}\n",
      "{% endfor %}\n",
      "\n",
      "You can respond to user messages with either a single message or one or more function calls.\n",
      "\n",
      "To respond with a message begin the message with \"message:\", use the following format:\n",
      "\n",
      "message:\n",
      "<message>\n",
      "\n",
      "To respond with one or more function calls begin the message with 'functions.<function_name>:', use the following format:\n",
      "\n",
      "functions.<function_name>:\n",
      "{ \"arg1\": \"value1\", \"arg2\": \"value2\" }\n",
      "functions.<function_name>:\n",
      "{ \"arg1\": \"value1\", \"arg2\": \"value2\" }\n",
      "When responding with function calls, only output the function calls, do not include any additional text.{% endif %}</s>\n",
      "{% endif %}{% if message.role == 'user' %}{{ message.content }}</s>\n",
      "{% endif %}{% if message.role == 'assistant' %}{% if message.content and message.content | length > 0 %}{% if tool_calls %}message:\n",
      "{% endif %}{{ message.content }}</s>\n",
      "{% endif %}{% if 'tool_calls' in message %}{% for tool_call in message.tool_calls %}functions.{{ tool_call.function.name }}:\n",
      "{{ tool_call.function.arguments }}{% endfor %}</s>\n",
      "{% endif %}{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n",
      "{% endif %}\n"
     ]
    }
   ],
   "source": [
    "# Copied and modified from https://github.com/abetlen/llama-cpp-python/blob/658b244c5aa924fc6f4d04f92445dd8f724b6017/llama_cpp/llama_chat_format.py#L3345\n",
    "# See also https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/chat-markup-language\n",
    "function_calling_template = (\n",
    "  \"{%- for message in messages %}\"\n",
    "  \"<|im_start|>{{- message.role }}\\n\"\n",
    "  # System message\n",
    "  \"{% if message.role == 'system' %}\"\n",
    "  \"{{ message.content }}\"\n",
    "  \"{% if tool_calls %}\"\n",
    "  \"\\n\\nYou have access to the following functions:\\n\"\n",
    "  \"{% for tool in tools %}\"\n",
    "  \"\\nfunctions.{{ tool.function.name }}:\\n\"\n",
    "  \"{{ tool.function.parameters | tojson }}\"\n",
    "  \"\\n{% endfor %}\"\n",
    "  \"\\n\\nYou can respond to user messages with either a single message or one or more function calls.\"\n",
    "  \"\\n\\nTo respond with a message begin the message with \\\"message:\\\", use the following format:\"\n",
    "  \"\\n\\nmessage:\"\n",
    "  \"\\n<message>\"\n",
    "  \"\\n\\nTo respond with one or more function calls begin the message with 'functions.<function_name>:', use the following format:\"\n",
    "  \"\\n\\nfunctions.<function_name>:\"\n",
    "  '\\n{ \"arg1\": \"value1\", \"arg2\": \"value2\" }'\n",
    "  \"\\nfunctions.<function_name>:\"\n",
    "  '\\n{ \"arg1\": \"value1\", \"arg2\": \"value2\" }'\n",
    "  \"\\nWhen responding with function calls, only output the function calls, do not include any additional text.\"\n",
    "  \"{% endif %}\"\n",
    "  \"<|im_end|>\\n\"\n",
    "  \"{% endif %}\"\n",
    "  # User message\n",
    "  \"{% if message.role == 'user' %}\"\n",
    "  \"{{ message.content }}\"\n",
    "  \"<|im_end|>\\n\"\n",
    "  \"{% endif %}\"\n",
    "  # Assistant message\n",
    "  \"{% if message.role == 'assistant' %}\"\n",
    "  ## Reglar message\n",
    "  \"{% if message.content and message.content | length > 0 %}\"\n",
    "  \"{% if tool_calls %}\"\n",
    "  \"message:\\n\"\n",
    "  \"{% endif %}\"\n",
    "  \"{{ message.content }}\"\n",
    "  \"<|im_end|>\\n\"\n",
    "  \"{% endif %}\"\n",
    "  ## Function calls\n",
    "  \"{% if 'tool_calls' in message %}\"\n",
    "  \"{% for tool_call in message.tool_calls %}\"\n",
    "  \"functions.{{ tool_call.function.name }}:\\n\"\n",
    "  \"{{ tool_call.function.arguments }}\"\n",
    "  \"{% endfor %}\"\n",
    "  \"<|im_end|>\\n\"\n",
    "  \"{% endif %}\"\n",
    "  \"{% endif %}\"\n",
    "  \"{% endfor %}\"\n",
    "  \"{% if add_generation_prompt %}<|im_start|>assistant\\n{% endif %}\"\n",
    ")\n",
    "\n",
    "# function_calling_template = function_calling_template.replace(\"<|im_start|>\", tokenizer.bos_token)\n",
    "function_calling_template = function_calling_template.replace(\"<|im_end|>\", tokenizer.eos_token)\n",
    "\n",
    "print(function_calling_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- set date_string = \"26 Jul 2024\" %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message + builtin tools #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if builtin_tools is defined or tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{%- if builtin_tools is defined %}\n",
      "    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"</s>\n",
      "\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"</s>\n",
      "\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim }}\n",
      "        {%- if not loop.last or add_generation_prompt %}\n",
      "            {{- '</s>\n",
      "' }}\n",
      "        {%- endif %}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n",
      "            {%- for arg_name, arg_val in tool_call.arguments | items %}\n",
      "                {{- arg_name + '=\"' + arg_val + '\"' }}\n",
      "                {%- if not loop.last %}\n",
      "                    {{- \", \" }}\n",
      "                {%- endif %}\n",
      "                {%- endfor %}\n",
      "            {{- \")\" }}\n",
      "        {%- else  %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "            {{- '\"parameters\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- \"}\" }}\n",
      "        {%- endif %}\n",
      "        {%- if builtin_tools is defined %}\n",
      "            {#- This means we're in ipython mode #}\n",
      "            {{- \"<|eom_id|>\" }}\n",
      "        {%- else %}\n",
      "            {{- \"</s>\n",
      "\" }}\n",
      "        {%- endif %}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"</s>\n",
      "\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "\n",
    "# llama_3_1_8b_instruct_chat_template = \"{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- set date_string = \\\"26 Jul 2024\\\" %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \\\"\\\" %}\\n{%- endif %}\\n\\n{#- System message + builtin tools #}\\n{{- \\\"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\\\" }}\\n{%- if builtin_tools is defined or tools is not none %}\\n    {{- \\\"Environment: ipython\\\\n\\\" }}\\n{%- endif %}\\n{%- if builtin_tools is defined %}\\n    {{- \\\"Tools: \\\" + builtin_tools | reject('equalto', 'code_interpreter') | join(\\\", \\\") + \\\"\\\\n\\\\n\\\"}}\\n{%- endif %}\\n{{- \\\"Cutting Knowledge Date: December 2023\\\\n\\\" }}\\n{{- \\\"Today Date: \\\" + date_string + \\\"\\\\n\\\\n\\\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \\\"You have access to the following functions. To call a function, please respond with JSON for a function call.\\\" }}\\n    {{- 'Respond in the format {\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}.' }}\\n    {{- \\\"Do not use variables.\\\\n\\\\n\\\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \\\"\\\\n\\\\n\\\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \\\"<|eot_id|>\\\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0]['content']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\\\"Cannot put tools in the first user message when there's no first user message!\\\") }}\\n{%- endif %}\\n    {{- '<|start_header_id|>user<|end_header_id|>\\\\n\\\\n' -}}\\n    {{- \\\"Given the following functions, please respond with a JSON for a function call \\\" }}\\n    {{- \\\"with its proper arguments that best answers the given prompt.\\\\n\\\\n\\\" }}\\n    {{- 'Respond in the format {\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}.' }}\\n    {{- \\\"Do not use variables.\\\\n\\\\n\\\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \\\"\\\\n\\\\n\\\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \\\"<|eot_id|>\\\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\\\n\\\\n'+ message['content'] | trim + '<|eot_id|>' }}\\n    {%- elif 'tool_calls' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\\\"This model only supports single tool-calls at once!\\\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' -}}\\n            {{- \\\"<|python_tag|>\\\" + tool_call.name + \\\".call(\\\" }}\\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\\n                {{- arg_name + '=\\\"' + arg_val + '\\\"' }}\\n                {%- if not loop.last %}\\n                    {{- \\\", \\\" }}\\n                {%- endif %}\\n                {%- endfor %}\\n            {{- \\\")\\\" }}\\n        {%- else  %}\\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' -}}\\n            {{- '{\\\"name\\\": \\\"' + tool_call.name + '\\\", ' }}\\n            {{- '\\\"parameters\\\": ' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\\"}\\\" }}\\n        {%- endif %}\\n        {%- if builtin_tools is defined %}\\n            {#- This means we're in ipython mode #}\\n            {{- \\\"<|eom_id|>\\\" }}\\n        {%- else %}\\n            {{- \\\"<|eot_id|>\\\" }}\\n        {%- endif %}\\n    {%- elif message.role == \\\"tool\\\" or message.role == \\\"ipython\\\" %}\\n        {{- \\\"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\\\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \\\"<|eot_id|>\\\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' }}\\n{%- endif %}\\n\"\n",
    "\n",
    "# llama_3_1_8b_instruct_chat_template = \"{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- set date_string = \\\"26 Jul 2024\\\" %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \\\"\\\" %}\\n{%- endif %}\\n\\n{#- System message + builtin tools #}\\n{{- \\\"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\\\" }}\\n{%- if builtin_tools is defined or tools is not none %}\\n    {{- \\\"Environment: ipython\\\\n\\\" }}\\n{%- endif %}\\n{%- if builtin_tools is defined %}\\n    {{- \\\"Tools: \\\" + builtin_tools | reject('equalto', 'code_interpreter') | join(\\\", \\\") + \\\"\\\\n\\\\n\\\"}}\\n{%- endif %}\\n{{- \\\"Cutting Knowledge Date: December 2023\\\\n\\\" }}\\n{{- \\\"Today Date: \\\" + date_string + \\\"\\\\n\\\\n\\\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \\\"You have access to the following functions. To call a function, please respond with JSON for a function call.\\\" }}\\n    {{- 'Respond in the format {\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}.' }}\\n    {{- \\\"Do not use variables.\\\\n\\\\n\\\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \\\"\\\\n\\\\n\\\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \\\"<|eot_id|>\\\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0]['content']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\\\"Cannot put tools in the first user message when there's no first user message!\\\") }}\\n{%- endif %}\\n    {{- '<|start_header_id|>user<|end_header_id|>\\\\n\\\\n' -}}\\n    {{- \\\"Given the following functions, please respond with a JSON for a function call \\\" }}\\n    {{- \\\"with its proper arguments that best answers the given prompt.\\\\n\\\\n\\\" }}\\n    {{- 'Respond in the format {\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}.' }}\\n    {{- \\\"Do not use variables.\\\\n\\\\n\\\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \\\"\\\\n\\\\n\\\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \\\"<|eot_id|>\\\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\\\n\\\\n'+ message['content'] | trim }}\\n        {%- if not loop.last or add_generation_prompt %}\\n            {{- '<|eot_id|>' }}\\n        {%- endif %}\\n    {%- elif 'tool_calls' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\\\"This model only supports single tool-calls at once!\\\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' -}}\\n            {{- \\\"<|python_tag|>\\\" + tool_call.name + \\\".call(\\\" }}\\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\\n                {{- arg_name + '=\\\"' + arg_val + '\\\"' }}\\n                {%- if not loop.last %}\\n                    {{- \\\", \\\" }}\\n                {%- endif %}\\n                {%- endfor %}\\n            {{- \\\")\\\" }}\\n        {%- else  %}\\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' -}}\\n            {{- '{\\\"name\\\": \\\"' + tool_call.name + '\\\", ' }}\\n            {{- '\\\"parameters\\\": ' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\\"}\\\" }}\\n        {%- endif %}\\n        {%- if builtin_tools is defined %}\\n            {#- This means we're in ipython mode #}\\n            {{- \\\"<|eom_id|>\\\" }}\\n        {%- else %}\\n            {{- \\\"<|eot_id|>\\\" }}\\n        {%- endif %}\\n    {%- elif message.role == \\\"tool\\\" or message.role == \\\"ipython\\\" %}\\n        {{- \\\"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\\\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \\\"<|eot_id|>\\\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' }}\\n{%- endif %}\\n\"\n",
    "\n",
    "llama_3_1_8b_instruct_chat_template = \"{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- set date_string = \\\"26 Jul 2024\\\" %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \\\"\\\" %}\\n{%- endif %}\\n\\n{#- System message + builtin tools #}\\n{{- \\\"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\\\" }}\\n{%- if builtin_tools is defined or tools is not none %}\\n    {{- \\\"Environment: ipython\\\\n\\\" }}\\n{%- endif %}\\n{%- if builtin_tools is defined %}\\n    {{- \\\"Tools: \\\" + builtin_tools | reject('equalto', 'code_interpreter') | join(\\\", \\\") + \\\"\\\\n\\\\n\\\"}}\\n{%- endif %}\\n{{- \\\"Cutting Knowledge Date: December 2023\\\\n\\\" }}\\n{{- \\\"Today Date: \\\" + date_string + \\\"\\\\n\\\\n\\\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \\\"You have access to the following functions. To call a function, please respond with JSON for a function call.\\\" }}\\n    {{- 'Respond in the format {\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}.' }}\\n    {{- \\\"Do not use variables.\\\\n\\\\n\\\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \\\"\\\\n\\\\n\\\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \\\"<|eot_id|>\\\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0]['content']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\\\"Cannot put tools in the first user message when there's no first user message!\\\") }}\\n{%- endif %}\\n    {{- '<|start_header_id|>user<|end_header_id|>\\\\n\\\\n' -}}\\n    {{- \\\"Given the following functions, please respond with a JSON for a function call \\\" }}\\n    {{- \\\"with its proper arguments that best answers the given prompt.\\\\n\\\\n\\\" }}\\n    {{- 'Respond in the format {\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}.' }}\\n    {{- \\\"Do not use variables.\\\\n\\\\n\\\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \\\"\\\\n\\\\n\\\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \\\"<|eot_id|>\\\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\\\n\\\\n'+ message['content'] | trim }}\\n        {%- if not loop.last or add_generation_prompt %}\\n            {{- '<|eot_id|>' }}\\n        {%- endif %}\\n    {%- elif 'tool_calls' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\\\"This model only supports single tool-calls at once!\\\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' -}}\\n            {{- \\\"<|python_tag|>\\\" + tool_call.name + \\\".call(\\\" }}\\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\\n                {{- arg_name + '=\\\"' + arg_val + '\\\"' }}\\n                {%- if not loop.last %}\\n                    {{- \\\", \\\" }}\\n                {%- endif %}\\n                {%- endfor %}\\n            {{- \\\")\\\" }}\\n        {%- else  %}\\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' -}}\\n            {{- '{\\\"name\\\": \\\"' + tool_call.name + '\\\", ' }}\\n            {{- '\\\"parameters\\\": ' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\\"}\\\" }}\\n        {%- endif %}\\n        {%- if builtin_tools is defined %}\\n            {#- This means we're in ipython mode #}\\n            {{- \\\"<|eom_id|>\\\" }}\\n        {%- else %}\\n            {{- \\\"<|eot_id|>\\\" }}\\n        {%- endif %}\\n    {%- elif message.role == \\\"tool\\\" or message.role == \\\"ipython\\\" %}\\n        {{- \\\"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\\\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \\\"<|eot_id|>\\\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' }}\\n{%- endif %}\\n\"\n",
    "\n",
    "llama_3_1_8b_instruct_chat_template = llama_3_1_8b_instruct_chat_template.replace(\"<|eot_id|>\", f\"{tokenizer.eos_token}\\n\")\n",
    "\n",
    "print(llama_3_1_8b_instruct_chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# template = \"\"\"{%- for message in messages %}\n",
    "# {%- if message['role'] == 'user' %}\n",
    "# {{- '<|user|>\n",
    "# ' + message['content'] + eos_token + '\\n' }}\n",
    "# {%- elif message['role'] == 'system' %}\n",
    "# {{- '<|system|>\n",
    "# ' + message['content'] + \n",
    "# eos_token + '\\n' }}\n",
    "# {%- elif message['role'] == 'assistant' %}\n",
    "# {{- '<|assistant|>\n",
    "# '  + message['content'] + \n",
    "# eos_token + '\\n' }}\n",
    "# {%- endif %}\n",
    "# {%- if loop.last and add_generation_prompt %}\n",
    "# {{- '<|assistant|>' }}\n",
    "# {%- endif %}\n",
    "# {%- endfor %}\"\"\"\n",
    "\n",
    "template = \"\"\"{%- for message in messages %}\n",
    "{%- if message['role'] == 'assistant' %}\n",
    "{{- '<|assistant|>' + '\\n' }}\n",
    "{%- if message['content'] %}\n",
    "{{- message['content'] + eos_token + '\\n' }}\n",
    "{%- elif message['tool_calls'] %}\n",
    "{{- 'tool_calls:' + '\\n' }}\n",
    "{%- for tool_call in message['tool_calls'] %}\n",
    "{{- tool_call['function']['name'] + ': ' + tool_call['function']['arguments'] }}\n",
    "{%- endfor %}\n",
    "{{- eos_token + '\\n' }}\n",
    "{%- endif %}\n",
    "{%- elif message['role'] == 'system' %}\n",
    "{{- '<|system|>' + '\\n' }}\n",
    "{{- message['content'] }}\n",
    "{%- if tools %}\n",
    "{{- '\\n\\nYou are aware of the following tools:\\n' }}\n",
    "{%- for tool in tools %}\n",
    "{{- tool.function.name }}: {{ tool.function.parameters | tojson }}\n",
    "{%- endfor %}\n",
    "{{- '\\nYou can suggest tool calls by responding in the following format:\\n' }}\n",
    "{{- 'tool_calls:' + '\\n' }}\n",
    "{{- 'tool_name: {\\\"arg1\\\": \\\"value1\\\", \\\"arg2\\\": \\\"value2\\\"}' }}\n",
    "{{- eos_token + '\\n' }}\n",
    "{%- endif %}\n",
    "{%- elif message['role'] == 'user' %}\n",
    "{{- '<|user|>' + '\\n' }}\n",
    "{{- message['content'] + eos_token + '\\n' }}\n",
    "{%- endif %}\n",
    "{%- if loop.last and add_generation_prompt %}\n",
    "{{- '<|assistant|>' + '\\n' }}\n",
    "{%- endif %}\n",
    "{%- endfor %}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open(\"template.jinja\", \"w\").write(template)\n",
    "# open(\"template.jinja\", \"w\").write(tokenizer.chat_template)\n",
    "open(\"template.jinja\", \"w\").write(function_calling_template)\n",
    "# open(\"template.jinja\", \"w\").write(llama_3_1_8b_instruct_chat_template)\n",
    "\n",
    "tokenizer.chat_template = open(\"template.jinja\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are an intelligent AI that controls a drone. Given a command or request from the user,\\ncall one of your functions to complete the request. If the request cannot be completed by your available functions, call the reject_request function.\\nIf the request is ambiguous or unclear, reject the request.'},\n",
       " {'role': 'user',\n",
       "  'content': \"Let's get the drone in the air, how high should it go?\"},\n",
       " {'role': 'assistant',\n",
       "  'tool_calls': [{'id': 'call_id',\n",
       "    'type': 'function',\n",
       "    'function': {'name': 'takeoff_drone', 'arguments': '{\"altitude\": 100}'}}]}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations = []\n",
    "\n",
    "with open(\"../../data/drone_training.jsonl\") as f:\n",
    "    for line in f:\n",
    "        conversations.append(json.loads(line))\n",
    "\n",
    "conversation = conversations[0]\n",
    "\n",
    "# print('messages: ', conversation[\"messages\"])\n",
    "\n",
    "# print('parallel_tool_calls: ', conversation[\"parallel_tool_calls\"])\n",
    "\n",
    "# print('tools: ', conversation[\"tools\"])\n",
    "\n",
    "# conversation = conversation[\"messages\"][0:2]\n",
    "# conversation = conversation[\"messages\"]\n",
    "conversation[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are an intelligent AI that controls a drone. Given a command or request from the user,\n",
      "call one of your functions to complete the request. If the request cannot be completed by your available functions, call the reject_request function.\n",
      "If the request is ambiguous or unclear, reject the request.\n",
      "You have access to the following functions:\n",
      "functions.takeoff_drone:\n",
      "{\"type\": \"object\", \"properties\": {\"altitude\": {\"type\": \"integer\"}}, \"required\": [\"altitude\"]}\n",
      "functions.land_drone:\n",
      "{\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"enum\": [\"current\", \"home_base\", \"custom\"]}, \"coordinates\": {\"type\": \"object\"}}, \"required\": [\"location\"]}\n",
      "functions.control_drone_movement:\n",
      "{\"type\": \"object\", \"properties\": {\"direction\": {\"type\": \"string\", \"enum\": [\"forward\", \"backward\", \"left\", \"right\", \"up\", \"down\"]}, \"distance\": {\"type\": \"integer\"}}, \"required\": [\"direction\", \"distance\"]}\n",
      "functions.set_drone_speed:\n",
      "{\"type\": \"object\", \"properties\": {\"speed\": {\"type\": \"integer\", \"minimum\": 0}}, \"required\": [\"speed\"]}\n",
      "functions.control_camera:\n",
      "{\"type\": \"object\", \"properties\": {\"mode\": {\"type\": \"string\", \"enum\": [\"photo\", \"video\", \"panorama\"]}, \"duration\": {\"type\": \"integer\"}}, \"required\": [\"mode\"]}\n",
      "functions.control_gimbal:\n",
      "{\"type\": \"object\", \"properties\": {\"tilt\": {\"type\": \"integer\"}, \"pan\": {\"type\": \"integer\"}}, \"required\": [\"tilt\", \"pan\"]}\n",
      "functions.set_drone_lighting:\n",
      "{\"type\": \"object\", \"properties\": {\"mode\": {\"type\": \"string\", \"enum\": [\"on\", \"off\", \"blink\", \"sos\"]}}, \"required\": [\"mode\"]}\n",
      "functions.return_to_home:\n",
      "{\"type\": \"object\", \"properties\": {}}\n",
      "functions.set_battery_saver_mode:\n",
      "{\"type\": \"object\", \"properties\": {\"status\": {\"type\": \"string\", \"enum\": [\"on\", \"off\"]}}, \"required\": [\"status\"]}\n",
      "functions.set_obstacle_avoidance:\n",
      "{\"type\": \"object\", \"properties\": {\"mode\": {\"type\": \"string\", \"enum\": [\"on\", \"off\"]}}, \"required\": [\"mode\"]}\n",
      "functions.set_follow_me_mode:\n",
      "{\"type\": \"object\", \"properties\": {\"status\": {\"type\": \"string\", \"enum\": [\"on\", \"off\"]}}, \"required\": [\"status\"]}\n",
      "functions.calibrate_sensors:\n",
      "{\"type\": \"object\", \"properties\": {}}\n",
      "functions.set_autopilot:\n",
      "{\"type\": \"object\", \"properties\": {\"status\": {\"type\": \"string\", \"enum\": [\"on\", \"off\"]}}, \"required\": [\"status\"]}\n",
      "functions.configure_led_display:\n",
      "{\"type\": \"object\", \"properties\": {\"pattern\": {\"type\": \"string\", \"enum\": [\"solid\", \"blink\", \"pulse\", \"rainbow\"]}, \"color\": {\"type\": \"string\", \"enum\": [\"red\", \"blue\", \"green\", \"yellow\", \"white\"]}}, \"required\": [\"pattern\"]}\n",
      "functions.set_home_location:\n",
      "{\"type\": \"object\", \"properties\": {\"coordinates\": {\"type\": \"object\"}}, \"required\": [\"coordinates\"]}\n",
      "functions.reject_request:\n",
      "{\"type\": \"object\", \"properties\": {}}\n",
      "\n",
      "You can respond to user messages with either a single message or one or more function calls.\n",
      "\n",
      "To respond with a message begin the message with \"message:\", use the following format:\n",
      "\n",
      "message:\n",
      "<message>\n",
      "\n",
      "To respond with one or more function calls begin the message with 'functions.<function_name>:', use the following format:\n",
      "\n",
      "functions.<function_name>:\n",
      "{ \"arg1\": \"value1\", \"arg2\": \"value2\" }\n",
      "functions.<function_name>:\n",
      "{ \"arg1\": \"value1\", \"arg2\": \"value2\" }\n",
      "When responding with function calls, only output the function calls, do not include any additional text.</s>\n",
      "<|im_start|>user\n",
      "Let's get the drone in the air, how high should it go?</s>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def generate_prompt(add_generation_prompt, date_string=f\"{datetime.now():%d %b %Y}\", messages=[], tool_calls=True, tools=[]):\n",
    "    return tokenizer.apply_chat_template(\n",
    "        add_generation_prompt=add_generation_prompt,\n",
    "        # add_special_tokens=False,\n",
    "        # conversation=conversation,\n",
    "        # conversation=conversation[\"messages\"][0:2], # Skip tool messages for now\n",
    "        # conversation=conversation[\"messages\"],\n",
    "        conversation=messages,\n",
    "        date_string=date_string,\n",
    "        # return_dict=True,\n",
    "        return_tensors=False,\n",
    "        tokenize=False,\n",
    "        tool_calls=tool_calls,\n",
    "        # tools=conversation[\"tools\"],\n",
    "        tools=tools,\n",
    "    )\n",
    "\n",
    "prompt = generate_prompt(\n",
    "    add_generation_prompt=True,\n",
    "    messages=conversation[\"messages\"][0:2],\n",
    "    # messages=conversation[\"messages\"],\n",
    "    tool_calls=True,\n",
    "    tools=conversation[\"tools\"],\n",
    ")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When responding with function calls, only output the function calls, do not include any additional text.</s> \n",
      "!=\n",
      "When responding with function calls, only output the function calls, do not include any additional text.</s>\n",
      "Let's get the drone in the air, how high should it go?</s> \n",
      "!=\n",
      "Let's get the drone in the air, how high should it go?</s>\n"
     ]
    }
   ],
   "source": [
    "hf_tokenized_prompt = tokenizer.tokenize(prompt, add_special_tokens=False)\n",
    "hf_encoded_tokenized_prompt = tokenizer.convert_tokens_to_ids(hf_tokenized_prompt)\n",
    "hf_encoded_prompt = tokenizer.encode(prompt, add_special_tokens=False) # Same as doing self.convert_tokens_to_ids(self.tokenize(text))\n",
    "\n",
    "assert hf_encoded_prompt == hf_encoded_tokenized_prompt, f\"\\n{hf_encoded_prompt}\\n!=\\n{hf_encoded_tokenized_prompt}\"\n",
    "\n",
    "# decoded_hf_encoded_prompt = tokenizer.decode(hf_encoded_prompt, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "# decoded_hf_encoded_prompt = tokenizer.decode(hf_encoded_prompt, skip_special_tokens=True)\n",
    "decoded_hf_encoded_prompt = tokenizer.decode(hf_encoded_prompt)\n",
    "\n",
    "# assert decoded_hf_encoded_prompt == prompt, f\"\\n{decoded_hf_encoded_prompt}\\n!=\\n{prompt}\"\n",
    "\n",
    "for line_a, line_b in zip(prompt.split(\"\\n\"), decoded_hf_encoded_prompt.split(\"\\n\")):\n",
    "    try:\n",
    "        assert line_b == line_a\n",
    "\n",
    "    except AssertionError:\n",
    "        print(f\"{line_b}\\n!=\\n{line_a}\")\n",
    "        assert line_b.strip() == line_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def greedy_until(prompt:str, until, max_length, temperature):\n",
    "#     # toks = [self.tokenizer.bos_id()] + self.tokenizer.encode(prompt)\n",
    "#     toks = tokenizer.encode(prompt)\n",
    "#     assert toks[0] == tokenizer.bos_token_id\n",
    "#     initial_len = len(toks)\n",
    "\n",
    "#     start_pos = 0\n",
    "#     stop_pos = start_pos + 1\n",
    "\n",
    "#     # for i in range(max_length):\n",
    "#     for i in range(initial_len + max_length):\n",
    "#     #   probs = llama.model(Tensor([toks[start_pos:]]), start_pos, temperature).realize()\n",
    "#         start_i = datetime.now()\n",
    "\n",
    "#         tokens = Tensor([toks[start_pos:stop_pos]])\n",
    "#         print('tokens.shape: ', tokens.shape)\n",
    "\n",
    "#         assert tokens.shape[0:2] == (1,1), f\"{tokens.shape[0:2]} != (1,1)\"\n",
    "\n",
    "#         logits = model.model(tokens, start_pos, temperature).realize()\n",
    "#         # probs_np = probs.numpy()\n",
    "#         # print('probs_np: ', probs_np)\n",
    "#         # print('len(probs_np): ', len(probs_np))\n",
    "#         # probs_np = probs_np[0]\n",
    "#         # tok = int(np.random.choice(len(probs_np), p=probs_np))\n",
    "#         tok = logits.argmax()\n",
    "#         # start_pos = len(toks)\n",
    "#         start_pos = stop_pos\n",
    "#         stop_pos = start_pos + 1\n",
    "#         toks.append(tok)\n",
    "\n",
    "#         # # if tok == self.tokenizer.eos_id(): break\n",
    "#         if tok == tokenizer.eos_token_id: break\n",
    "\n",
    "#         # # output = self.tokenizer.decode(toks)\n",
    "#         output = tokenizer.decode(toks)\n",
    "\n",
    "#         for s in until:\n",
    "#             if output.endswith(s): return output[0:-len(s)]\n",
    "\n",
    "#         print(f\"Elapsed ({i}): {datetime.now()-start_i}\")\n",
    "\n",
    "#     return output\n",
    "\n",
    "# start = datetime.now()\n",
    "\n",
    "# prediction = greedy_until(prompt, until=[tokenizer.eos_token], max_length=256, temperature=0.0)\n",
    "\n",
    "# print(f\"Elapsed: {datetime.now()-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 0:00:01.289119\n",
      "Elapsed: 0:00:00.463616\n",
      "Elapsed: 0:00:00.033095\n",
      "Elapsed: 0:00:00.001113\n",
      "Elapsed: 0:00:00.001184\n",
      "Elapsed: 0:00:00.002183\n",
      "Elapsed: 0:00:00.001768\n",
      "Elapsed: 0:00:00.002082\n",
      "Elapsed: 0:00:00.000998\n",
      "Elapsed: 0:00:00.000865\n",
      "Elapsed: 0:00:00.000886\n",
      "Elapsed: 0:00:00.000863\n",
      "Elapsed: 0:00:00.000853\n",
      "Elapsed: 0:00:00.001050\n",
      "Elapsed: 0:00:00.001469\n",
      "Elapsed: 0:00:00.000928\n",
      "Elapsed: 0:00:00.001214\n",
      "Elapsed: 0:00:00.000871\n",
      "Elapsed: 0:00:00.001431\n",
      "Elapsed: 0:00:00.001720\n",
      "Elapsed: 0:00:00.001503\n",
      "Elapsed: 0:00:00.001705\n",
      "Elapsed: 0:00:00.000958\n",
      "Elapsed: 0:00:00.000888\n",
      "Elapsed: 0:00:00.000894\n",
      "Elapsed: 0:00:00.000898\n",
      "Elapsed: 0:00:00.001003\n",
      "Elapsed: 0:00:00.001237\n",
      "Elapsed: 0:00:00.000998\n",
      "Elapsed: 0:00:00.001009\n",
      "Elapsed: 0:00:00.000874\n",
      "Elapsed: 0:00:00.000876\n",
      "Elapsed: 0:00:00.000972\n",
      "Elapsed: 0:00:00.002522\n",
      "Elapsed: 0:00:00.002092\n",
      "Elapsed: 0:00:00.001182\n",
      "Elapsed: 0:00:00.000999\n",
      "Elapsed: 0:00:00.000914\n",
      "Elapsed: 0:00:00.000917\n",
      "Elapsed: 0:00:00.001303\n",
      "Elapsed: 0:00:00.001517\n",
      "Elapsed: 0:00:00.001292\n",
      "Elapsed: 0:00:00.000963\n",
      "Elapsed: 0:00:00.000909\n",
      "Elapsed: 0:00:00.001064\n",
      "Elapsed: 0:00:00.002080\n",
      "Elapsed: 0:00:00.001320\n",
      "Elapsed: 0:00:00.001669\n",
      "Elapsed: 0:00:00.001130\n",
      "Elapsed: 0:00:00.001271\n",
      "Elapsed: 0:00:00.001482\n",
      "Elapsed: 0:00:00.001342\n",
      "Elapsed: 0:00:00.000972\n",
      "Elapsed: 0:00:00.000885\n",
      "Elapsed: 0:00:00.000823\n",
      "Elapsed: 0:00:00.000879\n",
      "Elapsed: 0:00:00.000882\n",
      "Elapsed: 0:00:00.000880\n",
      "Elapsed: 0:00:00.000927\n",
      "Elapsed: 0:00:00.001961\n",
      "Elapsed: 0:00:00.001387\n",
      "Elapsed: 0:00:00.002283\n",
      "Elapsed: 0:00:00.001093\n",
      "Elapsed: 0:00:00.000891\n",
      "Elapsed: 0:00:00.000883\n",
      "Elapsed: 0:00:00.000867\n",
      "Elapsed: 0:00:00.000843\n",
      "Elapsed: 0:00:00.000808\n",
      "Elapsed: 0:00:00.000843\n",
      "Elapsed: 0:00:00.000845\n",
      "Elapsed: 0:00:00.000848\n",
      "Elapsed: 0:00:00.000844\n",
      "Elapsed: 0:00:00.000846\n",
      "Elapsed: 0:00:00.000885\n",
      "Elapsed: 0:00:00.002207\n",
      "Elapsed: 0:00:00.002292\n",
      "Elapsed: 0:00:00.001782\n",
      "Elapsed: 0:00:00.000951\n",
      "Elapsed: 0:00:00.000985\n",
      "Elapsed: 0:00:00.001404\n",
      "Elapsed: 0:00:00.000883\n",
      "Elapsed: 0:00:00.000880\n",
      "Elapsed: 0:00:00.000849\n",
      "Elapsed: 0:00:00.000931\n",
      "Elapsed: 0:00:00.000876\n",
      "Elapsed: 0:00:00.000849\n",
      "Elapsed: 0:00:00.000857\n",
      "Elapsed: 0:00:00.000946\n",
      "Elapsed: 0:00:00.002365\n",
      "Elapsed: 0:00:00.002260\n",
      "Elapsed: 0:00:00.001501\n",
      "Elapsed: 0:00:00.001005\n",
      "Elapsed: 0:00:00.000969\n",
      "Elapsed: 0:00:00.000891\n",
      "Elapsed: 0:00:00.000854\n",
      "Elapsed: 0:00:00.000811\n",
      "Elapsed: 0:00:00.000831\n",
      "Elapsed: 0:00:00.000801\n",
      "Elapsed: 0:00:00.000809\n",
      "Elapsed: 0:00:00.000810\n",
      "Elapsed: 0:00:00.000809\n",
      "Elapsed: 0:00:00.000909\n",
      "Elapsed: 0:00:00.001998\n",
      "Elapsed: 0:00:00.002242\n",
      "Elapsed: 0:00:00.002043\n",
      "Elapsed: 0:00:00.000969\n",
      "Elapsed: 0:00:00.168555\n",
      "Elapsed: 0:00:00.187534\n",
      "Elapsed: 0:00:00.188718\n",
      "Elapsed: 0:00:00.189860\n",
      "Elapsed: 0:00:00.196989\n",
      "Elapsed: 0:00:00.187889\n",
      "Elapsed: 0:00:00.188778\n",
      "Elapsed: 0:00:00.191228\n",
      "Elapsed: 0:00:00.188158\n",
      "Elapsed: 0:00:00.188405\n",
      "Elapsed: 0:00:00.188700\n",
      "Elapsed: 0:00:00.188012\n",
      "Elapsed: 0:00:00.190866\n",
      "Elapsed: 0:00:00.188354\n",
      "Elapsed: 0:00:00.190671\n",
      "Elapsed: 0:00:00.190928\n",
      "Elapsed: 0:00:00.190083\n",
      "Elapsed: 0:00:00.192091\n",
      "Elapsed: 0:00:00.188923\n",
      "Elapsed: 0:00:00.193589\n",
      "Elapsed: 0:00:00.204398\n",
      "Elapsed: 0:00:00.190541\n",
      "Elapsed: 0:00:00.190677\n",
      "Elapsed: 0:00:00.190152\n",
      "Elapsed: 0:00:00.191210\n",
      "Elapsed: 0:00:00.192647\n",
      "Elapsed: 0:00:00.193552\n",
      "Elapsed: 0:00:00.194607\n",
      "Elapsed: 0:00:00.193177\n",
      "Elapsed: 0:00:00.193853\n",
      "Elapsed: 0:00:00.192612\n",
      "Elapsed: 0:00:00.193749\n",
      "Elapsed: 0:00:00.191773\n",
      "Elapsed: 0:00:00.192955\n",
      "Elapsed: 0:00:00.193391\n",
      "Elapsed: 0:00:00.193192\n",
      "Elapsed: 0:00:00.202287\n",
      "Elapsed: 0:00:00.193772\n",
      "Elapsed: 0:00:00.196254\n",
      "Elapsed: 0:00:00.191091\n",
      "Elapsed: 0:00:00.197421\n",
      "Elapsed: 0:00:00.190939\n",
      "Elapsed: 0:00:00.193615\n",
      "Elapsed: 0:00:00.205595\n",
      "Elapsed: 0:00:00.193625\n",
      "Elapsed: 0:00:00.197074\n",
      "Elapsed: 0:00:00.193543\n",
      "Elapsed: 0:00:00.193169\n",
      "Elapsed: 0:00:00.200248\n",
      "Elapsed: 0:00:00.196706\n",
      "Elapsed: 0:00:00.192787\n",
      "Elapsed: 0:00:00.196058\n",
      "Elapsed: 0:00:00.193345\n",
      "Elapsed: 0:00:00.197041\n",
      "Elapsed: 0:00:00.191796\n",
      "Elapsed: 0:00:00.199789\n",
      "Elapsed: 0:00:00.194459\n",
      "Elapsed: 0:00:00.193153\n",
      "Elapsed: 0:00:00.195159\n",
      "Elapsed: 0:00:00.194814\n",
      "Elapsed: 0:00:00.193191\n",
      "Elapsed: 0:00:00.001091\n",
      "Elapsed: 0:00:00.000856\n",
      "Elapsed: 0:00:00.000841\n",
      "Elapsed: 0:00:00.001898\n",
      "Elapsed: 0:00:00.978655\n",
      "Elapsed: 0:00:00.201827\n",
      "Elapsed: 0:00:00.194839\n",
      "Elapsed: 0:00:00.192207\n",
      "Elapsed: 0:00:00.193554\n",
      "Elapsed: 0:00:00.192571\n",
      "Elapsed: 0:00:00.199839\n",
      "Elapsed: 0:00:00.193179\n",
      "Elapsed: 0:00:00.195778\n",
      "Elapsed: 0:00:00.205108\n",
      "Elapsed: 0:00:00.193723\n",
      "Elapsed: 0:00:00.194078\n",
      "Elapsed: 0:00:00.195640\n",
      "Elapsed: 0:00:00.192573\n",
      "Elapsed: 0:00:00.194315\n",
      "Elapsed: 0:00:00.196408\n",
      "Elapsed: 0:00:00.195192\n",
      "Elapsed: 0:00:00.197754\n",
      "Elapsed: 0:00:00.192474\n",
      "Elapsed: 0:00:00.195952\n",
      "Elapsed: 0:00:00.193737\n",
      "Elapsed: 0:00:00.192714\n",
      "Elapsed: 0:00:00.203917\n",
      "Elapsed: 0:00:00.195433\n",
      "Elapsed: 0:00:00.193405\n",
      "Elapsed: 0:00:00.194270\n",
      "Elapsed: 0:00:00.194088\n",
      "Elapsed: 0:00:00.191361\n",
      "Elapsed: 0:00:00.194500\n",
      "Elapsed: 0:00:00.194600\n",
      "Elapsed: 0:00:00.194471\n",
      "Elapsed: 0:00:00.191263\n",
      "Elapsed: 0:00:00.194879\n",
      "Elapsed: 0:00:00.195482\n",
      "Elapsed: 0:00:00.195093\n",
      "Elapsed: 0:00:00.194957\n",
      "Elapsed: 0:00:00.194424\n",
      "Elapsed: 0:00:00.194774\n",
      "Elapsed: 0:00:00.196425\n",
      "Elapsed: 0:00:00.195123\n",
      "Elapsed: 0:00:00.195139\n",
      "Elapsed: 0:00:00.202906\n",
      "Elapsed: 0:00:00.192622\n",
      "Elapsed: 0:00:00.194670\n",
      "Elapsed: 0:00:00.194772\n",
      "Elapsed: 0:00:00.195041\n",
      "Elapsed: 0:00:00.194534\n",
      "Elapsed: 0:00:00.195305\n",
      "Elapsed: 0:00:00.193340\n",
      "Elapsed: 0:00:00.213883\n",
      "Elapsed: 0:00:00.192967\n",
      "Elapsed: 0:00:00.194566\n",
      "Elapsed: 0:00:00.197055\n",
      "Elapsed: 0:00:00.206942\n",
      "Elapsed: 0:00:00.194672\n",
      "Elapsed: 0:00:00.195546\n",
      "Elapsed: 0:00:00.195480\n",
      "Elapsed: 0:00:00.192328\n",
      "Elapsed: 0:00:00.193609\n",
      "Elapsed: 0:00:00.193437\n",
      "Elapsed: 0:00:00.193511\n",
      "Elapsed: 0:00:00.194166\n",
      "Elapsed: 0:00:00.192848\n",
      "Elapsed: 0:00:00.194610\n",
      "Elapsed: 0:00:00.194792\n",
      "Elapsed: 0:00:00.195649\n",
      "Elapsed: 0:00:00.194310\n",
      "Elapsed: 0:00:00.200243\n",
      "Elapsed: 0:00:00.195514\n",
      "Elapsed: 0:00:00.195174\n",
      "Elapsed: 0:00:00.193104\n",
      "Elapsed: 0:00:00.195838\n",
      "Elapsed: 0:00:00.192066\n",
      "Elapsed: 0:00:00.195127\n",
      "Elapsed: 0:00:00.195378\n",
      "Elapsed: 0:00:00.193254\n",
      "Elapsed: 0:00:00.195888\n",
      "Elapsed: 0:00:00.196711\n",
      "Elapsed: 0:00:00.195429\n",
      "Elapsed: 0:00:00.196059\n",
      "Elapsed: 0:00:00.194695\n",
      "Elapsed: 0:00:00.197962\n",
      "Elapsed: 0:00:00.197369\n",
      "Elapsed: 0:00:00.195946\n",
      "Elapsed: 0:00:00.201482\n",
      "Elapsed: 0:00:00.204951\n",
      "Elapsed: 0:00:00.198726\n",
      "Elapsed: 0:00:00.203902\n",
      "Elapsed: 0:00:00.198136\n",
      "Elapsed: 0:00:00.197858\n",
      "Elapsed: 0:00:00.198856\n",
      "Elapsed: 0:00:00.198917\n",
      "Elapsed: 0:00:00.197997\n",
      "Elapsed: 0:00:00.196106\n",
      "Elapsed: 0:00:00.197770\n",
      "Elapsed: 0:00:00.198077\n",
      "Elapsed: 0:00:00.205766\n",
      "Elapsed: 0:00:00.198957\n",
      "Elapsed: 0:00:00.195083\n",
      "Elapsed: 0:00:00.196848\n",
      "Elapsed: 0:00:00.198027\n",
      "Elapsed: 0:00:00.198926\n",
      "Elapsed: 0:00:00.209159\n",
      "Elapsed: 0:00:00.196719\n",
      "Elapsed: 0:00:00.001087\n",
      "Elapsed: 0:00:00.000832\n",
      "Elapsed: 0:00:00.000779\n",
      "Elapsed: 0:00:00.000790\n",
      "Elapsed: 0:00:01.040659\n",
      "Elapsed: 0:00:00.196254\n",
      "Elapsed: 0:00:00.203046\n",
      "Elapsed: 0:00:00.199510\n",
      "Elapsed: 0:00:00.203367\n",
      "Elapsed: 0:00:00.198345\n",
      "Elapsed: 0:00:00.196444\n",
      "Elapsed: 0:00:00.199239\n",
      "Elapsed: 0:00:00.198473\n",
      "Elapsed: 0:00:00.198022\n",
      "Elapsed: 0:00:00.198548\n",
      "Elapsed: 0:00:00.208396\n",
      "Elapsed: 0:00:00.199887\n",
      "Elapsed: 0:00:00.198918\n",
      "Elapsed: 0:00:00.196402\n",
      "Elapsed: 0:00:00.198691\n",
      "Elapsed: 0:00:00.201717\n",
      "Elapsed: 0:00:00.199068\n",
      "Elapsed: 0:00:00.203236\n",
      "Elapsed: 0:00:00.199379\n",
      "Elapsed: 0:00:00.197255\n",
      "Elapsed: 0:00:00.198738\n",
      "Elapsed: 0:00:00.196633\n",
      "Elapsed: 0:00:00.205432\n",
      "Elapsed: 0:00:00.207227\n",
      "Elapsed: 0:00:00.204649\n",
      "Elapsed: 0:00:00.201662\n",
      "Elapsed: 0:00:00.199926\n",
      "Elapsed: 0:00:00.200462\n",
      "Elapsed: 0:00:00.205354\n",
      "Elapsed: 0:00:00.199634\n",
      "Elapsed: 0:00:00.206474\n",
      "Elapsed: 0:00:00.199810\n",
      "Elapsed: 0:00:00.197863\n",
      "Elapsed: 0:00:00.197627\n",
      "Elapsed: 0:00:00.198872\n",
      "Elapsed: 0:00:00.206197\n",
      "Elapsed: 0:00:00.198831\n",
      "Elapsed: 0:00:00.199862\n",
      "Elapsed: 0:00:00.197563\n",
      "Elapsed: 0:00:00.199174\n",
      "Elapsed: 0:00:00.197574\n",
      "Elapsed: 0:00:00.209463\n",
      "Elapsed: 0:00:00.199159\n",
      "Elapsed: 0:00:00.205786\n",
      "Elapsed: 0:00:00.200385\n",
      "Elapsed: 0:00:00.199420\n",
      "Elapsed: 0:00:00.197632\n",
      "Elapsed: 0:00:00.201819\n",
      "Elapsed: 0:00:00.210090\n",
      "Elapsed: 0:00:00.199381\n",
      "Elapsed: 0:00:00.197781\n",
      "Elapsed: 0:00:00.199382\n",
      "Elapsed: 0:00:00.200506\n",
      "Elapsed: 0:00:00.197581\n",
      "Elapsed: 0:00:00.201479\n",
      "Elapsed: 0:00:00.207651\n",
      "Elapsed: 0:00:00.199803\n",
      "Elapsed: 0:00:00.200051\n",
      "Elapsed: 0:00:00.207394\n",
      "Elapsed: 0:00:00.200725\n",
      "Elapsed: 0:00:00.199792\n",
      "Elapsed: 0:00:00.200488\n",
      "Elapsed: 0:00:00.208531\n",
      "Elapsed: 0:00:00.199713\n",
      "Elapsed: 0:00:00.200920\n",
      "Elapsed: 0:00:00.211287\n",
      "Elapsed: 0:00:00.200213\n",
      "Elapsed: 0:00:00.205458\n",
      "Elapsed: 0:00:00.202708\n",
      "Elapsed: 0:00:00.197565\n",
      "Elapsed: 0:00:00.200926\n",
      "Elapsed: 0:00:00.205175\n",
      "Elapsed: 0:00:00.200469\n",
      "Elapsed: 0:00:00.199977\n",
      "Elapsed: 0:00:00.199487\n",
      "Elapsed: 0:00:00.208295\n",
      "Elapsed: 0:00:00.200067\n",
      "Elapsed: 0:00:00.200325\n",
      "Elapsed: 0:00:00.207140\n",
      "Elapsed: 0:00:00.211182\n",
      "Elapsed: 0:00:00.221328\n",
      "Elapsed: 0:00:00.201380\n",
      "Elapsed: 0:00:00.203067\n",
      "Elapsed: 0:00:00.199998\n",
      "Elapsed: 0:00:00.200582\n",
      "Elapsed: 0:00:00.201752\n",
      "Elapsed: 0:00:00.201198\n",
      "Elapsed: 0:00:00.200016\n",
      "Elapsed: 0:00:00.202336\n",
      "Elapsed: 0:00:00.200710\n",
      "Elapsed: 0:00:00.206240\n",
      "Elapsed: 0:00:00.203170\n",
      "Elapsed: 0:00:00.199156\n",
      "Elapsed: 0:00:00.200721\n",
      "Elapsed: 0:00:00.200643\n",
      "Elapsed: 0:00:00.203355\n",
      "Elapsed: 0:00:00.199435\n",
      "Elapsed: 0:00:00.200432\n",
      "Elapsed: 0:00:00.201438\n",
      "Elapsed: 0:00:00.203229\n",
      "Elapsed: 0:00:00.199087\n",
      "Elapsed: 0:00:00.201224\n",
      "Elapsed: 0:00:00.200532\n",
      "Elapsed: 0:00:00.002048\n",
      "Elapsed: 0:00:00.001680\n",
      "Elapsed: 0:00:00.002137\n",
      "Elapsed: 0:00:00.001952\n",
      "Elapsed: 0:00:01.023159\n",
      "Elapsed: 0:00:00.211239\n",
      "Elapsed: 0:00:00.201542\n",
      "Elapsed: 0:00:00.200925\n",
      "Elapsed: 0:00:00.201921\n",
      "Elapsed: 0:00:00.204874\n",
      "Elapsed: 0:00:00.201907\n",
      "Elapsed: 0:00:00.203321\n",
      "Elapsed: 0:00:00.200070\n",
      "Elapsed: 0:00:00.202025\n",
      "Elapsed: 0:00:00.200447\n",
      "Elapsed: 0:00:00.200763\n",
      "Elapsed: 0:00:00.199556\n",
      "Elapsed: 0:00:00.199928\n",
      "Elapsed: 0:00:00.203733\n",
      "Elapsed: 0:00:00.201282\n",
      "Elapsed: 0:00:00.201352\n",
      "Elapsed: 0:00:00.206807\n",
      "Elapsed: 0:00:00.201548\n",
      "Elapsed: 0:00:00.199995\n",
      "Elapsed: 0:00:00.200600\n",
      "Elapsed: 0:00:00.202401\n",
      "Elapsed: 0:00:00.201405\n",
      "Elapsed: 0:00:00.200803\n",
      "Elapsed: 0:00:00.201715\n",
      "Elapsed: 0:00:00.200140\n",
      "Elapsed: 0:00:00.202293\n",
      "Elapsed: 0:00:00.202065\n",
      "Elapsed: 0:00:00.199485\n",
      "Elapsed: 0:00:00.202839\n",
      "Elapsed: 0:00:00.202564\n",
      "Elapsed: 0:00:00.202152\n",
      "Elapsed: 0:00:00.207989\n",
      "Elapsed: 0:00:00.201005\n",
      "Elapsed: 0:00:00.200402\n",
      "Elapsed: 0:00:00.201437\n",
      "Elapsed: 0:00:00.202843\n",
      "Elapsed: 0:00:00.198513\n",
      "Elapsed: 0:00:00.202490\n",
      "Elapsed: 0:00:00.199529\n",
      "Elapsed: 0:00:00.200971\n",
      "Elapsed: 0:00:00.202210\n",
      "Elapsed: 0:00:00.202633\n",
      "Elapsed: 0:00:00.202145\n",
      "Elapsed: 0:00:00.210698\n",
      "Elapsed: 0:00:00.200360\n",
      "Elapsed: 0:00:00.200497\n",
      "Elapsed: 0:00:00.202081\n",
      "Elapsed: 0:00:00.203620\n",
      "Elapsed: 0:00:00.202141\n",
      "Elapsed: 0:00:00.203137\n",
      "Elapsed: 0:00:00.202833\n",
      "Elapsed: 0:00:00.202945\n",
      "Elapsed: 0:00:00.201744\n",
      "Elapsed: 0:00:00.201269\n",
      "Elapsed: 0:00:00.204417\n",
      "Elapsed: 0:00:00.202547\n",
      "Elapsed: 0:00:00.202338\n",
      "Elapsed: 0:00:00.204756\n",
      "Elapsed: 0:00:00.202181\n",
      "Elapsed: 0:00:00.202575\n",
      "Elapsed: 0:00:00.205185\n",
      "Elapsed: 0:00:00.202802\n",
      "Elapsed: 0:00:00.202297\n",
      "Elapsed: 0:00:00.202134\n",
      "Elapsed: 0:00:00.203212\n",
      "Elapsed: 0:00:00.204819\n",
      "Elapsed: 0:00:00.201654\n",
      "Elapsed: 0:00:00.202076\n",
      "Elapsed: 0:00:00.202587\n",
      "Elapsed: 0:00:00.204065\n",
      "Elapsed: 0:00:00.202238\n",
      "Elapsed: 0:00:00.202671\n",
      "Elapsed: 0:00:00.202903\n",
      "Elapsed: 0:00:00.207853\n",
      "Elapsed: 0:00:00.202796\n",
      "Elapsed: 0:00:00.202384\n",
      "Elapsed: 0:00:00.203768\n",
      "Elapsed: 0:00:00.203295\n",
      "Elapsed: 0:00:00.202121\n",
      "Elapsed: 0:00:00.204035\n",
      "Elapsed: 0:00:00.203346\n",
      "Elapsed: 0:00:00.201307\n",
      "Elapsed: 0:00:00.203403\n",
      "Elapsed: 0:00:00.203013\n",
      "Elapsed: 0:00:00.202044\n",
      "Elapsed: 0:00:00.203034\n",
      "Elapsed: 0:00:00.202486\n",
      "Elapsed: 0:00:00.202185\n",
      "Elapsed: 0:00:00.203770\n",
      "Elapsed: 0:00:00.204165\n",
      "Elapsed: 0:00:00.201881\n",
      "Elapsed: 0:00:00.202499\n",
      "Elapsed: 0:00:00.209044\n",
      "Elapsed: 0:00:00.205568\n",
      "Elapsed: 0:00:00.203684\n",
      "Elapsed: 0:00:00.203543\n",
      "Elapsed: 0:00:00.202836\n",
      "Elapsed: 0:00:00.204285\n",
      "Elapsed: 0:00:00.202182\n",
      "Elapsed: 0:00:00.203218\n",
      "Elapsed: 0:00:00.201676\n",
      "Elapsed: 0:00:00.210474\n",
      "Elapsed: 0:00:00.213306\n",
      "Elapsed: 0:00:00.003726\n",
      "Elapsed: 0:00:00.002114\n",
      "Elapsed: 0:00:00.002029\n",
      "Elapsed: 0:00:00.001814\n",
      "Elapsed: 0:00:01.011932\n",
      "Elapsed: 0:00:00.204562\n",
      "Elapsed: 0:00:00.203074\n",
      "Elapsed: 0:00:00.202003\n",
      "Elapsed: 0:00:00.216955\n",
      "Elapsed: 0:00:00.202302\n",
      "Elapsed: 0:00:00.204126\n",
      "Elapsed: 0:00:00.202377\n",
      "Elapsed: 0:00:00.203470\n",
      "Elapsed: 0:00:00.204201\n",
      "Elapsed: 0:00:00.204531\n",
      "Elapsed: 0:00:00.208897\n",
      "Elapsed: 0:00:00.205504\n",
      "Elapsed: 0:00:00.202623\n",
      "Elapsed: 0:00:00.212998\n",
      "Elapsed: 0:00:00.202169\n",
      "Elapsed: 0:00:00.205171\n",
      "Elapsed: 0:00:00.203919\n",
      "Elapsed: 0:00:00.202369\n",
      "Elapsed: 0:00:00.204928\n",
      "Elapsed: 0:00:00.204595\n",
      "Elapsed: 0:00:00.204061\n",
      "Elapsed: 0:00:00.202833\n",
      "Elapsed: 0:00:00.220997\n",
      "Elapsed: 0:00:00.204168\n",
      "Elapsed: 0:00:00.203302\n",
      "Elapsed: 0:00:00.205397\n",
      "Elapsed: 0:00:00.203425\n",
      "Elapsed: 0:00:00.204802\n",
      "Elapsed: 0:00:00.204999\n",
      "Elapsed: 0:00:00.204251\n",
      "Elapsed: 0:00:00.205115\n",
      "Elapsed: 0:00:00.204953\n",
      "Elapsed: 0:00:00.208177\n",
      "Elapsed: 0:00:00.205077\n",
      "Elapsed: 0:00:00.209688\n",
      "Elapsed: 0:00:00.202829\n",
      "Elapsed: 0:00:00.203969\n",
      "Elapsed: 0:00:00.204948\n",
      "Elapsed: 0:00:00.202052\n",
      "Elapsed: 0:00:00.205529\n",
      "Elapsed: 0:00:00.205521\n",
      "Elapsed: 0:00:00.205293\n",
      "Elapsed: 0:00:00.208099\n",
      "Elapsed: 0:00:00.205514\n",
      "Elapsed: 0:00:00.205347\n",
      "Elapsed: 0:00:00.205678\n",
      "Elapsed: 0:00:00.203772\n",
      "Elapsed: 0:00:00.208483\n",
      "Elapsed: 0:00:00.205965\n",
      "Elapsed: 0:00:00.205690\n",
      "Elapsed: 0:00:00.216948\n",
      "Elapsed: 0:00:00.210622\n",
      "Elapsed: 0:00:00.203736\n",
      "Elapsed: 0:00:00.205974\n",
      "Elapsed: 0:00:00.209702\n",
      "Elapsed: 0:00:00.206264\n",
      "Elapsed: 0:00:00.207563\n",
      "Elapsed: 0:00:00.206571\n",
      "Elapsed: 0:00:00.206316\n",
      "Elapsed: 0:00:00.206447\n",
      "Elapsed: 0:00:00.208716\n",
      "Elapsed: 0:00:00.203854\n",
      "Elapsed: 0:00:00.203682\n",
      "Elapsed: 0:00:00.206135\n",
      "Elapsed: 0:00:00.205945\n",
      "Elapsed: 0:00:00.204816\n",
      "Elapsed: 0:00:00.204099\n",
      "Elapsed: 0:00:00.212097\n",
      "Elapsed: 0:00:00.205968\n",
      "Elapsed: 0:00:00.206439\n",
      "Elapsed: 0:00:00.204998\n",
      "Elapsed: 0:00:00.205570\n",
      "Elapsed: 0:00:00.206354\n",
      "Elapsed: 0:00:00.204632\n",
      "Elapsed: 0:00:00.206838\n",
      "Elapsed: 0:00:00.206481\n",
      "Elapsed: 0:00:00.206981\n",
      "Elapsed: 0:00:00.206305\n",
      "Elapsed: 0:00:00.208679\n",
      "Elapsed: 0:00:00.208916\n",
      "Elapsed: 0:00:00.209055\n",
      "Elapsed: 0:00:00.205901\n",
      "Elapsed: 0:00:00.206315\n",
      "Elapsed: 0:00:00.204314\n",
      "Elapsed: 0:00:00.206470\n",
      "Elapsed: 0:00:00.208892\n",
      "Elapsed: 0:00:00.206494\n",
      "Elapsed: 0:00:00.205184\n",
      "Elapsed: 0:00:00.204914\n",
      "Elapsed: 0:00:00.206545\n",
      "Elapsed: 0:00:00.204572\n",
      "Elapsed: 0:00:00.203745\n",
      "Elapsed: 0:00:00.219842\n",
      "Elapsed: 0:00:00.205117\n",
      "Elapsed: 0:00:00.206583\n",
      "Elapsed: 0:00:00.208268\n",
      "Elapsed: 0:00:00.211137\n",
      "Elapsed: 0:00:00.207646\n",
      "Elapsed: 0:00:00.207546\n",
      "Elapsed: 0:00:00.204839\n",
      "Elapsed: 0:00:00.206920\n",
      "Elapsed: 0:00:00.208163\n",
      "Elapsed: 0:00:00.207321\n",
      "Elapsed: 0:00:00.002315\n",
      "Elapsed: 0:00:00.004756\n",
      "Elapsed: 0:00:00.003219\n",
      "Elapsed: 0:00:00.002653\n",
      "Elapsed: 0:00:01.024141\n",
      "Elapsed: 0:00:00.205154\n",
      "Elapsed: 0:00:00.207422\n",
      "Elapsed: 0:00:00.206030\n",
      "Elapsed: 0:00:00.207111\n",
      "Elapsed: 0:00:00.218362\n",
      "Elapsed: 0:00:00.207426\n",
      "Elapsed: 0:00:00.207978\n",
      "Elapsed: 0:00:00.206049\n",
      "Elapsed: 0:00:00.211620\n",
      "Elapsed: 0:00:00.207467\n",
      "Elapsed: 0:00:00.209132\n",
      "Elapsed: 0:00:00.211161\n",
      "Elapsed: 0:00:00.207506\n",
      "Elapsed: 0:00:00.207998\n",
      "Elapsed: 0:00:00.206765\n",
      "Elapsed: 0:00:00.207873\n",
      "Elapsed: 0:00:00.209790\n",
      "Elapsed: 0:00:00.204555\n",
      "Elapsed: 0:00:00.209643\n",
      "Elapsed: 0:00:00.207956\n",
      "Elapsed: 0:00:00.207583\n",
      "Elapsed: 0:00:00.207275\n",
      "Elapsed: 0:00:00.207029\n",
      "Elapsed: 0:00:00.207256\n",
      "Elapsed: 0:00:00.206343\n",
      "Elapsed: 0:00:00.206793\n",
      "Elapsed: 0:00:00.207515\n",
      "Elapsed: 0:00:00.205130\n",
      "Elapsed: 0:00:00.207417\n",
      "Elapsed: 0:00:00.208555\n",
      "Elapsed: 0:00:00.207423\n",
      "Elapsed: 0:00:00.208058\n",
      "Elapsed: 0:00:00.206622\n",
      "Elapsed: 0:00:00.212478\n",
      "Elapsed: 0:00:00.209838\n",
      "Elapsed: 0:00:00.208461\n",
      "Elapsed: 0:00:00.208285\n",
      "Elapsed: 0:00:00.208142\n",
      "Elapsed: 0:00:00.208193\n",
      "Elapsed: 0:00:00.217233\n",
      "Elapsed: 0:00:00.207842\n",
      "Elapsed: 0:00:00.208474\n",
      "Elapsed: 0:00:00.207899\n",
      "Elapsed: 0:00:00.208097\n",
      "Elapsed: 0:00:00.208804\n",
      "Elapsed: 0:00:00.208736\n",
      "Elapsed: 0:00:00.206529\n",
      "Elapsed: 0:00:00.210388\n",
      "Elapsed: 0:00:00.208453\n",
      "Elapsed: 0:00:00.206152\n",
      "Elapsed: 0:00:00.208944\n",
      "Elapsed: 0:00:00.206395\n",
      "Elapsed: 0:00:00.211387\n",
      "Elapsed: 0:00:00.211599\n",
      "Elapsed: 0:00:00.206040\n",
      "Elapsed: 0:00:00.208094\n",
      "Elapsed: 0:00:00.208583\n",
      "Elapsed: 0:00:00.206693\n",
      "Elapsed: 0:00:00.209221\n",
      "Elapsed: 0:00:00.206407\n",
      "Elapsed: 0:00:00.221207\n",
      "Elapsed: 0:00:00.212732\n",
      "Elapsed: 0:00:00.208807\n",
      "Elapsed: 0:00:00.207221\n",
      "Elapsed: 0:00:00.209215\n",
      "Elapsed: 0:00:00.209190\n",
      "Elapsed: 0:00:00.209027\n",
      "Elapsed: 0:00:00.206925\n",
      "Elapsed: 0:00:00.206879\n",
      "Elapsed: 0:00:00.209015\n",
      "Elapsed: 0:00:00.208050\n",
      "Elapsed: 0:00:00.208870\n",
      "Elapsed: 0:00:00.209475\n",
      "Elapsed: 0:00:00.212871\n",
      "Elapsed: 0:00:00.205954\n",
      "Elapsed: 0:00:00.212962\n",
      "Elapsed: 0:00:00.209556\n",
      "Elapsed: 0:00:00.207407\n",
      "Elapsed: 0:00:00.207331\n",
      "Elapsed: 0:00:00.207972\n",
      "Elapsed: 0:00:00.209883\n",
      "Elapsed: 0:00:00.215678\n",
      "Elapsed: 0:00:00.209191\n",
      "Elapsed: 0:00:00.209647\n",
      "Elapsed: 0:00:00.209423\n",
      "Elapsed: 0:00:00.209356\n",
      "Elapsed: 0:00:00.207917\n",
      "Elapsed: 0:00:00.210304\n",
      "Elapsed: 0:00:00.208278\n",
      "Elapsed: 0:00:00.211301\n",
      "Elapsed: 0:00:00.215228\n",
      "Elapsed: 0:00:00.212492\n",
      "Elapsed: 0:00:00.210152\n",
      "Elapsed: 0:00:00.215016\n",
      "Elapsed: 0:00:00.217809\n",
      "Elapsed: 0:00:00.212408\n",
      "Elapsed: 0:00:00.217488\n",
      "Elapsed: 0:00:00.215520\n",
      "Elapsed: 0:00:00.220191\n",
      "Elapsed: 0:00:00.211250\n",
      "Elapsed: 0:00:00.213027\n",
      "Elapsed: 0:00:00.212506\n",
      "Elapsed: 0:00:00.213449\n",
      "Elapsed: 0:00:00.001912\n",
      "Elapsed: 0:00:00.001361\n",
      "Elapsed: 0:00:00.001269\n",
      "Elapsed: 0:00:00.001181\n",
      "Elapsed: 0:00:01.065223\n",
      "Elapsed: 0:00:00.212763\n",
      "Elapsed: 0:00:00.219337\n",
      "Elapsed: 0:00:00.213157\n",
      "Elapsed: 0:00:00.219363\n",
      "Elapsed: 0:00:00.212731\n",
      "Elapsed: 0:00:00.212112\n",
      "Elapsed: 0:00:00.210547\n",
      "Elapsed: 0:00:00.213752\n",
      "Elapsed: 0:00:00.211523\n",
      "Elapsed: 0:00:00.214058\n",
      "Elapsed: 0:00:00.213164\n",
      "Elapsed: 0:00:00.221136\n",
      "Elapsed: 0:00:00.210989\n",
      "Elapsed: 0:00:00.216673\n",
      "Elapsed: 0:00:00.211833\n",
      "Elapsed: 0:00:00.214285\n",
      "Elapsed: 0:00:00.215552\n",
      "Elapsed: 0:00:00.215411\n",
      "Elapsed: 0:00:00.218677\n",
      "Elapsed: 0:00:00.213715\n",
      "Elapsed: 0:00:00.213088\n",
      "Elapsed: 0:00:00.212187\n",
      "Elapsed: 0:00:00.219971\n",
      "Elapsed: 0:00:00.214135\n",
      "Elapsed: 0:00:00.212785\n",
      "Elapsed: 0:00:00.213674\n",
      "Elapsed: 0:00:00.213900\n",
      "Elapsed: 0:00:00.223153\n",
      "Elapsed: 0:00:00.212053\n",
      "Elapsed: 0:00:00.213476\n",
      "Elapsed: 0:00:00.214271\n",
      "Elapsed: 0:00:00.213275\n",
      "Elapsed: 0:00:00.214651\n",
      "Elapsed: 0:00:00.212360\n",
      "Elapsed: 0:00:00.215818\n",
      "Elapsed: 0:00:00.211884\n",
      "Elapsed: 0:00:00.213492\n",
      "Elapsed: 0:00:00.211565\n",
      "Elapsed: 0:00:00.225605\n",
      "Elapsed: 0:00:00.214305\n",
      "Elapsed: 0:00:00.213747\n",
      "Elapsed: 0:00:00.212428\n",
      "Elapsed: 0:00:00.214895\n",
      "Elapsed: 0:00:00.214153\n",
      "Elapsed: 0:00:00.217774\n",
      "Elapsed: 0:00:00.211972\n",
      "Elapsed: 0:00:00.213215\n",
      "Elapsed: 0:00:00.214669\n",
      "Elapsed: 0:00:00.214528\n",
      "Elapsed: 0:00:00.218443\n",
      "Elapsed: 0:00:00.214245\n",
      "Elapsed: 0:00:00.215913\n",
      "Elapsed: 0:00:00.219033\n",
      "Elapsed: 0:00:00.215295\n",
      "Elapsed: 0:00:00.216220\n",
      "Elapsed: 0:00:00.214410\n",
      "Elapsed: 0:00:00.213767\n",
      "Elapsed: 0:00:00.212354\n",
      "Elapsed: 0:00:00.220910\n",
      "Elapsed: 0:00:00.216597\n",
      "Elapsed: 0:00:00.214135\n",
      "Elapsed: 0:00:00.216091\n",
      "Elapsed: 0:00:00.217317\n",
      "Elapsed: 0:00:00.214371\n",
      "Elapsed: 0:00:00.214169\n",
      "Elapsed: 0:00:00.219223\n",
      "Elapsed: 0:00:00.214431\n",
      "Elapsed: 0:00:00.219358\n",
      "Elapsed: 0:00:00.214508\n",
      "Elapsed: 0:00:00.223062\n",
      "Elapsed: 0:00:00.213529\n",
      "Elapsed: 0:00:00.214536\n",
      "Elapsed: 0:00:00.224278\n",
      "Elapsed: 0:00:00.223754\n",
      "Elapsed: 0:00:00.226008\n",
      "Elapsed: 0:00:00.215475\n",
      "Elapsed: 0:00:00.212640\n",
      "Elapsed: 0:00:00.215087\n",
      "Elapsed: 0:00:00.221855\n",
      "Elapsed: 0:00:00.223666\n",
      "Elapsed: 0:00:00.215185\n",
      "Elapsed: 0:00:00.213061\n",
      "Elapsed: 0:00:00.213095\n",
      "Elapsed: 0:00:00.212425\n",
      "Elapsed: 0:00:00.215232\n",
      "Elapsed: 0:00:00.216350\n",
      "Elapsed: 0:00:00.217374\n",
      "Elapsed: 0:00:00.215440\n",
      "Elapsed: 0:00:00.219551\n",
      "Elapsed: 0:00:00.215416\n",
      "Elapsed: 0:00:00.215211\n",
      "Elapsed: 0:00:00.216398\n",
      "Elapsed: 0:00:00.213513\n",
      "Elapsed: 0:00:00.230412\n",
      "Elapsed: 0:00:00.214433\n",
      "Elapsed: 0:00:00.215452\n",
      "Elapsed: 0:00:00.218959\n",
      "Elapsed: 0:00:00.215929\n",
      "Elapsed: 0:00:00.215280\n",
      "Elapsed: 0:00:00.214743\n",
      "Elapsed: 0:00:00.222091\n",
      "Elapsed: 0:00:00.216346\n",
      "Elapsed: 0:00:00.216447\n",
      "Elapsed: 0:00:00.002810\n",
      "Elapsed: 0:00:00.001694\n",
      "Elapsed: 0:00:00.000968\n",
      "Elapsed: 0:00:00.000929\n",
      "Elapsed: 0:00:01.090061\n",
      "Elapsed: 0:00:00.217968\n",
      "Elapsed: 0:00:00.216802\n",
      "Elapsed: 0:00:00.216413\n",
      "Elapsed: 0:00:00.217962\n",
      "Elapsed: 0:00:00.216145\n",
      "Elapsed: 0:00:00.226654\n",
      "Elapsed: 0:00:00.217400\n",
      "Elapsed: 0:00:00.218104\n",
      "Elapsed: 0:00:00.216252\n",
      "Elapsed: 0:00:00.217001\n",
      "Elapsed: 0:00:00.215463\n",
      "Elapsed: 0:00:00.224484\n",
      "Elapsed: 0:00:00.228606\n",
      "Elapsed: 0:00:00.215110\n",
      "Elapsed: 0:00:00.218601\n",
      "Elapsed: 0:00:00.216228\n",
      "Elapsed: 0:00:00.214872\n",
      "Elapsed: 0:00:00.223471\n",
      "Elapsed: 0:00:00.216824\n",
      "Elapsed: 0:00:00.216821\n",
      "Elapsed: 0:00:00.216765\n",
      "Elapsed: 0:00:00.216045\n",
      "Elapsed: 0:00:00.214960\n",
      "Elapsed: 0:00:00.227336\n",
      "Elapsed: 0:00:00.217852\n",
      "Elapsed: 0:00:00.215225\n",
      "Elapsed: 0:00:00.223172\n",
      "Elapsed: 0:00:00.222730\n",
      "Elapsed: 0:00:00.221258\n",
      "Elapsed: 0:00:00.214920\n",
      "Elapsed: 0:00:00.218945\n",
      "Elapsed: 0:00:00.216210\n",
      "Elapsed: 0:00:00.220104\n",
      "Elapsed: 0:00:00.216873\n",
      "Elapsed: 0:00:00.223023\n",
      "Elapsed: 0:00:00.215759\n",
      "Elapsed: 0:00:00.217368\n",
      "Elapsed: 0:00:00.214851\n",
      "Elapsed: 0:00:00.216987\n",
      "Elapsed: 0:00:00.216261\n",
      "Elapsed: 0:00:00.216154\n",
      "Elapsed: 0:00:00.219003\n",
      "Elapsed: 0:00:00.219993\n",
      "Elapsed: 0:00:00.219413\n",
      "Elapsed: 0:00:00.219497\n",
      "Elapsed: 0:00:00.217165\n",
      "Elapsed: 0:00:00.228750\n",
      "Elapsed: 0:00:00.224922\n",
      "Elapsed: 0:00:00.222679\n",
      "Elapsed: 0:00:00.222473\n",
      "Elapsed: 0:00:00.215494\n",
      "Elapsed: 0:00:00.216844\n",
      "Elapsed: 0:00:00.226186\n",
      "Elapsed: 0:00:00.217741\n",
      "Elapsed: 0:00:00.228766\n",
      "Elapsed: 0:00:00.219141\n",
      "Elapsed: 0:00:00.223238\n",
      "Elapsed: 0:00:00.216032\n",
      "Elapsed: 0:00:00.230138\n",
      "Elapsed: 0:00:00.215499\n",
      "Elapsed: 0:00:00.223575\n",
      "Elapsed: 0:00:00.216893\n",
      "Elapsed: 0:00:00.224768\n",
      "Elapsed: 0:00:00.226249\n",
      "Elapsed: 0:00:00.217570\n",
      "Elapsed: 0:00:00.215843\n",
      "Elapsed: 0:00:00.219186\n",
      "Elapsed: 0:00:00.219237\n",
      "Elapsed: 0:00:00.215274\n",
      "Elapsed: 0:00:00.224858\n",
      "Elapsed: 0:00:00.216061\n",
      "Elapsed: 0:00:00.216258\n",
      "Elapsed: 0:00:00.222182\n",
      "Elapsed: 0:00:00.225463\n",
      "Elapsed: 0:00:00.217846\n",
      "Elapsed: 0:00:00.216689\n",
      "Elapsed: 0:00:00.221089\n",
      "Elapsed: 0:00:00.221606\n",
      "Elapsed: 0:00:00.220114\n",
      "Elapsed: 0:00:00.217931\n",
      "Elapsed: 0:00:00.216757\n",
      "Elapsed: 0:00:00.218063\n",
      "Elapsed: 0:00:00.222885\n",
      "Elapsed: 0:00:00.224063\n",
      "Elapsed: 0:00:00.216828\n",
      "Elapsed: 0:00:00.218690\n",
      "Elapsed: 0:00:00.216634\n",
      "Elapsed: 0:00:00.218654\n",
      "Elapsed: 0:00:00.218075\n",
      "Elapsed: 0:00:00.224086\n",
      "Elapsed: 0:00:00.228227\n",
      "Elapsed: 0:00:00.217173\n",
      "Elapsed: 0:00:00.218427\n",
      "Elapsed: 0:00:00.219831\n",
      "Elapsed: 0:00:00.220059\n",
      "Elapsed: 0:00:00.223897\n",
      "Elapsed: 0:00:00.218480\n",
      "Elapsed: 0:00:00.216959\n",
      "Elapsed: 0:00:00.217605\n",
      "Elapsed: 0:00:00.219263\n",
      "Elapsed: 0:00:00.226582\n",
      "Elapsed: 0:00:00.218597\n",
      "Elapsed: 0:00:00.218626\n",
      "Elapsed: 0:00:00.001285\n",
      "Elapsed: 0:00:00.000884\n",
      "Elapsed: 0:00:00.000840\n",
      "Elapsed: 0:00:00.000882\n",
      "Elapsed: 0:00:01.105942\n",
      "Elapsed: 0:00:00.216030\n",
      "Elapsed: 0:00:00.217001\n",
      "Elapsed: 0:00:00.218857\n",
      "Elapsed: 0:00:00.224817\n",
      "Elapsed: 0:00:00.225683\n",
      "Elapsed: 0:00:00.220595\n",
      "Elapsed: 0:00:00.217342\n",
      "Elapsed: 0:00:00.217014\n",
      "Elapsed: 0:00:00.217034\n",
      "Elapsed: 0:00:00.219802\n",
      "Elapsed: 0:00:00.219008\n",
      "Elapsed: 0:00:00.217991\n",
      "Elapsed: 0:00:00.219256\n",
      "Elapsed: 0:00:00.223383\n",
      "Elapsed: 0:00:00.217125\n",
      "Elapsed: 0:00:00.224624\n",
      "Elapsed: 0:00:00.232097\n",
      "Elapsed: 0:00:00.220164\n",
      "Elapsed: 0:00:00.219729\n",
      "Elapsed: 0:00:00.218303\n",
      "Elapsed: 0:00:00.221292\n",
      "Elapsed: 0:00:00.219278\n",
      "Elapsed: 0:00:00.232101\n",
      "Elapsed: 0:00:00.222422\n",
      "Elapsed: 0:00:00.219317\n",
      "Elapsed: 0:00:00.219083\n",
      "Elapsed: 0:00:00.219383\n",
      "Elapsed: 0:00:00.217350\n",
      "Elapsed: 0:00:00.222568\n",
      "Elapsed: 0:00:00.219080\n",
      "Elapsed: 0:00:00.218875\n",
      "Elapsed: 0:00:00.220520\n",
      "Elapsed: 0:00:00.219961\n",
      "Elapsed: 0:00:00.224046\n",
      "Elapsed: 0:00:00.219993\n",
      "Elapsed: 0:00:00.220217\n",
      "Elapsed: 0:00:00.220439\n",
      "Elapsed: 0:00:00.221080\n",
      "Elapsed: 0:00:00.219978\n",
      "Elapsed: 0:00:00.219473\n",
      "Elapsed: 0:00:00.218312\n",
      "Elapsed: 0:00:00.219763\n",
      "Elapsed: 0:00:00.221981\n",
      "Elapsed: 0:00:00.218031\n",
      "Elapsed: 0:00:00.219892\n",
      "Elapsed: 0:00:00.220288\n",
      "Elapsed: 0:00:00.218463\n",
      "Elapsed: 0:00:00.218135\n",
      "Elapsed: 0:00:00.228860\n",
      "Elapsed: 0:00:00.220810\n",
      "Elapsed: 0:00:00.217651\n",
      "Elapsed: 0:00:00.219857\n",
      "Elapsed: 0:00:00.227494\n",
      "Elapsed: 0:00:00.218963\n",
      "Elapsed: 0:00:00.228464\n",
      "Elapsed: 0:00:00.220751\n",
      "Elapsed: 0:00:00.221555\n",
      "Elapsed: 0:00:00.217867\n",
      "Elapsed: 0:00:00.220292\n",
      "Elapsed: 0:00:00.218406\n",
      "Elapsed: 0:00:00.219749\n",
      "Elapsed: 0:00:00.220650\n",
      "Elapsed: 0:00:00.220028\n",
      "Elapsed: 0:00:00.220012\n",
      "Elapsed: 0:00:00.221770\n",
      "Elapsed: 0:00:00.224063\n",
      "Elapsed: 0:00:00.219326\n",
      "Elapsed: 0:00:00.219184\n",
      "Elapsed: 0:00:00.218323\n",
      "Elapsed: 0:00:00.221202\n",
      "Elapsed: 0:00:00.220725\n",
      "Elapsed: 0:00:00.220609\n",
      "Elapsed: 0:00:00.223136\n",
      "Elapsed: 0:00:00.220394\n",
      "Elapsed: 0:00:00.221656\n",
      "Elapsed: 0:00:00.218817\n",
      "Elapsed: 0:00:00.219323\n",
      "Elapsed: 0:00:00.220925\n",
      "Elapsed: 0:00:00.219096\n",
      "Elapsed: 0:00:00.221917\n",
      "Elapsed: 0:00:00.220716\n",
      "Elapsed: 0:00:00.220890\n",
      "Elapsed: 0:00:00.220595\n",
      "Elapsed: 0:00:00.224486\n",
      "Elapsed: 0:00:00.225836\n",
      "Elapsed: 0:00:00.221434\n",
      "Elapsed: 0:00:00.226207\n",
      "Elapsed: 0:00:00.220006\n",
      "Elapsed: 0:00:00.220048\n",
      "Elapsed: 0:00:00.225244\n",
      "Elapsed: 0:00:00.220514\n",
      "Elapsed: 0:00:00.221439\n",
      "Elapsed: 0:00:00.221697\n",
      "Elapsed: 0:00:00.222038\n",
      "Elapsed: 0:00:00.222518\n",
      "Elapsed: 0:00:00.222186\n",
      "Elapsed: 0:00:00.221090\n",
      "Elapsed: 0:00:00.220806\n",
      "Elapsed: 0:00:00.230072\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "integer division or modulo by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoded_output\n\u001b[1;32m     12\u001b[0m start \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m---> 14\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mElapsed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m-\u001b[39mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 5\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m      2\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp\u001b[39m\u001b[38;5;124m\"\u001b[39m, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {k: Tensor(v, device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m----> 5\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# decoded_output = tokenizer.decode(outputs[0][inputs['input_ids'].size(1):], skip_special_tokens=True)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m decoded_output \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m][inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m):])\n",
      "File \u001b[0;32m~/Projects/Timestep-AI/.github-private/submodules/timestep/notebooks/Research/lib/transformers/src/transformers/models/llama/modeling_tinygrad_llama.py:422\u001b[0m, in \u001b[0;36mTinygradLlamaForCausalLM.generate\u001b[0;34m(self, input_ids, max_new_tokens, temperature, **kwargs)\u001b[0m\n\u001b[1;32m    418\u001b[0m start_i \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    420\u001b[0m tokens \u001b[38;5;241m=\u001b[39m Tensor(token_ids[:, start_pos:])\n\u001b[0;32m--> 422\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m   \u001b[49m\u001b[43mstart_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m   \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m   \u001b[49m\u001b[43mtokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;66;03m# ).realize()\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m logits\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mvocab_size)\n",
      "File \u001b[0;32m~/Projects/Timestep-AI/.github-private/submodules/timestep/notebooks/Research/lib/transformers/src/transformers/models/llama/modeling_tinygrad_llama.py:194\u001b[0m, in \u001b[0;36mTransformer.__call__\u001b[0;34m(self, tokens, start_pos, temperature, top_k, top_p, alpha_f, alpha_p)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m==\u001b[39m (\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_jit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_jit(tokens, Variable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_pos\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_context)\u001b[38;5;241m.\u001b[39mbind(start_pos), temperature, top_k, top_p, alpha_f, alpha_p)\n\u001b[0;32m--> 194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_p\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Timestep-AI/.github-private/submodules/timestep/notebooks/Research/lib/transformers/src/transformers/models/llama/modeling_tinygrad_llama.py:183\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, tokens, start_pos, temperature, top_k, top_p, alpha_f, alpha_p)\u001b[0m\n\u001b[1;32m    181\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtok_embeddings(tokens)\n\u001b[1;32m    182\u001b[0m mask \u001b[38;5;241m=\u001b[39m Tensor\u001b[38;5;241m.\u001b[39mfull((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, seqlen, start_pos\u001b[38;5;241m+\u001b[39mseqlen), \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m\"\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mh\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mh\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mtriu(start_pos\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrealize() \u001b[38;5;28;01mif\u001b[39;00m seqlen \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers: h \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(h))\u001b[38;5;241m.\u001b[39mfloat()[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# return sample(logits.flatten(), temperature, top_k, top_p, alpha_f, alpha_p).realize()\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/Timestep-AI/.github-private/submodules/timestep/notebooks/Research/lib/transformers/src/transformers/models/llama/modeling_tinygrad_llama.py:113\u001b[0m, in \u001b[0;36mTransformerBlock.__call__\u001b[0;34m(self, x, start_pos, freqs_cis, mask)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x:Tensor, start_pos:Union[Variable,\u001b[38;5;28mint\u001b[39m], freqs_cis:Tensor, mask:Optional[Tensor]):\n\u001b[0;32m--> 113\u001b[0m   h \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m (h \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn_norm(h)))\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[0;32m~/Projects/Timestep-AI/.github-private/submodules/timestep/notebooks/Research/lib/transformers/src/transformers/models/llama/modeling_tinygrad_llama.py:73\u001b[0m, in \u001b[0;36mAttention.__call__\u001b[0;34m(self, x, start_pos, freqs_cis, mask)\u001b[0m\n\u001b[1;32m     70\u001b[0m xk \u001b[38;5;241m=\u001b[39m xk\u001b[38;5;241m.\u001b[39mreshape(xk\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], xk\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_kv_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m     71\u001b[0m xv \u001b[38;5;241m=\u001b[39m xv\u001b[38;5;241m.\u001b[39mreshape(xv\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], xv\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_kv_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[0;32m---> 73\u001b[0m xq, xk \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rotary_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m bsz, seqlen, _, _ \u001b[38;5;241m=\u001b[39m xq\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# create kv cache\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/Timestep-AI/.github-private/submodules/timestep/notebooks/Research/lib/transformers/src/transformers/models/llama/modeling_tinygrad_llama.py:34\u001b[0m, in \u001b[0;36mapply_rotary_emb\u001b[0;34m(xq, xk, freqs_cis)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_rotary_emb\u001b[39m(xq:Tensor, xk:Tensor, freqs_cis:Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Tensor, Tensor]:\n\u001b[1;32m     33\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m freqs_cis\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m xq\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m xk\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfreqs_cis shape mismatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfreqs_cis\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m xq:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mxq\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m xk:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mxk\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 34\u001b[0m   xq \u001b[38;5;241m=\u001b[39m \u001b[43mxq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mxq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m   xk \u001b[38;5;241m=\u001b[39m xk\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39mxk\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     36\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(xq\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(xk\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(freqs_cis\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m\n",
      "File \u001b[0;32m~/Projects/Timestep-AI/.github-private/submodules/timestep/.venv/lib/python3.10/site-packages/tinygrad/tensor.py:3256\u001b[0m, in \u001b[0;36m_metadata_wrapper.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   3253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: caller \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3255\u001b[0m token \u001b[38;5;241m=\u001b[39m _METADATA\u001b[38;5;241m.\u001b[39mset(Metadata(name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, caller\u001b[38;5;241m=\u001b[39mcaller))\n\u001b[0;32m-> 3256\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3257\u001b[0m _METADATA\u001b[38;5;241m.\u001b[39mreset(token)\n\u001b[1;32m   3258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/Projects/Timestep-AI/.github-private/submodules/timestep/.venv/lib/python3.10/site-packages/tinygrad/tensor.py:800\u001b[0m, in \u001b[0;36mTensor.reshape\u001b[0;34m(self, shape, *args)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;66;03m# resolve -1\u001b[39;00m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (c \u001b[38;5;241m:=\u001b[39m new_shape\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m: \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly one dimension can be inferred using -1, getting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m c: new_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([\u001b[38;5;241m-\u001b[39mprod(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m prod(new_shape) \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m new_shape])\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mReshape\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mself\u001b[39m, shape\u001b[38;5;241m=\u001b[39mnew_shape) \u001b[38;5;28;01mif\u001b[39;00m new_shape \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Projects/Timestep-AI/.github-private/submodules/timestep/.venv/lib/python3.10/site-packages/tinygrad/tensor.py:800\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;66;03m# resolve -1\u001b[39;00m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (c \u001b[38;5;241m:=\u001b[39m new_shape\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m: \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly one dimension can be inferred using -1, getting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m c: new_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mprod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_shape\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m new_shape])\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mReshape\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mself\u001b[39m, shape\u001b[38;5;241m=\u001b[39mnew_shape) \u001b[38;5;28;01mif\u001b[39;00m new_shape \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: integer division or modulo by zero"
     ]
    }
   ],
   "source": [
    "def predict(prompt: str):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"np\", add_special_tokens=False)\n",
    "    inputs = {k: Tensor(v, device=device) for k, v in inputs.items()}\n",
    "\n",
    "    outputs = model.generate(**inputs, do_sample=True, max_new_tokens=256, temperature=0.0)\n",
    "\n",
    "    # decoded_output = tokenizer.decode(outputs[0][inputs['input_ids'].size(1):], skip_special_tokens=True)\n",
    "    decoded_output = tokenizer.decode(outputs[0][inputs['input_ids'].size(1):])\n",
    "\n",
    "    return decoded_output\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "prediction = predict(prompt)\n",
    "\n",
    "print(f\"Elapsed: {datetime.now()-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_output = generate_prompt(\n",
    "    add_generation_prompt=False,\n",
    "    messages=conversation[\"messages\"][2:],\n",
    "    tool_calls=True,\n",
    "    tools=conversation[\"tools\"],\n",
    ")\n",
    "\n",
    "expected_output = \"\\n\".join(expected_output.split(\"\\n\")[1:]) # Skip the first line\n",
    "\n",
    "print('expected_output:')\n",
    "print(expected_output)\n",
    "\n",
    "decoded_output = predict(prompt)\n",
    "\n",
    "print('decoded_output:')\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def compute_similarity_metric(prediction: str, target: str):\n",
    "    s_1 = SequenceMatcher(None, prediction, target)\n",
    "    s_2 = SequenceMatcher(None, target, prediction)\n",
    "\n",
    "    return (s_1.ratio() + s_2.ratio()) / 2\n",
    "\n",
    "similarity_metric = compute_similarity_metric(decoded_output, expected_output)\n",
    "print('similarity_metric:', similarity_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompts = [\n",
    "    generate_prompt(\n",
    "        # add_generation_prompt=False,\n",
    "        add_generation_prompt=True,\n",
    "        messages=conversation[\"messages\"][0:2], # TODO: do this until we reach the first assistant message where where \"weight\" is undefined or 1\n",
    "        tools=conversation[\"tools\"],\n",
    "    )\n",
    "    for conversation in conversations\n",
    "]\n",
    "\n",
    "ground_truth_responses = [\n",
    "    generate_prompt(\n",
    "        add_generation_prompt=False,\n",
    "        messages=conversation[\"messages\"][2:], # TODO: Start at the fisrt assistant message where \"weight\" is undefined or 1\n",
    "        tools=conversation[\"tools\"],\n",
    "    )\n",
    "    for conversation in conversations\n",
    "]\n",
    "\n",
    "# expected_output = \"\\n\".join(expected_output.split(\"\\n\")[1:]) # Skip the first line\n",
    "\n",
    "for i in range(len(ground_truth_responses)):\n",
    "    ground_truth_responses[i] = \"\\n\".join(ground_truth_responses[i].split(\"\\n\")[1:]) # Skip the first line\n",
    "\n",
    "len(input_prompts), len(ground_truth_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange\n",
    "\n",
    "stop_after = len(input_prompts)\n",
    "\n",
    "eval_df = pd.DataFrame({\n",
    "    \"inputs\": input_prompts[:stop_after],\n",
    "    \"ground_truth\": ground_truth_responses[:stop_after],\n",
    "})\n",
    "predictions = []\n",
    "\n",
    "# for i, row in tqdm(eval_df.iterrows()):\n",
    "for i in trange(len(eval_df)):\n",
    "    # print('i: ', i)\n",
    "    row = eval_df.iloc[i]\n",
    "\n",
    "    if i >= stop_after:\n",
    "        break\n",
    "\n",
    "    input_prompt = row[\"inputs\"]\n",
    "    ground_truth_response = row[\"ground_truth\"]\n",
    "\n",
    "    # print('input_prompt: ')\n",
    "    # print(input_prompt)\n",
    "    # print('ground_truth_response: ')\n",
    "    # print(ground_truth_response)\n",
    "\n",
    "    prediction = predict(input_prompt)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "    # print('prediction: ')\n",
    "    # print(prediction)\n",
    "\n",
    "eval_df[\"predictions\"] = predictions\n",
    "\n",
    "eval_df[\"similarity_metric\"] = eval_df.apply(lambda row: compute_similarity_metric(row[\"predictions\"], row[\"ground_truth\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('prompt:')\n",
    "print(eval_df[\"inputs\"].values[0])\n",
    "\n",
    "print('target:')\n",
    "print(eval_df[\"ground_truth\"].values[0])\n",
    "\n",
    "print('prediction:')\n",
    "print(eval_df[\"predictions\"].values[0])\n",
    "\n",
    "print('similarity_metric:')\n",
    "print(eval_df[\"similarity_metric\"].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"eval_df.jsonl\", \"w\") as f:\n",
    "    eval_df.to_json(f, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df_loaded = pd.read_json(\"eval_df.jsonl\", orient=\"records\", lines=True)\n",
    "eval_df_loaded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
