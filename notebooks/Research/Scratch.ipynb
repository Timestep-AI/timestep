{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tinygrad and CUDA device.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "from tinygrad import Device, Tensor, nn\n",
    "from tinygrad.nn.state import get_state_dict, safe_load, torch_load, load_state_dict, get_parameters, safe_save\n",
    "from transformers import AutoTokenizer, AutoConfig, DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "from timestep.config import settings\n",
    "\n",
    "device = Device.DEFAULT\n",
    "print(f'Using Tinygrad and {device} device.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_repo_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(settings.app_dir, f\"models/{hf_repo_id}/model.safetensors\")\n",
    "tokenizer_path = os.path.join(settings.app_dir, f\"models/{hf_repo_id}/tokenizer.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bos_token_id: 1; bos_token: <s>\n",
      "eos_token_id: 2; eos_token: </s>\n",
      "unk_token_id: 0; unk_token: <unk>\n",
      "Note that tokenizer.pad_id() is not the same as hf_tokenizer.pad_token_id, i.e. -1 != 2\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", eos_token=\"<|im_end|>\")\n",
    "# hf_tokenizer = AutoTokenizer.from_pretrained(hf_repo_id, pad_token=\"</s>\")\n",
    "# hf_tokenizer = AutoTokenizer.from_pretrained(hf_repo_id, bos_token=\"<|im_start|>\", eos_token=\"<|im_end|>\")\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(hf_repo_id)\n",
    "tokenizer = SentencePieceProcessor(model_file=str(tokenizer_path))\n",
    "\n",
    "print(f\"bos_token_id: {tokenizer.bos_id()}; bos_token: {tokenizer.id_to_piece(tokenizer.bos_id())}\")\n",
    "print(f\"eos_token_id: {tokenizer.eos_id()}; eos_token: {tokenizer.id_to_piece(tokenizer.eos_id())}\")\n",
    "# print(f\"pad_token_id: {tokenizer.pad_id()}; pad_token: {tokenizer.id_to_piece(tokenizer.pad_id())}\")\n",
    "print(f\"unk_token_id: {tokenizer.unk_id()}; unk_token: {tokenizer.id_to_piece(tokenizer.unk_id())}\")\n",
    "\n",
    "assert tokenizer.bos_id() == hf_tokenizer.bos_token_id, f\"{tokenizer.bos_id()} != {hf_tokenizer.bos_token_id}\"\n",
    "assert tokenizer.eos_id() == hf_tokenizer.eos_token_id, f\"{tokenizer.eos_id()} != {hf_tokenizer.eos_token_id}\"\n",
    "# assert tokenizer.pad_id() == hf_tokenizer.pad_token_id, f\"{tokenizer.pad_id()} != {hf_tokenizer.pad_token_id}\"\n",
    "assert tokenizer.unk_id() == hf_tokenizer.unk_token_id, f\"{tokenizer.unk_id()} != {hf_tokenizer.unk_token_id}\"\n",
    "\n",
    "print(f\"Note that tokenizer.pad_id() is not the same as hf_tokenizer.pad_token_id, i.e. {tokenizer.pad_id()} != {hf_tokenizer.pad_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(\"template.jinja\", \"w\").write(hf_tokenizer.chat_template)\n",
    "\n",
    "hf_tokenizer.chat_template = open(\"template.jinja\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations = []\n",
    "\n",
    "with open(\"../../data/drone_training.jsonl\") as f:\n",
    "    for line in f:\n",
    "        conversations.append(json.loads(line))\n",
    "\n",
    "conversation = conversations[0]\n",
    "\n",
    "# print('messages: ', conversation[\"messages\"])\n",
    "\n",
    "# print('parallel_tool_calls: ', conversation[\"parallel_tool_calls\"])\n",
    "\n",
    "# print('tools: ', conversation[\"tools\"])\n",
    "\n",
    "# conversation = conversation[\"messages\"][0:2]\n",
    "# conversation = conversation[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are an intelligent AI that controls a drone. Given a command or request from the user,\n",
      "call one of your functions to complete the request. If the request cannot be completed by your available functions, call the reject_request function.\n",
      "If the request is ambiguous or unclear, reject the request.</s>\n",
      "<|user|>\n",
      "Let's get the drone in the air, how high should it go?</s>\n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = hf_tokenizer.apply_chat_template(\n",
    "    add_generation_prompt=True,\n",
    "    # add_special_tokens=True,\n",
    "    # conversation=conversation,\n",
    "    conversation=conversation[\"messages\"][0:2], # Skip tool messages for now\n",
    "    # return_dict=True,\n",
    "    return_tensors=False,\n",
    "    tokenize=False,\n",
    "    tools=conversation[\"tools\"],\n",
    ")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[529, 29989, 5205, 29989, 29958, 13, 3492, 526, 385, 13052, 296, 319, 29902, 393, 11761, 263, 4192, 650, 29889, 11221, 263, 1899, 470, 2009, 515, 278, 1404, 29892, 13, 4804, 697, 310, 596, 3168, 304, 4866, 278, 2009, 29889, 960, 278, 2009, 2609, 367, 8676, 491, 596, 3625, 3168, 29892, 1246, 278, 12560, 29918, 3827, 740, 29889, 13, 3644, 278, 2009, 338, 22363, 681, 470, 20871, 29892, 12560, 278, 2009, 21106, 29879, 29958, 13, 29966, 29989, 1792, 29989, 29958, 13, 12024, 29915, 29879, 679, 278, 4192, 650, 297, 278, 4799, 29892, 920, 1880, 881, 372, 748, 29973, 829, 29879, 29958, 13, 29966, 29989, 465, 22137, 29989, 29958, 13]\n"
     ]
    }
   ],
   "source": [
    "encoded_prompt = tokenizer.encode(prompt)\n",
    "print(encoded_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁<', '|', 'system', '|', '>', '<0x0A>', 'You', '▁are', '▁an', '▁intellig', 'ent', '▁A', 'I', '▁that', '▁controls', '▁a', '▁dr', 'one', '.', '▁Given', '▁a', '▁command', '▁or', '▁request', '▁from', '▁the', '▁user', ',', '<0x0A>', 'call', '▁one', '▁of', '▁your', '▁functions', '▁to', '▁complete', '▁the', '▁request', '.', '▁If', '▁the', '▁request', '▁cannot', '▁be', '▁completed', '▁by', '▁your', '▁available', '▁functions', ',', '▁call', '▁the', '▁reject', '_', 'request', '▁function', '.', '<0x0A>', 'If', '▁the', '▁request', '▁is', '▁ambigu', 'ous', '▁or', '▁unclear', ',', '▁reject', '▁the', '▁request', '.', '</s>', '▁', '<0x0A>', '<', '|', 'user', '|', '>', '<0x0A>', 'Let', \"'\", 's', '▁get', '▁the', '▁dr', 'one', '▁in', '▁the', '▁air', ',', '▁how', '▁high', '▁should', '▁it', '▁go', '?', '</s>', '▁', '<0x0A>', '<', '|', 'ass', 'istant', '|', '>', '<0x0A>']\n"
     ]
    }
   ],
   "source": [
    "hf_tokenized_prompt = hf_tokenizer.tokenize(prompt, add_special_tokens=False)\n",
    "print(hf_tokenized_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[529, 29989, 5205, 29989, 29958, 13, 3492, 526, 385, 13052, 296, 319, 29902, 393, 11761, 263, 4192, 650, 29889, 11221, 263, 1899, 470, 2009, 515, 278, 1404, 29892, 13, 4804, 697, 310, 596, 3168, 304, 4866, 278, 2009, 29889, 960, 278, 2009, 2609, 367, 8676, 491, 596, 3625, 3168, 29892, 1246, 278, 12560, 29918, 3827, 740, 29889, 13, 3644, 278, 2009, 338, 22363, 681, 470, 20871, 29892, 12560, 278, 2009, 29889, 2, 29871, 13, 29966, 29989, 1792, 29989, 29958, 13, 12024, 29915, 29879, 679, 278, 4192, 650, 297, 278, 4799, 29892, 920, 1880, 881, 372, 748, 29973, 2, 29871, 13, 29966, 29989, 465, 22137, 29989, 29958, 13]\n"
     ]
    }
   ],
   "source": [
    "hf_encoded_tokenized_prompt = hf_tokenizer.convert_tokens_to_ids(hf_tokenized_prompt)\n",
    "print(hf_encoded_tokenized_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[529, 29989, 5205, 29989, 29958, 13, 3492, 526, 385, 13052, 296, 319, 29902, 393, 11761, 263, 4192, 650, 29889, 11221, 263, 1899, 470, 2009, 515, 278, 1404, 29892, 13, 4804, 697, 310, 596, 3168, 304, 4866, 278, 2009, 29889, 960, 278, 2009, 2609, 367, 8676, 491, 596, 3625, 3168, 29892, 1246, 278, 12560, 29918, 3827, 740, 29889, 13, 3644, 278, 2009, 338, 22363, 681, 470, 20871, 29892, 12560, 278, 2009, 29889, 2, 29871, 13, 29966, 29989, 1792, 29989, 29958, 13, 12024, 29915, 29879, 679, 278, 4192, 650, 297, 278, 4799, 29892, 920, 1880, 881, 372, 748, 29973, 2, 29871, 13, 29966, 29989, 465, 22137, 29989, 29958, 13]\n"
     ]
    }
   ],
   "source": [
    "hf_encoded_prompt = hf_tokenizer.encode(prompt, add_special_tokens=False) # Same as doing self.convert_tokens_to_ids(self.tokenize(text))\n",
    "print(hf_encoded_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert hf_encoded_prompt == hf_encoded_tokenized_prompt, f\"\\n{hf_encoded_prompt}\\n!=\\n{hf_encoded_tokenized_prompt}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are an intelligent AI that controls a drone. Given a command or request from the user,\n",
      "call one of your functions to complete the request. If the request cannot be completed by your available functions, call the reject_request function.\n",
      "If the request is ambiguous or unclear, reject the request.</s>\n",
      "<|user|>\n",
      "Let's get the drone in the air, how high should it go?</s>\n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoded_encoded_prompt = tokenizer.decode(encoded_prompt)\n",
    "print(decoded_encoded_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are an intelligent AI that controls a drone. Given a command or request from the user,\n",
      "call one of your functions to complete the request. If the request cannot be completed by your available functions, call the reject_request function.\n",
      "If the request is ambiguous or unclear, reject the request.</s> \n",
      "<|user|>\n",
      "Let's get the drone in the air, how high should it go?</s> \n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# decoded_hf_encoded_prompt = hf_tokenizer.decode(hf_encoded_prompt, clean_up_tokenization_spaces=True)\n",
    "decoded_hf_encoded_prompt = hf_tokenizer.decode(hf_encoded_prompt)\n",
    "print(decoded_hf_encoded_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "<|system|>\n",
      "\n",
      "You are an intelligent AI that controls a drone. Given a command or request from the user,\n",
      "You are an intelligent AI that controls a drone. Given a command or request from the user,\n",
      "\n",
      "call one of your functions to complete the request. If the request cannot be completed by your available functions, call the reject_request function.\n",
      "call one of your functions to complete the request. If the request cannot be completed by your available functions, call the reject_request function.\n",
      "\n",
      "If the request is ambiguous or unclear, reject the request.</s>\n",
      "If the request is ambiguous or unclear, reject the request.</s> \n",
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "63 != 64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(line_b)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line_a) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(line_b), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(line_a)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(line_b)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m line_a \u001b[38;5;241m==\u001b[39m line_b, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mline_a\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m!=\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mline_b\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 63 != 64"
     ]
    }
   ],
   "source": [
    "for line_a, line_b in zip(decoded_encoded_prompt.split(\"\\n\"), decoded_hf_encoded_prompt.split(\"\\n\")):\n",
    "    print(line_a)\n",
    "    print(line_b)\n",
    "    print()\n",
    "\n",
    "    assert len(line_a) == len(line_b), f\"{len(line_a)} != {len(line_b)}\"\n",
    "    assert line_a == line_b, f\"\\n{line_a}\\n!=\\n{line_b}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert encoded_prompt == hf_encoded_prompt, f\"\\n{encoded_prompt}\\n!=\\n{hf_encoded_prompt}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert prompt == decoded_hf_encoded_prompt, f\"\\n{prompt}\\n!=\\n{decoded_hf_encoded_prompt}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
