{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tinygrad and CUDA device.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "from os import getenv\n",
    "\n",
    "from tqdm import tqdm\n",
    "import mlflow\n",
    "import openai\n",
    "import pandas as pd\n",
    "# from sentencepiece import SentencePieceProcessor\n",
    "from tinygrad import Device, Tensor, nn, Variable, GlobalCounters\n",
    "from tinygrad.helpers import Context, Timing, Profiling, DEBUG, JIT, getenv, colored\n",
    "from tinygrad.nn.state import get_state_dict, safe_load, torch_load, load_state_dict, get_parameters, safe_save\n",
    "from transformers import AutoTokenizer, AutoConfig, DataCollatorWithPadding, pipeline, Trainer, TrainingArguments\n",
    "\n",
    "from notebooks.Research.lib.transformers.src.transformers.models.llama.modeling_tinygrad_llama import TinygradLlamaForCausalLM\n",
    "# from notebooks.Research.tinygrad.train_llama import function_calling_template\n",
    "from timestep.config import settings\n",
    "\n",
    "device = Device.DEFAULT\n",
    "print(f'Using Tinygrad and {device} device.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bos_token=\"<|begin_of_text|>\"\n",
    "bos_token=\"<s>\"\n",
    "bos_token_id=1\n",
    "# eos_token=\"<|eot_id|>\"\n",
    "eos_token=\"</s>\"\n",
    "eos_token_id=2\n",
    "pad_token=eos_token\n",
    "pad_token_id=eos_token_id\n",
    "# unk_token=\"<|unk_id|>\"\n",
    "unk_token=\"<unk>\"\n",
    "unk_token_id=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_repo_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_name_or_path\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 2048,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 5632,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 22,\n",
       "  \"num_key_value_heads\": 4,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.44.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(hf_repo_id, bos_token_id=bos_token_id, eos_token_id=eos_token_id)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ram used:  2.20 GB, freqs_cis                                         : 100%|â–ˆ| \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded weights in 2971.83 ms, 2.20 GB loaded at 0.74 GB/s\n"
     ]
    }
   ],
   "source": [
    "# model_path = os.path.join(settings.app_dir, f\"models/{hf_repo_id}/model.safetensors\")\n",
    "model_path: Path = Path(settings.app_dir) / f\"models/{hf_repo_id}/model.safetensors\"\n",
    "# tokenizer_path = os.path.join(settings.app_dir, f\"models/{hf_repo_id}/tokenizer.model\")\n",
    "\n",
    "# model = TinygradLlamaForCausalLM.from_pretrained(hf_repo_id)\n",
    "model = TinygradLlamaForCausalLM(\n",
    "    config=config,\n",
    "    device=device,\n",
    "    model_path=model_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bos_token_id: 1\n",
      "eos_token_id: 2\n",
      "pad_token_id: None\n"
     ]
    }
   ],
   "source": [
    "print('bos_token_id:', model.config.bos_token_id)\n",
    "print('eos_token_id:', model.config.eos_token_id)\n",
    "print('pad_token_id:', model.config.pad_token_id)\n",
    "# print('unk_token_id:', model.config.unk_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bos_token: <s>\n",
      "eos_token: </s>\n",
      "pad_token: </s>\n",
      "unk_token: <unk>\n",
      "bos_token_id: 1\n",
      "eos_token_id: 2\n",
      "pad_token_id: 2\n",
      "unk_token_id: 0\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    hf_repo_id,\n",
    "    # bos_token=\"<|begin_of_text|>\",\n",
    "    bos_token=bos_token,\n",
    "    clean_up_tokenization_spaces=True,\n",
    "    eos_token=eos_token,\n",
    "    # pad_token='<|im_end|>',\n",
    "    # pad_token=pad_token,\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print('bos_token:', tokenizer.bos_token)\n",
    "print('eos_token:', tokenizer.eos_token)\n",
    "print('pad_token:', tokenizer.pad_token)\n",
    "print('unk_token:', tokenizer.unk_token)\n",
    "\n",
    "print('bos_token_id:', tokenizer.bos_token_id)\n",
    "print('eos_token_id:', tokenizer.eos_token_id)\n",
    "print('pad_token_id:', tokenizer.pad_token_id)\n",
    "print('unk_token_id:', tokenizer.unk_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokenizer.bos_token_id == model.config.bos_token_id, f'{tokenizer.bos_token_id} != {model.config.bos_token_id}'\n",
    "assert tokenizer.eos_token_id == model.config.eos_token_id, f'{tokenizer.eos_token_id} != {model.config.eos_token_id}'\n",
    "# assert tokenizer.pad_token_id == model.config.pad_token_id, f'{tokenizer.pad_token_id} != {model.config.pad_token_id}'\n",
    "# assert tokenizer.unk_token_id == model.config.unk_token_id, f'{tokenizer.unk_token_id} != {model.config.unk_token_id}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tiktoken \n",
    "\n",
    "# cl100k_base = tiktoken.get_encoding(\"cl100k_base\") \n",
    "\n",
    "# enc = tiktoken.Encoding( \n",
    "#     name=\"gpt-35-turbo\",  \n",
    "#     pat_str=cl100k_base._pat_str, \n",
    "#     mergeable_ranks=cl100k_base._mergeable_ranks, \n",
    "#     special_tokens={ \n",
    "#         **cl100k_base._special_tokens, \n",
    "#         \"<|im_start|>\": 100264, \n",
    "#         \"<|im_end|>\": 100265\n",
    "#     } \n",
    "# ) \n",
    "\n",
    "# tokens = enc.encode( \n",
    "#     \"<|im_start|>user\\nHello<|im_end|><|im_start|>assistant\",  \n",
    "#     allowed_special={\"<|im_start|>\", \"<|im_end|>\"} \n",
    "# ) \n",
    "\n",
    "# assert len(tokens) == 7 \n",
    "# assert tokens == [100264, 882, 198, 9906, 100265, 100264, 78191]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{%- for message in messages %}<|im_start|>{{- message.role }}\n",
      "{% if message.role == 'system' %}{{ message.content }}{% if tool_calls %}\n",
      "\n",
      "You have access to the following functions:\n",
      "{% for tool in tools %}\n",
      "functions.{{ tool.function.name }}:\n",
      "{{ tool.function.parameters | tojson }}\n",
      "{% endfor %}\n",
      "\n",
      "You can respond to user messages with either a single message or one or more function calls.\n",
      "\n",
      "To respond with a message begin the message with \"message:\", use the following format:\n",
      "\n",
      "message:\n",
      "<message>\n",
      "\n",
      "To respond with one or more function calls begin the message with 'functions.<function_name>:', use the following format:\n",
      "\n",
      "functions.<function_name>:\n",
      "{ \"arg1\": \"value1\", \"arg2\": \"value2\" }\n",
      "functions.<function_name>:\n",
      "{ \"arg1\": \"value1\", \"arg2\": \"value2\" }\n",
      "When responding with function calls, only output the function calls, do not include any additional text.{% endif %}</s>\n",
      "{% endif %}{% if message.role == 'user' %}{{ message.content }}</s>\n",
      "{% endif %}{% if message.role == 'assistant' %}{% if message.content and message.content | length > 0 %}{% if tool_calls %}message:\n",
      "{% endif %}{{ message.content }}</s>\n",
      "{% endif %}{% if 'tool_calls' in message %}{% for tool_call in message.tool_calls %}functions.{{ tool_call.function.name }}:\n",
      "{{ tool_call.function.arguments }}{% endfor %}</s>\n",
      "{% endif %}{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n",
      "{% endif %}\n"
     ]
    }
   ],
   "source": [
    "# Copied and modified from https://github.com/abetlen/llama-cpp-python/blob/658b244c5aa924fc6f4d04f92445dd8f724b6017/llama_cpp/llama_chat_format.py#L3345\n",
    "# See also https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/chat-markup-language\n",
    "function_calling_template = (\n",
    "  \"{%- for message in messages %}\"\n",
    "  \"<|im_start|>{{- message.role }}\\n\"\n",
    "  # System message\n",
    "  \"{% if message.role == 'system' %}\"\n",
    "  \"{{ message.content }}\"\n",
    "  \"{% if tool_calls %}\"\n",
    "  \"\\n\\nYou have access to the following functions:\\n\"\n",
    "  \"{% for tool in tools %}\"\n",
    "  \"\\nfunctions.{{ tool.function.name }}:\\n\"\n",
    "  \"{{ tool.function.parameters | tojson }}\"\n",
    "  \"\\n{% endfor %}\"\n",
    "  \"\\n\\nYou can respond to user messages with either a single message or one or more function calls.\"\n",
    "  \"\\n\\nTo respond with a message begin the message with \\\"message:\\\", use the following format:\"\n",
    "  \"\\n\\nmessage:\"\n",
    "  \"\\n<message>\"\n",
    "  \"\\n\\nTo respond with one or more function calls begin the message with 'functions.<function_name>:', use the following format:\"\n",
    "  \"\\n\\nfunctions.<function_name>:\"\n",
    "  '\\n{ \"arg1\": \"value1\", \"arg2\": \"value2\" }'\n",
    "  \"\\nfunctions.<function_name>:\"\n",
    "  '\\n{ \"arg1\": \"value1\", \"arg2\": \"value2\" }'\n",
    "  \"\\nWhen responding with function calls, only output the function calls, do not include any additional text.\"\n",
    "  \"{% endif %}\"\n",
    "  \"<|im_end|>\\n\"\n",
    "  \"{% endif %}\"\n",
    "  # User message\n",
    "  \"{% if message.role == 'user' %}\"\n",
    "  \"{{ message.content }}\"\n",
    "  \"<|im_end|>\\n\"\n",
    "  \"{% endif %}\"\n",
    "  # Assistant message\n",
    "  \"{% if message.role == 'assistant' %}\"\n",
    "  ## Reglar message\n",
    "  \"{% if message.content and message.content | length > 0 %}\"\n",
    "  \"{% if tool_calls %}\"\n",
    "  \"message:\\n\"\n",
    "  \"{% endif %}\"\n",
    "  \"{{ message.content }}\"\n",
    "  \"<|im_end|>\\n\"\n",
    "  \"{% endif %}\"\n",
    "  ## Function calls\n",
    "  \"{% if 'tool_calls' in message %}\"\n",
    "  \"{% for tool_call in message.tool_calls %}\"\n",
    "  \"functions.{{ tool_call.function.name }}:\\n\"\n",
    "  \"{{ tool_call.function.arguments }}\"\n",
    "  \"{% endfor %}\"\n",
    "  \"<|im_end|>\\n\"\n",
    "  \"{% endif %}\"\n",
    "  \"{% endif %}\"\n",
    "  \"{% endfor %}\"\n",
    "  \"{% if add_generation_prompt %}<|im_start|>assistant\\n{% endif %}\"\n",
    ")\n",
    "\n",
    "# function_calling_template = function_calling_template.replace(\"<|im_start|>\", tokenizer.bos_token)\n",
    "function_calling_template = function_calling_template.replace(\"<|im_end|>\", tokenizer.eos_token)\n",
    "\n",
    "print(function_calling_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- set date_string = \"26 Jul 2024\" %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message + builtin tools #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if builtin_tools is defined or tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{%- if builtin_tools is defined %}\n",
      "    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"</s>\n",
      "\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"</s>\n",
      "\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim }}\n",
      "        {%- if not loop.last or add_generation_prompt %}\n",
      "            {{- '</s>\n",
      "' }}\n",
      "        {%- endif %}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n",
      "            {%- for arg_name, arg_val in tool_call.arguments | items %}\n",
      "                {{- arg_name + '=\"' + arg_val + '\"' }}\n",
      "                {%- if not loop.last %}\n",
      "                    {{- \", \" }}\n",
      "                {%- endif %}\n",
      "                {%- endfor %}\n",
      "            {{- \")\" }}\n",
      "        {%- else  %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "            {{- '\"parameters\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- \"}\" }}\n",
      "        {%- endif %}\n",
      "        {%- if builtin_tools is defined %}\n",
      "            {#- This means we're in ipython mode #}\n",
      "            {{- \"<|eom_id|>\" }}\n",
      "        {%- else %}\n",
      "            {{- \"</s>\n",
      "\" }}\n",
      "        {%- endif %}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"</s>\n",
      "\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
    "\n",
    "# llama_3_1_8b_instruct_chat_template = \"{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- set date_string = \\\"26 Jul 2024\\\" %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \\\"\\\" %}\\n{%- endif %}\\n\\n{#- System message + builtin tools #}\\n{{- \\\"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\\\" }}\\n{%- if builtin_tools is defined or tools is not none %}\\n    {{- \\\"Environment: ipython\\\\n\\\" }}\\n{%- endif %}\\n{%- if builtin_tools is defined %}\\n    {{- \\\"Tools: \\\" + builtin_tools | reject('equalto', 'code_interpreter') | join(\\\", \\\") + \\\"\\\\n\\\\n\\\"}}\\n{%- endif %}\\n{{- \\\"Cutting Knowledge Date: December 2023\\\\n\\\" }}\\n{{- \\\"Today Date: \\\" + date_string + \\\"\\\\n\\\\n\\\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \\\"You have access to the following functions. To call a function, please respond with JSON for a function call.\\\" }}\\n    {{- 'Respond in the format {\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}.' }}\\n    {{- \\\"Do not use variables.\\\\n\\\\n\\\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \\\"\\\\n\\\\n\\\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \\\"<|eot_id|>\\\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0]['content']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\\\"Cannot put tools in the first user message when there's no first user message!\\\") }}\\n{%- endif %}\\n    {{- '<|start_header_id|>user<|end_header_id|>\\\\n\\\\n' -}}\\n    {{- \\\"Given the following functions, please respond with a JSON for a function call \\\" }}\\n    {{- \\\"with its proper arguments that best answers the given prompt.\\\\n\\\\n\\\" }}\\n    {{- 'Respond in the format {\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}.' }}\\n    {{- \\\"Do not use variables.\\\\n\\\\n\\\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \\\"\\\\n\\\\n\\\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \\\"<|eot_id|>\\\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\\\n\\\\n'+ message['content'] | trim + '<|eot_id|>' }}\\n    {%- elif 'tool_calls' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\\\"This model only supports single tool-calls at once!\\\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' -}}\\n            {{- \\\"<|python_tag|>\\\" + tool_call.name + \\\".call(\\\" }}\\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\\n                {{- arg_name + '=\\\"' + arg_val + '\\\"' }}\\n                {%- if not loop.last %}\\n                    {{- \\\", \\\" }}\\n                {%- endif %}\\n                {%- endfor %}\\n            {{- \\\")\\\" }}\\n        {%- else  %}\\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' -}}\\n            {{- '{\\\"name\\\": \\\"' + tool_call.name + '\\\", ' }}\\n            {{- '\\\"parameters\\\": ' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\\"}\\\" }}\\n        {%- endif %}\\n        {%- if builtin_tools is defined %}\\n            {#- This means we're in ipython mode #}\\n            {{- \\\"<|eom_id|>\\\" }}\\n        {%- else %}\\n            {{- \\\"<|eot_id|>\\\" }}\\n        {%- endif %}\\n    {%- elif message.role == \\\"tool\\\" or message.role == \\\"ipython\\\" %}\\n        {{- \\\"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\\\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \\\"<|eot_id|>\\\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' }}\\n{%- endif %}\\n\"\n",
    "\n",
    "# llama_3_1_8b_instruct_chat_template = \"{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- set date_string = \\\"26 Jul 2024\\\" %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \\\"\\\" %}\\n{%- endif %}\\n\\n{#- System message + builtin tools #}\\n{{- \\\"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\\\" }}\\n{%- if builtin_tools is defined or tools is not none %}\\n    {{- \\\"Environment: ipython\\\\n\\\" }}\\n{%- endif %}\\n{%- if builtin_tools is defined %}\\n    {{- \\\"Tools: \\\" + builtin_tools | reject('equalto', 'code_interpreter') | join(\\\", \\\") + \\\"\\\\n\\\\n\\\"}}\\n{%- endif %}\\n{{- \\\"Cutting Knowledge Date: December 2023\\\\n\\\" }}\\n{{- \\\"Today Date: \\\" + date_string + \\\"\\\\n\\\\n\\\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \\\"You have access to the following functions. To call a function, please respond with JSON for a function call.\\\" }}\\n    {{- 'Respond in the format {\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}.' }}\\n    {{- \\\"Do not use variables.\\\\n\\\\n\\\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \\\"\\\\n\\\\n\\\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \\\"<|eot_id|>\\\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0]['content']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\\\"Cannot put tools in the first user message when there's no first user message!\\\") }}\\n{%- endif %}\\n    {{- '<|start_header_id|>user<|end_header_id|>\\\\n\\\\n' -}}\\n    {{- \\\"Given the following functions, please respond with a JSON for a function call \\\" }}\\n    {{- \\\"with its proper arguments that best answers the given prompt.\\\\n\\\\n\\\" }}\\n    {{- 'Respond in the format {\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}.' }}\\n    {{- \\\"Do not use variables.\\\\n\\\\n\\\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \\\"\\\\n\\\\n\\\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \\\"<|eot_id|>\\\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\\\n\\\\n'+ message['content'] | trim }}\\n        {%- if not loop.last or add_generation_prompt %}\\n            {{- '<|eot_id|>' }}\\n        {%- endif %}\\n    {%- elif 'tool_calls' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\\\"This model only supports single tool-calls at once!\\\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' -}}\\n            {{- \\\"<|python_tag|>\\\" + tool_call.name + \\\".call(\\\" }}\\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\\n                {{- arg_name + '=\\\"' + arg_val + '\\\"' }}\\n                {%- if not loop.last %}\\n                    {{- \\\", \\\" }}\\n                {%- endif %}\\n                {%- endfor %}\\n            {{- \\\")\\\" }}\\n        {%- else  %}\\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' -}}\\n            {{- '{\\\"name\\\": \\\"' + tool_call.name + '\\\", ' }}\\n            {{- '\\\"parameters\\\": ' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\\"}\\\" }}\\n        {%- endif %}\\n        {%- if builtin_tools is defined %}\\n            {#- This means we're in ipython mode #}\\n            {{- \\\"<|eom_id|>\\\" }}\\n        {%- else %}\\n            {{- \\\"<|eot_id|>\\\" }}\\n        {%- endif %}\\n    {%- elif message.role == \\\"tool\\\" or message.role == \\\"ipython\\\" %}\\n        {{- \\\"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\\\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \\\"<|eot_id|>\\\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' }}\\n{%- endif %}\\n\"\n",
    "\n",
    "llama_3_1_8b_instruct_chat_template = \"{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- set date_string = \\\"26 Jul 2024\\\" %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \\\"\\\" %}\\n{%- endif %}\\n\\n{#- System message + builtin tools #}\\n{{- \\\"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\\\" }}\\n{%- if builtin_tools is defined or tools is not none %}\\n    {{- \\\"Environment: ipython\\\\n\\\" }}\\n{%- endif %}\\n{%- if builtin_tools is defined %}\\n    {{- \\\"Tools: \\\" + builtin_tools | reject('equalto', 'code_interpreter') | join(\\\", \\\") + \\\"\\\\n\\\\n\\\"}}\\n{%- endif %}\\n{{- \\\"Cutting Knowledge Date: December 2023\\\\n\\\" }}\\n{{- \\\"Today Date: \\\" + date_string + \\\"\\\\n\\\\n\\\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \\\"You have access to the following functions. To call a function, please respond with JSON for a function call.\\\" }}\\n    {{- 'Respond in the format {\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}.' }}\\n    {{- \\\"Do not use variables.\\\\n\\\\n\\\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \\\"\\\\n\\\\n\\\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \\\"<|eot_id|>\\\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0]['content']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\\\"Cannot put tools in the first user message when there's no first user message!\\\") }}\\n{%- endif %}\\n    {{- '<|start_header_id|>user<|end_header_id|>\\\\n\\\\n' -}}\\n    {{- \\\"Given the following functions, please respond with a JSON for a function call \\\" }}\\n    {{- \\\"with its proper arguments that best answers the given prompt.\\\\n\\\\n\\\" }}\\n    {{- 'Respond in the format {\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}.' }}\\n    {{- \\\"Do not use variables.\\\\n\\\\n\\\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \\\"\\\\n\\\\n\\\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \\\"<|eot_id|>\\\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\\\n\\\\n'+ message['content'] | trim }}\\n        {%- if not loop.last or add_generation_prompt %}\\n            {{- '<|eot_id|>' }}\\n        {%- endif %}\\n    {%- elif 'tool_calls' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\\\"This model only supports single tool-calls at once!\\\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' -}}\\n            {{- \\\"<|python_tag|>\\\" + tool_call.name + \\\".call(\\\" }}\\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\\n                {{- arg_name + '=\\\"' + arg_val + '\\\"' }}\\n                {%- if not loop.last %}\\n                    {{- \\\", \\\" }}\\n                {%- endif %}\\n                {%- endfor %}\\n            {{- \\\")\\\" }}\\n        {%- else  %}\\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' -}}\\n            {{- '{\\\"name\\\": \\\"' + tool_call.name + '\\\", ' }}\\n            {{- '\\\"parameters\\\": ' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\\"}\\\" }}\\n        {%- endif %}\\n        {%- if builtin_tools is defined %}\\n            {#- This means we're in ipython mode #}\\n            {{- \\\"<|eom_id|>\\\" }}\\n        {%- else %}\\n            {{- \\\"<|eot_id|>\\\" }}\\n        {%- endif %}\\n    {%- elif message.role == \\\"tool\\\" or message.role == \\\"ipython\\\" %}\\n        {{- \\\"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\\\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \\\"<|eot_id|>\\\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' }}\\n{%- endif %}\\n\"\n",
    "\n",
    "llama_3_1_8b_instruct_chat_template = llama_3_1_8b_instruct_chat_template.replace(\"<|eot_id|>\", f\"{tokenizer.eos_token}\\n\")\n",
    "\n",
    "print(llama_3_1_8b_instruct_chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# template = \"\"\"{%- for message in messages %}\n",
    "# {%- if message['role'] == 'user' %}\n",
    "# {{- '<|user|>\n",
    "# ' + message['content'] + eos_token + '\\n' }}\n",
    "# {%- elif message['role'] == 'system' %}\n",
    "# {{- '<|system|>\n",
    "# ' + message['content'] + \n",
    "# eos_token + '\\n' }}\n",
    "# {%- elif message['role'] == 'assistant' %}\n",
    "# {{- '<|assistant|>\n",
    "# '  + message['content'] + \n",
    "# eos_token + '\\n' }}\n",
    "# {%- endif %}\n",
    "# {%- if loop.last and add_generation_prompt %}\n",
    "# {{- '<|assistant|>' }}\n",
    "# {%- endif %}\n",
    "# {%- endfor %}\"\"\"\n",
    "\n",
    "template = \"\"\"{%- for message in messages %}\n",
    "{%- if message['role'] == 'assistant' %}\n",
    "{{- '<|assistant|>' + '\\n' }}\n",
    "{%- if message['content'] %}\n",
    "{{- message['content'] + eos_token + '\\n' }}\n",
    "{%- elif message['tool_calls'] %}\n",
    "{{- 'tool_calls:' + '\\n' }}\n",
    "{%- for tool_call in message['tool_calls'] %}\n",
    "{{- tool_call['function']['name'] + ': ' + tool_call['function']['arguments'] }}\n",
    "{%- endfor %}\n",
    "{{- eos_token + '\\n' }}\n",
    "{%- endif %}\n",
    "{%- elif message['role'] == 'system' %}\n",
    "{{- '<|system|>' + '\\n' }}\n",
    "{{- message['content'] }}\n",
    "{%- if tools %}\n",
    "{{- '\\n\\nYou are aware of the following tools:\\n' }}\n",
    "{%- for tool in tools %}\n",
    "{{- tool.function.name }}: {{ tool.function.parameters | tojson }}\n",
    "{%- endfor %}\n",
    "{{- '\\nYou can suggest tool calls by responding in the following format:\\n' }}\n",
    "{{- 'tool_calls:' + '\\n' }}\n",
    "{{- 'tool_name: {\\\"arg1\\\": \\\"value1\\\", \\\"arg2\\\": \\\"value2\\\"}' }}\n",
    "{{- eos_token + '\\n' }}\n",
    "{%- endif %}\n",
    "{%- elif message['role'] == 'user' %}\n",
    "{{- '<|user|>' + '\\n' }}\n",
    "{{- message['content'] + eos_token + '\\n' }}\n",
    "{%- endif %}\n",
    "{%- if loop.last and add_generation_prompt %}\n",
    "{{- '<|assistant|>' + '\\n' }}\n",
    "{%- endif %}\n",
    "{%- endfor %}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open(\"template.jinja\", \"w\").write(template)\n",
    "# open(\"template.jinja\", \"w\").write(tokenizer.chat_template)\n",
    "open(\"template.jinja\", \"w\").write(function_calling_template)\n",
    "# open(\"template.jinja\", \"w\").write(llama_3_1_8b_instruct_chat_template)\n",
    "\n",
    "tokenizer.chat_template = open(\"template.jinja\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are an intelligent AI that controls a drone. Given a command or request from the user,\\ncall one of your functions to complete the request. If the request cannot be completed by your available functions, call the reject_request function.\\nIf the request is ambiguous or unclear, reject the request.'},\n",
       " {'role': 'user',\n",
       "  'content': \"Let's get the drone in the air, how high should it go?\"},\n",
       " {'role': 'assistant',\n",
       "  'tool_calls': [{'id': 'call_id',\n",
       "    'type': 'function',\n",
       "    'function': {'name': 'takeoff_drone', 'arguments': '{\"altitude\": 100}'}}]}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations = []\n",
    "\n",
    "with open(\"../../data/drone_training.jsonl\") as f:\n",
    "    for line in f:\n",
    "        conversations.append(json.loads(line))\n",
    "\n",
    "conversation = conversations[0]\n",
    "\n",
    "# print('messages: ', conversation[\"messages\"])\n",
    "\n",
    "# print('parallel_tool_calls: ', conversation[\"parallel_tool_calls\"])\n",
    "\n",
    "# print('tools: ', conversation[\"tools\"])\n",
    "\n",
    "# conversation = conversation[\"messages\"][0:2]\n",
    "# conversation = conversation[\"messages\"]\n",
    "conversation[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are an intelligent AI that controls a drone. Given a command or request from the user,\n",
      "call one of your functions to complete the request. If the request cannot be completed by your available functions, call the reject_request function.\n",
      "If the request is ambiguous or unclear, reject the request.\n",
      "You have access to the following functions:\n",
      "functions.takeoff_drone:\n",
      "{\"type\": \"object\", \"properties\": {\"altitude\": {\"type\": \"integer\"}}, \"required\": [\"altitude\"]}\n",
      "functions.land_drone:\n",
      "{\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"enum\": [\"current\", \"home_base\", \"custom\"]}, \"coordinates\": {\"type\": \"object\"}}, \"required\": [\"location\"]}\n",
      "functions.control_drone_movement:\n",
      "{\"type\": \"object\", \"properties\": {\"direction\": {\"type\": \"string\", \"enum\": [\"forward\", \"backward\", \"left\", \"right\", \"up\", \"down\"]}, \"distance\": {\"type\": \"integer\"}}, \"required\": [\"direction\", \"distance\"]}\n",
      "functions.set_drone_speed:\n",
      "{\"type\": \"object\", \"properties\": {\"speed\": {\"type\": \"integer\", \"minimum\": 0}}, \"required\": [\"speed\"]}\n",
      "functions.control_camera:\n",
      "{\"type\": \"object\", \"properties\": {\"mode\": {\"type\": \"string\", \"enum\": [\"photo\", \"video\", \"panorama\"]}, \"duration\": {\"type\": \"integer\"}}, \"required\": [\"mode\"]}\n",
      "functions.control_gimbal:\n",
      "{\"type\": \"object\", \"properties\": {\"tilt\": {\"type\": \"integer\"}, \"pan\": {\"type\": \"integer\"}}, \"required\": [\"tilt\", \"pan\"]}\n",
      "functions.set_drone_lighting:\n",
      "{\"type\": \"object\", \"properties\": {\"mode\": {\"type\": \"string\", \"enum\": [\"on\", \"off\", \"blink\", \"sos\"]}}, \"required\": [\"mode\"]}\n",
      "functions.return_to_home:\n",
      "{\"type\": \"object\", \"properties\": {}}\n",
      "functions.set_battery_saver_mode:\n",
      "{\"type\": \"object\", \"properties\": {\"status\": {\"type\": \"string\", \"enum\": [\"on\", \"off\"]}}, \"required\": [\"status\"]}\n",
      "functions.set_obstacle_avoidance:\n",
      "{\"type\": \"object\", \"properties\": {\"mode\": {\"type\": \"string\", \"enum\": [\"on\", \"off\"]}}, \"required\": [\"mode\"]}\n",
      "functions.set_follow_me_mode:\n",
      "{\"type\": \"object\", \"properties\": {\"status\": {\"type\": \"string\", \"enum\": [\"on\", \"off\"]}}, \"required\": [\"status\"]}\n",
      "functions.calibrate_sensors:\n",
      "{\"type\": \"object\", \"properties\": {}}\n",
      "functions.set_autopilot:\n",
      "{\"type\": \"object\", \"properties\": {\"status\": {\"type\": \"string\", \"enum\": [\"on\", \"off\"]}}, \"required\": [\"status\"]}\n",
      "functions.configure_led_display:\n",
      "{\"type\": \"object\", \"properties\": {\"pattern\": {\"type\": \"string\", \"enum\": [\"solid\", \"blink\", \"pulse\", \"rainbow\"]}, \"color\": {\"type\": \"string\", \"enum\": [\"red\", \"blue\", \"green\", \"yellow\", \"white\"]}}, \"required\": [\"pattern\"]}\n",
      "functions.set_home_location:\n",
      "{\"type\": \"object\", \"properties\": {\"coordinates\": {\"type\": \"object\"}}, \"required\": [\"coordinates\"]}\n",
      "functions.reject_request:\n",
      "{\"type\": \"object\", \"properties\": {}}\n",
      "\n",
      "You can respond to user messages with either a single message or one or more function calls.\n",
      "\n",
      "To respond with a message begin the message with \"message:\", use the following format:\n",
      "\n",
      "message:\n",
      "<message>\n",
      "\n",
      "To respond with one or more function calls begin the message with 'functions.<function_name>:', use the following format:\n",
      "\n",
      "functions.<function_name>:\n",
      "{ \"arg1\": \"value1\", \"arg2\": \"value2\" }\n",
      "functions.<function_name>:\n",
      "{ \"arg1\": \"value1\", \"arg2\": \"value2\" }\n",
      "When responding with function calls, only output the function calls, do not include any additional text.</s>\n",
      "<|im_start|>user\n",
      "Let's get the drone in the air, how high should it go?</s>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def generate_prompt(add_generation_prompt, date_string=f\"{datetime.now():%d %b %Y}\", messages=[], tool_calls=True, tools=[]):\n",
    "    return tokenizer.apply_chat_template(\n",
    "        add_generation_prompt=add_generation_prompt,\n",
    "        # add_special_tokens=False,\n",
    "        # conversation=conversation,\n",
    "        # conversation=conversation[\"messages\"][0:2], # Skip tool messages for now\n",
    "        # conversation=conversation[\"messages\"],\n",
    "        conversation=messages,\n",
    "        date_string=date_string,\n",
    "        # return_dict=True,\n",
    "        return_tensors=False,\n",
    "        tokenize=False,\n",
    "        tool_calls=tool_calls,\n",
    "        # tools=conversation[\"tools\"],\n",
    "        tools=tools,\n",
    "    )\n",
    "\n",
    "prompt = generate_prompt(\n",
    "    add_generation_prompt=True,\n",
    "    messages=conversation[\"messages\"][0:2],\n",
    "    # messages=conversation[\"messages\"],\n",
    "    tool_calls=True,\n",
    "    tools=conversation[\"tools\"],\n",
    ")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When responding with function calls, only output the function calls, do not include any additional text.</s> \n",
      "!=\n",
      "When responding with function calls, only output the function calls, do not include any additional text.</s>\n",
      "Let's get the drone in the air, how high should it go?</s> \n",
      "!=\n",
      "Let's get the drone in the air, how high should it go?</s>\n"
     ]
    }
   ],
   "source": [
    "hf_tokenized_prompt = tokenizer.tokenize(prompt, add_special_tokens=False)\n",
    "hf_encoded_tokenized_prompt = tokenizer.convert_tokens_to_ids(hf_tokenized_prompt)\n",
    "hf_encoded_prompt = tokenizer.encode(prompt, add_special_tokens=False) # Same as doing self.convert_tokens_to_ids(self.tokenize(text))\n",
    "\n",
    "assert hf_encoded_prompt == hf_encoded_tokenized_prompt, f\"\\n{hf_encoded_prompt}\\n!=\\n{hf_encoded_tokenized_prompt}\"\n",
    "\n",
    "# decoded_hf_encoded_prompt = tokenizer.decode(hf_encoded_prompt, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "# decoded_hf_encoded_prompt = tokenizer.decode(hf_encoded_prompt, skip_special_tokens=True)\n",
    "decoded_hf_encoded_prompt = tokenizer.decode(hf_encoded_prompt)\n",
    "\n",
    "# assert decoded_hf_encoded_prompt == prompt, f\"\\n{decoded_hf_encoded_prompt}\\n!=\\n{prompt}\"\n",
    "\n",
    "for line_a, line_b in zip(prompt.split(\"\\n\"), decoded_hf_encoded_prompt.split(\"\\n\")):\n",
    "    try:\n",
    "        assert line_b == line_a\n",
    "\n",
    "    except AssertionError:\n",
    "        print(f\"{line_b}\\n!=\\n{line_a}\")\n",
    "        assert line_b.strip() == line_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(prompt: str):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"np\", add_special_tokens=False)\n",
    "    inputs = {k: Tensor(v, device=device) for k, v in inputs.items()}\n",
    "\n",
    "    outputs = model.generate(**inputs, do_sample=True, max_new_tokens=256, temperature=0.0)\n",
    "\n",
    "    # decoded_output = tokenizer.decode(outputs[0][inputs['input_ids'].size(1):], skip_special_tokens=True)\n",
    "    decoded_output = tokenizer.decode(outputs[0][inputs['input_ids'].size(1):])\n",
    "\n",
    "    return decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected_output:\n",
      "functions.takeoff_drone:\n",
      "{\"altitude\": 100}</s>\n",
      "\n",
      "decoded_output:\n",
      "Sure! To set the drone's altitude, you can use the \"takeoff_drone\" function. Here's an example:\n",
      "\n",
      "```\n",
      "functions.takeoff_drone(altitude=500)\n",
      "```\n",
      "\n",
      "This will take the drone up to an altitude of 500 meters (1640 feet). You can also pass in additional parameters to customize the takeoff, such as the drone's current location, speed, and direction.</s>\n"
     ]
    }
   ],
   "source": [
    "expected_output = generate_prompt(\n",
    "    add_generation_prompt=False,\n",
    "    messages=conversation[\"messages\"][2:],\n",
    "    tool_calls=True,\n",
    "    tools=conversation[\"tools\"],\n",
    ")\n",
    "\n",
    "expected_output = \"\\n\".join(expected_output.split(\"\\n\")[1:]) # Skip the first line\n",
    "\n",
    "print('expected_output:')\n",
    "print(expected_output)\n",
    "\n",
    "decoded_output = predict(prompt)\n",
    "\n",
    "print('decoded_output:')\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity_metric: 0.14249363867684478\n"
     ]
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def compute_similarity_metric(prediction: str, target: str):\n",
    "    s_1 = SequenceMatcher(None, prediction, target)\n",
    "    s_2 = SequenceMatcher(None, target, prediction)\n",
    "\n",
    "    return (s_1.ratio() + s_2.ratio()) / 2\n",
    "\n",
    "similarity_metric = compute_similarity_metric(decoded_output, expected_output)\n",
    "print('similarity_metric:', similarity_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103, 103)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_prompts = [\n",
    "    generate_prompt(\n",
    "        # add_generation_prompt=False,\n",
    "        add_generation_prompt=True,\n",
    "        messages=conversation[\"messages\"][0:2], # TODO: do this until we reach the first assistant message where where \"weight\" is undefined or 1\n",
    "        tools=conversation[\"tools\"],\n",
    "    )\n",
    "    for conversation in conversations\n",
    "]\n",
    "\n",
    "ground_truth_responses = [\n",
    "    generate_prompt(\n",
    "        add_generation_prompt=False,\n",
    "        messages=conversation[\"messages\"][2:], # TODO: Start at the fisrt assistant message where \"weight\" is undefined or 1\n",
    "        tools=conversation[\"tools\"],\n",
    "    )\n",
    "    for conversation in conversations\n",
    "]\n",
    "\n",
    "# expected_output = \"\\n\".join(expected_output.split(\"\\n\")[1:]) # Skip the first line\n",
    "\n",
    "for i in range(len(ground_truth_responses)):\n",
    "    ground_truth_responses[i] = \"\\n\".join(ground_truth_responses[i].split(\"\\n\")[1:]) # Skip the first line\n",
    "\n",
    "len(input_prompts), len(ground_truth_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a87508f8e244708218506b2bbcc949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import trange\n",
    "\n",
    "stop_after = len(input_prompts)\n",
    "\n",
    "eval_df = pd.DataFrame({\n",
    "    \"inputs\": input_prompts[:stop_after],\n",
    "    \"ground_truth\": ground_truth_responses[:stop_after],\n",
    "})\n",
    "predictions = []\n",
    "\n",
    "# for i, row in tqdm(eval_df.iterrows()):\n",
    "for i in trange(len(eval_df)):\n",
    "    # print('i: ', i)\n",
    "    row = eval_df.iloc[i]\n",
    "\n",
    "    if i >= stop_after:\n",
    "        break\n",
    "\n",
    "    input_prompt = row[\"inputs\"]\n",
    "    ground_truth_response = row[\"ground_truth\"]\n",
    "\n",
    "    # print('input_prompt: ')\n",
    "    # print(input_prompt)\n",
    "    # print('ground_truth_response: ')\n",
    "    # print(ground_truth_response)\n",
    "\n",
    "    prediction = predict(input_prompt)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "    # print('prediction: ')\n",
    "    # print(prediction)\n",
    "\n",
    "eval_df[\"predictions\"] = predictions\n",
    "\n",
    "eval_df[\"similarity_metric\"] = eval_df.apply(lambda row: compute_similarity_metric(row[\"predictions\"], row[\"ground_truth\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:\n",
      "<|im_start|>system\n",
      "You are an intelligent AI that controls a drone. Given a command or request from the user,\n",
      "call one of your functions to complete the request. If the request cannot be completed by your available functions, call the reject_request function.\n",
      "If the request is ambiguous or unclear, reject the request.\n",
      "You have access to the following functions:\n",
      "functions.takeoff_drone:\n",
      "{\"type\": \"object\", \"properties\": {\"altitude\": {\"type\": \"integer\"}}, \"required\": [\"altitude\"]}\n",
      "functions.land_drone:\n",
      "{\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"enum\": [\"current\", \"home_base\", \"custom\"]}, \"coordinates\": {\"type\": \"object\"}}, \"required\": [\"location\"]}\n",
      "functions.control_drone_movement:\n",
      "{\"type\": \"object\", \"properties\": {\"direction\": {\"type\": \"string\", \"enum\": [\"forward\", \"backward\", \"left\", \"right\", \"up\", \"down\"]}, \"distance\": {\"type\": \"integer\"}}, \"required\": [\"direction\", \"distance\"]}\n",
      "functions.set_drone_speed:\n",
      "{\"type\": \"object\", \"properties\": {\"speed\": {\"type\": \"integer\", \"minimum\": 0}}, \"required\": [\"speed\"]}\n",
      "functions.control_camera:\n",
      "{\"type\": \"object\", \"properties\": {\"mode\": {\"type\": \"string\", \"enum\": [\"photo\", \"video\", \"panorama\"]}, \"duration\": {\"type\": \"integer\"}}, \"required\": [\"mode\"]}\n",
      "functions.control_gimbal:\n",
      "{\"type\": \"object\", \"properties\": {\"tilt\": {\"type\": \"integer\"}, \"pan\": {\"type\": \"integer\"}}, \"required\": [\"tilt\", \"pan\"]}\n",
      "functions.set_drone_lighting:\n",
      "{\"type\": \"object\", \"properties\": {\"mode\": {\"type\": \"string\", \"enum\": [\"on\", \"off\", \"blink\", \"sos\"]}}, \"required\": [\"mode\"]}\n",
      "functions.return_to_home:\n",
      "{\"type\": \"object\", \"properties\": {}}\n",
      "functions.set_battery_saver_mode:\n",
      "{\"type\": \"object\", \"properties\": {\"status\": {\"type\": \"string\", \"enum\": [\"on\", \"off\"]}}, \"required\": [\"status\"]}\n",
      "functions.set_obstacle_avoidance:\n",
      "{\"type\": \"object\", \"properties\": {\"mode\": {\"type\": \"string\", \"enum\": [\"on\", \"off\"]}}, \"required\": [\"mode\"]}\n",
      "functions.set_follow_me_mode:\n",
      "{\"type\": \"object\", \"properties\": {\"status\": {\"type\": \"string\", \"enum\": [\"on\", \"off\"]}}, \"required\": [\"status\"]}\n",
      "functions.calibrate_sensors:\n",
      "{\"type\": \"object\", \"properties\": {}}\n",
      "functions.set_autopilot:\n",
      "{\"type\": \"object\", \"properties\": {\"status\": {\"type\": \"string\", \"enum\": [\"on\", \"off\"]}}, \"required\": [\"status\"]}\n",
      "functions.configure_led_display:\n",
      "{\"type\": \"object\", \"properties\": {\"pattern\": {\"type\": \"string\", \"enum\": [\"solid\", \"blink\", \"pulse\", \"rainbow\"]}, \"color\": {\"type\": \"string\", \"enum\": [\"red\", \"blue\", \"green\", \"yellow\", \"white\"]}}, \"required\": [\"pattern\"]}\n",
      "functions.set_home_location:\n",
      "{\"type\": \"object\", \"properties\": {\"coordinates\": {\"type\": \"object\"}}, \"required\": [\"coordinates\"]}\n",
      "functions.reject_request:\n",
      "{\"type\": \"object\", \"properties\": {}}\n",
      "\n",
      "You can respond to user messages with either a single message or one or more function calls.\n",
      "\n",
      "To respond with a message begin the message with \"message:\", use the following format:\n",
      "\n",
      "message:\n",
      "<message>\n",
      "\n",
      "To respond with one or more function calls begin the message with 'functions.<function_name>:', use the following format:\n",
      "\n",
      "functions.<function_name>:\n",
      "{ \"arg1\": \"value1\", \"arg2\": \"value2\" }\n",
      "functions.<function_name>:\n",
      "{ \"arg1\": \"value1\", \"arg2\": \"value2\" }\n",
      "When responding with function calls, only output the function calls, do not include any additional text.</s>\n",
      "<|im_start|>user\n",
      "Let's get the drone in the air, how high should it go?</s>\n",
      "<|im_start|>assistant\n",
      "\n",
      "target:\n",
      "functions.takeoff_drone:\n",
      "{\"altitude\": 100}</s>\n",
      "\n",
      "prediction:\n",
      "Sure! To set the drone's altitude, you can use the \"takeoff_drone\" function. Here's an example:\n",
      "\n",
      "```\n",
      "functions.takeoff_drone(altitude=500)\n",
      "```\n",
      "\n",
      "This will take the drone up to an altitude of 500 meters (1640 feet). You can also pass in additional parameters to customize the takeoff, such as the drone's current location, speed, and direction.</s>\n",
      "similarity_metric:\n",
      "0.14249363867684478\n"
     ]
    }
   ],
   "source": [
    "print('prompt:')\n",
    "print(eval_df[\"inputs\"].values[0])\n",
    "\n",
    "print('target:')\n",
    "print(eval_df[\"ground_truth\"].values[0])\n",
    "\n",
    "print('prediction:')\n",
    "print(eval_df[\"predictions\"].values[0])\n",
    "\n",
    "print('similarity_metric:')\n",
    "print(eval_df[\"similarity_metric\"].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"eval_df.jsonl\", \"w\") as f:\n",
    "    eval_df.to_json(f, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>predictions</th>\n",
       "      <th>similarity_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are an intelligent AI ...</td>\n",
       "      <td>functions.takeoff_drone:\\n{\"altitude\": 100}&lt;/s&gt;\\n</td>\n",
       "      <td>Sure! To set the drone's altitude, you can use...</td>\n",
       "      <td>0.142494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are an intelligent AI ...</td>\n",
       "      <td>functions.takeoff_drone:\\n{\"altitude\": 100}&lt;/s&gt;\\n</td>\n",
       "      <td>Sure, to set the drone's height, you can use t...</td>\n",
       "      <td>0.076280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are an intelligent AI ...</td>\n",
       "      <td>functions.land_drone:\\n{\"location\": \"current\"}...</td>\n",
       "      <td>Sure, I can bring the drone down to where you ...</td>\n",
       "      <td>0.047862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are an intelligent AI ...</td>\n",
       "      <td>functions.land_drone:\\n{\"location\": \"current\"}...</td>\n",
       "      <td>Sure, here's an updated version of the AI syst...</td>\n",
       "      <td>0.051860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;|im_start|&gt;system\\nYou are an intelligent AI ...</td>\n",
       "      <td>functions.land_drone:\\n{\"location\": \"home_base...</td>\n",
       "      <td>To bring the drone back to base for landing, c...</td>\n",
       "      <td>0.066802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              inputs  \\\n",
       "0  <|im_start|>system\\nYou are an intelligent AI ...   \n",
       "1  <|im_start|>system\\nYou are an intelligent AI ...   \n",
       "2  <|im_start|>system\\nYou are an intelligent AI ...   \n",
       "3  <|im_start|>system\\nYou are an intelligent AI ...   \n",
       "4  <|im_start|>system\\nYou are an intelligent AI ...   \n",
       "\n",
       "                                        ground_truth  \\\n",
       "0  functions.takeoff_drone:\\n{\"altitude\": 100}</s>\\n   \n",
       "1  functions.takeoff_drone:\\n{\"altitude\": 100}</s>\\n   \n",
       "2  functions.land_drone:\\n{\"location\": \"current\"}...   \n",
       "3  functions.land_drone:\\n{\"location\": \"current\"}...   \n",
       "4  functions.land_drone:\\n{\"location\": \"home_base...   \n",
       "\n",
       "                                         predictions  similarity_metric  \n",
       "0  Sure! To set the drone's altitude, you can use...           0.142494  \n",
       "1  Sure, to set the drone's height, you can use t...           0.076280  \n",
       "2  Sure, I can bring the drone down to where you ...           0.047862  \n",
       "3  Sure, here's an updated version of the AI syst...           0.051860  \n",
       "4  To bring the drone back to base for landing, c...           0.066802  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df_loaded = pd.read_json(\"eval_df.jsonl\", orient=\"records\", lines=True)\n",
    "eval_df_loaded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
