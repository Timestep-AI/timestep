{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "from tinygrad import Device, Tensor, nn\n",
    "from tinygrad.nn.state import get_state_dict, safe_load, torch_load, load_state_dict, get_parameters, safe_save\n",
    "from transformers import AutoTokenizer, AutoConfig, DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "from timestep.config import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  CUDA\n"
     ]
    }
   ],
   "source": [
    "device = Device.DEFAULT\n",
    "print('device: ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages:  [{'role': 'system', 'content': 'You are an intelligent AI that controls a drone. Given a command or request from the user,\\ncall one of your functions to complete the request. If the request cannot be completed by your available functions, call the reject_request function.\\nIf the request is ambiguous or unclear, reject the request.'}, {'role': 'user', 'content': \"Let's get the drone in the air, how high should it go?\"}, {'role': 'assistant', 'tool_calls': [{'id': 'call_id', 'type': 'function', 'function': {'name': 'takeoff_drone', 'arguments': '{\"altitude\": 100}'}}]}]\n",
      "parallel_tool_calls:  False\n",
      "tools:  [{'type': 'function', 'function': {'name': 'takeoff_drone', 'parameters': {'type': 'object', 'properties': {'altitude': {'type': 'integer'}}, 'required': ['altitude']}}}, {'type': 'function', 'function': {'name': 'land_drone', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'enum': ['current', 'home_base', 'custom']}, 'coordinates': {'type': 'object'}}, 'required': ['location']}}}, {'type': 'function', 'function': {'name': 'control_drone_movement', 'parameters': {'type': 'object', 'properties': {'direction': {'type': 'string', 'enum': ['forward', 'backward', 'left', 'right', 'up', 'down']}, 'distance': {'type': 'integer'}}, 'required': ['direction', 'distance']}}}, {'type': 'function', 'function': {'name': 'set_drone_speed', 'parameters': {'type': 'object', 'properties': {'speed': {'type': 'integer', 'minimum': 0}}, 'required': ['speed']}}}, {'type': 'function', 'function': {'name': 'control_camera', 'parameters': {'type': 'object', 'properties': {'mode': {'type': 'string', 'enum': ['photo', 'video', 'panorama']}, 'duration': {'type': 'integer'}}, 'required': ['mode']}}}, {'type': 'function', 'function': {'name': 'control_gimbal', 'parameters': {'type': 'object', 'properties': {'tilt': {'type': 'integer'}, 'pan': {'type': 'integer'}}, 'required': ['tilt', 'pan']}}}, {'type': 'function', 'function': {'name': 'set_drone_lighting', 'parameters': {'type': 'object', 'properties': {'mode': {'type': 'string', 'enum': ['on', 'off', 'blink', 'sos']}}, 'required': ['mode']}}}, {'type': 'function', 'function': {'name': 'return_to_home', 'parameters': {'type': 'object', 'properties': {}}}}, {'type': 'function', 'function': {'name': 'set_battery_saver_mode', 'parameters': {'type': 'object', 'properties': {'status': {'type': 'string', 'enum': ['on', 'off']}}, 'required': ['status']}}}, {'type': 'function', 'function': {'name': 'set_obstacle_avoidance', 'parameters': {'type': 'object', 'properties': {'mode': {'type': 'string', 'enum': ['on', 'off']}}, 'required': ['mode']}}}, {'type': 'function', 'function': {'name': 'set_follow_me_mode', 'parameters': {'type': 'object', 'properties': {'status': {'type': 'string', 'enum': ['on', 'off']}}, 'required': ['status']}}}, {'type': 'function', 'function': {'name': 'calibrate_sensors', 'parameters': {'type': 'object', 'properties': {}}}}, {'type': 'function', 'function': {'name': 'set_autopilot', 'parameters': {'type': 'object', 'properties': {'status': {'type': 'string', 'enum': ['on', 'off']}}, 'required': ['status']}}}, {'type': 'function', 'function': {'name': 'configure_led_display', 'parameters': {'type': 'object', 'properties': {'pattern': {'type': 'string', 'enum': ['solid', 'blink', 'pulse', 'rainbow']}, 'color': {'type': 'string', 'enum': ['red', 'blue', 'green', 'yellow', 'white']}}, 'required': ['pattern']}}}, {'type': 'function', 'function': {'name': 'set_home_location', 'parameters': {'type': 'object', 'properties': {'coordinates': {'type': 'object'}}, 'required': ['coordinates']}}}, {'type': 'function', 'function': {'name': 'reject_request', 'parameters': {'type': 'object', 'properties': {}}}}]\n"
     ]
    }
   ],
   "source": [
    "conversations = []\n",
    "\n",
    "with open(\"../../data/drone_training.jsonl\") as f:\n",
    "    for line in f:\n",
    "        conversations.append(json.loads(line))\n",
    "\n",
    "conversation = conversations[0]\n",
    "\n",
    "print('messages: ', conversation[\"messages\"])\n",
    "\n",
    "print('parallel_tool_calls: ', conversation[\"parallel_tool_calls\"])\n",
    "\n",
    "print('tools: ', conversation[\"tools\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", eos_token=\"<|im_end|>\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% for message in messages %}<|im_start|>{{ message.role }}\n",
      "{% if message.role == 'system' %}{{ message.content }}{% if tool_calls %}\n",
      "\n",
      "You have access to the following functions:\n",
      "{% for tool in tools %}\n",
      "functions.{{ tool.function.name }}:\n",
      "{{ tool.function.parameters | tojson }}\n",
      "{% endfor %}\n",
      "\n",
      "You can respond to users messages with either a single message or one or more function calls.\n",
      "\n",
      "To respond with a message begin the message with 'message:', use the following format:\n",
      "\n",
      "message:\n",
      "<message>\n",
      "\n",
      "To respond with one or more function calls begin the message with 'functions.<function_name>:', use the following format:\n",
      "\n",
      "functions.<function_name>:\n",
      "{ \"arg1\": \"value1\", \"arg2\": \"value2\" }\n",
      "functions.<function_name>:\n",
      "{ \"arg1\": \"value1\", \"arg2\": \"value2\" }{% endif %}<|im_end|>\n",
      "{% endif %}{% if message.role == 'user' %}{{ message.content }}<|im_end|>\n",
      "{% endif %}{% if message.role == 'assistant' %}{% if message.content and message.content | length > 0 %}{% if tool_calls %}message:\n",
      "{% endif %}{{ message.content }}<|im_end|>\n",
      "{% endif %}{% if 'tool_calls' in message %}{% for tool_call in message.tool_calls %}functions.{{ tool_call.function.name }}:\n",
      "{{ tool_call.function.arguments }}{% endfor %}<|im_end|>\n",
      "{% endif %}{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n",
      "{% endif %}\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/abetlen/llama-cpp-python/blob/658b244c5aa924fc6f4d04f92445dd8f724b6017/llama_cpp/llama_chat_format.py#L3345\n",
    "function_calling_template = (\n",
    "        \"{% for message in messages %}\"\n",
    "        \"<|im_start|>{{ message.role }}\\n\"\n",
    "        # System message\n",
    "        \"{% if message.role == 'system' %}\"\n",
    "        \"{{ message.content }}\"\n",
    "        \"{% if tool_calls %}\"\n",
    "        \"\\n\\nYou have access to the following functions:\\n\"\n",
    "        \"{% for tool in tools %}\"\n",
    "        \"\\nfunctions.{{ tool.function.name }}:\\n\"\n",
    "        \"{{ tool.function.parameters | tojson }}\"\n",
    "        \"\\n{% endfor %}\"\n",
    "        \"\\n\\nYou can respond to users messages with either a single message or one or more function calls.\"\n",
    "        \"\\n\\nTo respond with a message begin the message with 'message:', use the following format:\"\n",
    "        \"\\n\\nmessage:\"\n",
    "        \"\\n<message>\"\n",
    "        \"\\n\\nTo respond with one or more function calls begin the message with 'functions.<function_name>:', use the following format:\"\n",
    "        \"\\n\\nfunctions.<function_name>:\"\n",
    "        '\\n{ \"arg1\": \"value1\", \"arg2\": \"value2\" }'\n",
    "        \"\\nfunctions.<function_name>:\"\n",
    "        '\\n{ \"arg1\": \"value1\", \"arg2\": \"value2\" }'\n",
    "        \"{% endif %}\"\n",
    "        \"<|im_end|>\\n\"\n",
    "        \"{% endif %}\"\n",
    "        # User message\n",
    "        \"{% if message.role == 'user' %}\"\n",
    "        \"{{ message.content }}\"\n",
    "        \"<|im_end|>\\n\"\n",
    "        \"{% endif %}\"\n",
    "        # Assistant message\n",
    "        \"{% if message.role == 'assistant' %}\"\n",
    "        ## Reglar message\n",
    "        \"{% if message.content and message.content | length > 0 %}\"\n",
    "        \"{% if tool_calls %}\"\n",
    "        \"message:\\n\"\n",
    "        \"{% endif %}\"\n",
    "        \"{{ message.content }}\"\n",
    "        \"<|im_end|>\\n\"\n",
    "        \"{% endif %}\"\n",
    "        ## Function calls\n",
    "        \"{% if 'tool_calls' in message %}\"\n",
    "        \"{% for tool_call in message.tool_calls %}\"\n",
    "        \"functions.{{ tool_call.function.name }}:\\n\"\n",
    "        \"{{ tool_call.function.arguments }}\"\n",
    "        \"{% endfor %}\"\n",
    "        \"<|im_end|>\\n\"\n",
    "        \"{% endif %}\"\n",
    "        \"{% endif %}\"\n",
    "        \"{% endfor %}\"\n",
    "        \"{% if add_generation_prompt %}<|im_start|>assistant\\n{% endif %}\"\n",
    "    )\n",
    "\n",
    "print(function_calling_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% for message in messages %}\n",
      "{% if message['role'] == 'user' %}\n",
      "{{ '<|user|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'system' %}\n",
      "{{ '<|system|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'assistant' %}\n",
      "{{ '<|assistant|>\n",
      "'  + message['content'] + eos_token }}\n",
      "{% endif %}\n",
      "{% if loop.last and add_generation_prompt %}\n",
      "{{ '<|assistant|>' }}\n",
      "{% endif %}\n",
      "{% endfor %}\n"
     ]
    }
   ],
   "source": [
    "# tokenizer.chat_template = function_calling_template\n",
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method apply_chat_template in module transformers.tokenization_utils_base:\n",
      "\n",
      "apply_chat_template(conversation: Union[List[Dict[str, str]], List[List[Dict[str, str]]]], tools: Optional[List[Dict]] = None, documents: Optional[List[Dict[str, str]]] = None, chat_template: Optional[str] = None, add_generation_prompt: bool = False, tokenize: bool = True, padding: bool = False, truncation: bool = False, max_length: Optional[int] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, return_dict: bool = False, return_assistant_tokens_mask: bool = False, tokenizer_kwargs: Optional[Dict[str, Any]] = None, **kwargs) -> Union[str, List[int], List[str], List[List[int]], transformers.tokenization_utils_base.BatchEncoding] method of transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast instance\n",
      "    Converts a list of dictionaries with `\"role\"` and `\"content\"` keys to a list of token\n",
      "    ids. This method is intended for use with chat models, and will read the tokenizer's chat_template attribute to\n",
      "    determine the format and control tokens to use when converting.\n",
      "    \n",
      "    Args:\n",
      "        conversation (Union[List[Dict[str, str]], List[List[Dict[str, str]]]]): A list of dicts\n",
      "            with \"role\" and \"content\" keys, representing the chat history so far.\n",
      "        tools (`List[Dict]`, *optional*):\n",
      "            A list of tools (callable functions) that will be accessible to the model. If the template does not\n",
      "            support function calling, this argument will have no effect. Each tool should be passed as a JSON Schema,\n",
      "            giving the name, description and argument types for the tool. See our\n",
      "            [chat templating guide](https://huggingface.co/docs/transformers/main/en/chat_templating#automated-function-conversion-for-tool-use)\n",
      "            for more information.\n",
      "        documents (`List[Dict[str, str]]`, *optional*):\n",
      "            A list of dicts representing documents that will be accessible to the model if it is performing RAG\n",
      "            (retrieval-augmented generation). If the template does not support RAG, this argument will have no\n",
      "            effect. We recommend that each document should be a dict containing \"title\" and \"text\" keys. Please\n",
      "            see the RAG section of the [chat templating guide](https://huggingface.co/docs/transformers/main/en/chat_templating#arguments-for-RAG)\n",
      "            for examples of passing documents with chat templates.\n",
      "        chat_template (`str`, *optional*):\n",
      "            A Jinja template to use for this conversion. It is usually not necessary to pass anything to this\n",
      "            argument, as the model's template will be used by default.\n",
      "        add_generation_prompt (bool, *optional*): Whether to end the prompt with the token(s) that indicate\n",
      "            the start of an assistant message. This is useful when you want to generate a response from the model.\n",
      "            Note that this argument will be passed to the chat template, and so it must be supported in the\n",
      "            template for this argument to have any effect.\n",
      "        tokenize (`bool`, defaults to `True`):\n",
      "            Whether to tokenize the output. If `False`, the output will be a string.\n",
      "        padding (`bool`, defaults to `False`):\n",
      "            Whether to pad sequences to the maximum length. Has no effect if tokenize is `False`.\n",
      "        truncation (`bool`, defaults to `False`):\n",
      "            Whether to truncate sequences at the maximum length. Has no effect if tokenize is `False`.\n",
      "        max_length (`int`, *optional*):\n",
      "            Maximum length (in tokens) to use for padding or truncation. Has no effect if tokenize is `False`. If\n",
      "            not specified, the tokenizer's `max_length` attribute will be used as a default.\n",
      "        return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      "            If set, will return tensors of a particular framework. Has no effect if tokenize is `False`. Acceptable\n",
      "            values are:\n",
      "            - `'tf'`: Return TensorFlow `tf.Tensor` objects.\n",
      "            - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      "            - `'np'`: Return NumPy `np.ndarray` objects.\n",
      "            - `'jax'`: Return JAX `jnp.ndarray` objects.\n",
      "        return_dict (`bool`, defaults to `False`):\n",
      "            Whether to return a dictionary with named outputs. Has no effect if tokenize is `False`.\n",
      "        tokenizer_kwargs (`Dict[str: Any]`, *optional*): Additional kwargs to pass to the tokenizer.\n",
      "        return_assistant_tokens_mask (`bool`, defaults to `False`):\n",
      "            Whether to return a mask of the assistant generated tokens. For tokens generated by the assistant,\n",
      "            the mask will contain 1. For user and system tokens, the mask will contain 0.\n",
      "            This functionality is only available for chat templates that support it via the `{% generation %}` keyword.\n",
      "        **kwargs: Additional kwargs to pass to the template renderer. Will be accessible by the chat template.\n",
      "    \n",
      "    Returns:\n",
      "        `Union[List[int], Dict]`: A list of token ids representing the tokenized chat so far, including control tokens. This\n",
      "        output is ready to pass to the model, either directly or via methods like `generate()`. If `return_dict` is\n",
      "        set, will return a dict of tokenizer outputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tokenizer.apply_chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are an intelligent AI that controls a drone. Given a command or request from the user,\\ncall one of your functions to complete the request. If the request cannot be completed by your available functions, call the reject_request function.\\nIf the request is ambiguous or unclear, reject the request.'},\n",
       " {'role': 'user',\n",
       "  'content': \"Let's get the drone in the air, how high should it go?\"}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_messages = conversation[\"messages\"][0:2]\n",
    "content_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[529,\n",
       " 29989,\n",
       " 5205,\n",
       " 29989,\n",
       " 29958,\n",
       " 13,\n",
       " 3492,\n",
       " 526,\n",
       " 385,\n",
       " 13052,\n",
       " 296,\n",
       " 319,\n",
       " 29902,\n",
       " 393,\n",
       " 11761,\n",
       " 263,\n",
       " 4192,\n",
       " 650,\n",
       " 29889,\n",
       " 11221,\n",
       " 263,\n",
       " 1899,\n",
       " 470,\n",
       " 2009,\n",
       " 515,\n",
       " 278,\n",
       " 1404,\n",
       " 29892,\n",
       " 13,\n",
       " 4804,\n",
       " 697,\n",
       " 310,\n",
       " 596,\n",
       " 3168,\n",
       " 304,\n",
       " 4866,\n",
       " 278,\n",
       " 2009,\n",
       " 29889,\n",
       " 960,\n",
       " 278,\n",
       " 2009,\n",
       " 2609,\n",
       " 367,\n",
       " 8676,\n",
       " 491,\n",
       " 596,\n",
       " 3625,\n",
       " 3168,\n",
       " 29892,\n",
       " 1246,\n",
       " 278,\n",
       " 12560,\n",
       " 29918,\n",
       " 3827,\n",
       " 740,\n",
       " 29889,\n",
       " 13,\n",
       " 3644,\n",
       " 278,\n",
       " 2009,\n",
       " 338,\n",
       " 22363,\n",
       " 681,\n",
       " 470,\n",
       " 20871,\n",
       " 29892,\n",
       " 12560,\n",
       " 278,\n",
       " 2009,\n",
       " 29889,\n",
       " 2,\n",
       " 29871,\n",
       " 13,\n",
       " 29966,\n",
       " 29989,\n",
       " 1792,\n",
       " 29989,\n",
       " 29958,\n",
       " 13,\n",
       " 12024,\n",
       " 29915,\n",
       " 29879,\n",
       " 679,\n",
       " 278,\n",
       " 4192,\n",
       " 650,\n",
       " 297,\n",
       " 278,\n",
       " 4799,\n",
       " 29892,\n",
       " 920,\n",
       " 1880,\n",
       " 881,\n",
       " 372,\n",
       " 748,\n",
       " 29973,\n",
       " 2,\n",
       " 29871,\n",
       " 13]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.apply_chat_template(conversation=conversation[\"messages\"], tools=conversation[\"tools\"])\n",
    "tokenizer.apply_chat_template(conversation=content_messages, tools=conversation[\"tools\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenizer.apply_chat_template(conversation=conversation[\"messages\"], tools=conversation[\"tools\"], tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are an intelligent AI that controls a drone. Given a command or request from the user,\n",
      "call one of your functions to complete the request. If the request cannot be completed by your available functions, call the reject_request function.\n",
      "If the request is ambiguous or unclear, reject the request.</s>\n",
      "<|user|>\n",
      "Let's get the drone in the air, how high should it go?</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.apply_chat_template(conversation=content_messages, tools=conversation[\"tools\"], tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(conversation):\n",
    "    return tokenizer.apply_chat_template(\n",
    "        conversation=conversation[\"messages\"],\n",
    "        padding=\"max_length\",\n",
    "        tools=conversation[\"tools\"],\n",
    "        tokenize=True,\n",
    "        truncation=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_conversation = tokenize(conversation)\n",
    "print(tokenized_conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(settings.app_dir, \"models/TinyLlama/TinyLlama-1.1B-Chat-v1.0/model.safetensors\")\n",
    "model_path\n",
    "\n",
    "tokenizer_path = os.path.join(settings.app_dir, \"models/TinyLlama/TinyLlama-1.1B-Chat-v1.0/tokenizer.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = SentencePieceProcessor(model_file=str(tokenizer_path))\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    # add_generation_prompt=True,\n",
    "    conversation=messages,\n",
    "    # padding=\"max_length\",\n",
    "    # tools=conversation[\"tools\"],\n",
    "    tokenize=False,\n",
    "    # truncation=True,\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.bos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_prompt = tokenizer.encode(prompt)\n",
    "encoded_prompt_2 = t.encode(prompt)\n",
    "\n",
    "print(encoded_prompt)\n",
    "print([1] + encoded_prompt_2)\n",
    "\n",
    "# assert encoded_prompt == encoded_prompt_2, f\"\\n{encoded_prompt} \\n!=\\n{encoded_prompt_2}\"\n",
    "assert encoded_prompt == [1] + encoded_prompt_2, f\"\\n{encoded_prompt} \\n!=\\n{[1] + encoded_prompt_2}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.bos_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.decode([1]), tokenizer.decode([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"Hello, how are you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(t.encode)\n",
    "# t.encode(conversation[\"messages\"][0][\"content\"])\n",
    "t.encode(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert t.encode(message) == tokenizer.encode(message), f\"{t.encode(message)} != {tokenizer.encode(message)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception('stop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    return {\"bleu\": 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"args\": {\"dim\": 2048, \"n_layers\": 22, \"n_heads\": 32, \"n_kv_heads\": 4, \"norm_eps\": 1e-05, \"vocab_size\": 32000, \"hidden_dim\": 5632},\n",
    "}\n",
    "\n",
    "assert config.hidden_size == params[\"args\"][\"dim\"]\n",
    "assert config.intermediate_size == params[\"args\"][\"hidden_dim\"]\n",
    "assert config.num_attention_heads == params[\"args\"][\"n_heads\"]\n",
    "assert config.num_hidden_layers == params[\"args\"][\"n_layers\"]\n",
    "assert config.num_key_value_heads == params[\"args\"][\"n_kv_heads\"]\n",
    "assert config.rms_norm_eps == params[\"args\"][\"norm_eps\"]\n",
    "assert config.vocab_size == params[\"args\"][\"vocab_size\"]\n",
    "assert config.vocab_size == tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.Research.tinygrad.llama import Transformer, convert_from_huggingface, fix_bf16\n",
    "# from notebooks.Research.tinygrad.train_llama import LLaMa\n",
    "\n",
    "# llama = LLaMa.build(MODEL_PATH, TOKENIZER_PATH, model_gen=args.gen, model_size=args.size, quantize=args.quantize, device=device)\n",
    "\n",
    "# model = Transformer(**params[\"args\"], linear=linear, max_context=MAX_CONTEXT, jit=bool(JIT))\n",
    "model = Transformer(\n",
    "    dim=config.hidden_size,\n",
    "    hidden_dim=config.intermediate_size,\n",
    "    # max_context=4096,\n",
    "    n_heads=config.num_attention_heads,\n",
    "    n_layers=config.num_hidden_layers,\n",
    "    norm_eps=config.rms_norm_eps,\n",
    "    vocab_size=config.vocab_size,\n",
    ")\n",
    "# model\n",
    "\n",
    "weights = safe_load(str(model_path))\n",
    "\n",
    "if \"model.embed_tokens.weight\" in weights:\n",
    "    weights = convert_from_huggingface(weights, model, params[\"args\"][\"n_heads\"], params[\"args\"].get(\"n_kv_heads\", params[\"args\"][\"n_heads\"]))\n",
    "\n",
    "weights = fix_bf16(weights)\n",
    "\n",
    "load_state_dict(model, weights, strict=False, consume=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.bos_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_conversation_tensor = Tensor([tokenized_conversation], device=device)\n",
    "# tokenized_conversation_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What's 2 + 2?\"\n",
    "messages = [ {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": query,\n",
    "} ]\n",
    "tokenized_query = tokenize({\"messages\": messages, \"tools\": []})\n",
    "tokenized_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_pos = 0\n",
    "# toks = tokenized_conversation\n",
    "toks = tokenized_query\n",
    "\n",
    "Tensor.training = False\n",
    "# model(tokenized_conversation_tensor, start_pos=start_pos, temperature=0.0)\n",
    "\n",
    "# llama.model(Tensor([toks], device=device), 0, args.temperature).realize()\n",
    "# model(Tensor([toks], device=device), 0, args.temperature).realize()\n",
    "model(Tensor([toks], device=device), 0, 0.0).realize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
