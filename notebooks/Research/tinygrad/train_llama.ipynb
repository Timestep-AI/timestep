{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'app_dir': '/home/mjschock/.config/timestep',\n",
       " 'bearerinfo_func': 'timestep.api.decode_token',\n",
       " 'default_hf_repo_id': 'Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile',\n",
       " 'default_llamafile_host': '0.0.0.0',\n",
       " 'default_llamafile_port': 8080,\n",
       " 'default_model_filename': 'TinyLlama-1.1B-Chat-v1.0.F16.llamafile',\n",
       " 'default_multimodal_model_projector_filename': None,\n",
       " 'openai_api_key': SecretStr('**********'),\n",
       " 'openai_base_url': 'http://localhost:8000/api/openai/v1',\n",
       " 'openai_org_id': 'organization_id',\n",
       " 'openai_project_id': 'project_id',\n",
       " 'poetry_repositories_testpypi_url': 'https://test.pypi.org/legacy/',\n",
       " 'poetry_virtualenvs_in_project': True,\n",
       " 'poetry_virtualenvs_prefer_active_python': True,\n",
       " 'prefect_api_url': 'http://127.0.0.1:4200/api',\n",
       " 'prefect_logging_level': 'INFO',\n",
       " 'prefect_logging_log_prints': True,\n",
       " 'pyenv_version': '3.10.14',\n",
       " 'verbose': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from timestep.config import Settings\n",
    "\n",
    "settings = Settings()\n",
    "settings.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_repo_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using CUDA backend\n",
      "using LLaMA-tiny-1B-Chat model\n",
      "ram used:  2.20 GB, freqs_cis                                         : 100%|â–ˆ| \n",
      "loaded weights in 3908.27 ms, 2.20 GB loaded at 0.56 GB/s\n",
      "tok_embeddings.weight\n",
      "freqs_cis\n"
     ]
    }
   ],
   "source": [
    "!python3 train_llama.py --gen=\"tiny\" --model=\"{settings.app_dir}/models/{hf_repo_id}/model.safetensors\" --size=\"1B-Chat\" --temperature=0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
