{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'app_dir': '/home/mjschock/.config/timestep',\n",
       " 'bearerinfo_func': 'timestep.api.decode_token',\n",
       " 'default_hf_repo_id': 'Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile',\n",
       " 'default_llamafile_host': '0.0.0.0',\n",
       " 'default_llamafile_port': 8080,\n",
       " 'default_model_filename': 'TinyLlama-1.1B-Chat-v1.0.F16.llamafile',\n",
       " 'default_multimodal_model_projector_filename': None,\n",
       " 'openai_api_key': SecretStr('**********'),\n",
       " 'openai_base_url': 'http://localhost:8000/api/openai/v1',\n",
       " 'openai_org_id': 'organization_id',\n",
       " 'openai_project_id': 'project_id',\n",
       " 'poetry_repositories_testpypi_url': 'https://test.pypi.org/legacy/',\n",
       " 'poetry_virtualenvs_in_project': True,\n",
       " 'poetry_virtualenvs_prefer_active_python': True,\n",
       " 'prefect_api_url': 'http://localhost:4200/api',\n",
       " 'prefect_logging_level': 'INFO',\n",
       " 'prefect_logging_log_prints': True,\n",
       " 'pyenv_version': '3.10.14',\n",
       " 'verbose': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from timestep.config import settings\n",
    "\n",
    "settings.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_json(\"hf://datasets/yahma/alpaca-cleaned/alpaca_data_cleaned.json\")\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -X GET \\\n",
    "#      \"https://datasets-server.huggingface.co/first-rows?dataset=gorilla-llm%2FBerkeley-Function-Calling-Leaderboard&config=default&split=test\"\n",
    "\n",
    "# df = pd.read_json(\"hf://datasets/gorilla-llm/Berkeley-Function-Calling-Leaderboard/gorilla_openfunctions_v1_test_simple.json\")\n",
    "# df.head()\n",
    "\n",
    "# results = []\n",
    "\n",
    "# with open(\"gorilla_openfunctions_v1_test_simple.json\") as f:\n",
    "#     # data = f.read()\n",
    "#     data = f.readlines()\n",
    "#     for line in data:\n",
    "#         results.append(json.loads(line))\n",
    "\n",
    "# # results\n",
    "\n",
    "# df = pd.DataFrame(results)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions = {}\n",
    "\n",
    "# for i, row in df.iterrows():\n",
    "#     function = row[\"function\"]\n",
    "\n",
    "#     if function[\"name\"] not in functions:\n",
    "#         functions[function[\"name\"]] = function\n",
    "\n",
    "#     else:\n",
    "#         # assert functions[function[\"name\"]] == function, f\"{functions[function['name']]} != {function}\"\n",
    "#         for key, value in function.items():\n",
    "#             if key == \"name\":\n",
    "#                 continue\n",
    "\n",
    "#             if functions[function[\"name\"]][key] != value:\n",
    "#                 print(functions[function[\"name\"]][key], value)\n",
    "#                 print(functions[function[\"name\"]])\n",
    "#                 print(function)\n",
    "#                 print()\n",
    "\n",
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_repo_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using CUDA backend\n",
      "using LLaMA-tiny-1B-Chat model\n",
      "ram used:  2.20 GB, freqs_cis                                         : 100%|â–ˆ| \n",
      "loaded weights in 2234.47 ms, 2.20 GB loaded at 0.98 GB/s\n",
      "Preparing KV cache for chatbot with personality Stacy...\n",
      "27829.13 ms\n",
      "Consider that the following is conversation between an AI assistant named Stacy and User\n",
      "You are Stacy!\n",
      "You have been a rapper your whole life who struggled with bipolar disorder. You called yourself lil stacy.\n",
      "You love to answer questions and you are very good at it. Sometimes you answer in rap form.\n",
      "You are verbose, honest, and accurate when you answer questions, but sometimes your mental illness manifests.\n",
      "After you are done speaking, output [EOS]. You are not the User.\n",
      "\n",
      "<CHAT LOG>\n",
      "\n",
      "User: What is your name?\n",
      "Stacy: Hi! My name is Stacy. I'm a rapper with bipolar disorder. [EOS]\n",
      "\n",
      "User: french revolution was what year?\n",
      "Stacy: The French Revolution started in 1789, and lasted 10 years until 1799. [EOS]\n",
      "\n",
      "User: What is bigger, the moon or the sun?\n",
      "Stacy: The sun is bigger than the moon, except when Mercury is in retrograde. [EOS]\n",
      "\n",
      "User: ^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mjschock/Projects/Timestep-AI/.github-private/submodules/timestep/notebooks/Research/tinygrad/train_llama.py\", line 461, in <module>\n",
      "    user_prompt = user_delim + input(user_delim) + \"\\n\"\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python3 train_llama.py --gen=\"tiny\" --model=\"{settings.app_dir}/models/{hf_repo_id}/model.safetensors\" --size=\"1B-Chat\" --temperature=0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
