{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tinygrad TinyLlama-1.1B-Chat-v1.0\n",
    "\n",
    "This example was copied and modified from:\n",
    "- https://github.com/tinygrad/tinygrad/blob/66d0b14a205c33e8a6ae4e9a765f6351b34c2923/examples/llama.py.\n",
    "- https://github.com/tinygrad/tinygrad/blob/66d0b14a205c33e8a6ae4e9a765f6351b34c2923/extra/models/llama.py\n",
    "- https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tinygrad and CUDA device.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'app_dir': '/home/mjschock/.config/timestep',\n",
       " 'bearerinfo_func': 'timestep.api.decode_token',\n",
       " 'default_hf_repo_id': 'Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile',\n",
       " 'default_llamafile_host': '0.0.0.0',\n",
       " 'default_llamafile_port': 8080,\n",
       " 'default_model_filename': 'TinyLlama-1.1B-Chat-v1.0.F16.llamafile',\n",
       " 'default_multimodal_model_projector_filename': None,\n",
       " 'openai_api_key': SecretStr('**********'),\n",
       " 'openai_base_url': 'http://localhost:8000/api/openai/v1',\n",
       " 'openai_org_id': 'organization_id',\n",
       " 'openai_project_id': 'project_id',\n",
       " 'poetry_repositories_testpypi_url': 'https://test.pypi.org/legacy/',\n",
       " 'poetry_virtualenvs_in_project': True,\n",
       " 'poetry_virtualenvs_prefer_active_python': True,\n",
       " 'prefect_api_url': 'http://127.0.0.1:4200/api',\n",
       " 'prefect_logging_level': 'INFO',\n",
       " 'prefect_logging_log_prints': True,\n",
       " 'pyenv_version': '3.10.14',\n",
       " 'verbose': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "from os import getenv\n",
    "\n",
    "from tqdm import tqdm\n",
    "import mlflow\n",
    "import openai\n",
    "import pandas as pd\n",
    "# from sentencepiece import SentencePieceProcessor\n",
    "from tinygrad import Device, Tensor, nn, Variable, GlobalCounters\n",
    "from tinygrad.helpers import Context, Timing, Profiling, DEBUG, JIT, getenv, colored\n",
    "from tinygrad.nn.state import get_state_dict, safe_load, torch_load, load_state_dict, get_parameters, safe_save\n",
    "from transformers import AutoTokenizer, AutoConfig, DataCollatorWithPadding, pipeline, Trainer, TrainingArguments\n",
    "\n",
    "# from notebooks.Research.lib.transformers.src.transformers.models.llama.modeling_tinygrad_llama import TinygradLlamaForCausalLM\n",
    "# from notebooks.Research.tinygrad.train_llama import function_calling_template\n",
    "from timestep.config import settings\n",
    "\n",
    "from src.transformers.models.llama.modeling_tinygrad_llama import TinygradLlamaForCausalLM\n",
    "\n",
    "device = Device.DEFAULT\n",
    "print(f'Using Tinygrad and {device} device.')\n",
    "\n",
    "settings.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_repo_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_name_or_path\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 2048,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 5632,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 22,\n",
       "  \"num_key_value_heads\": 4,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.44.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# config = AutoConfig.from_pretrained(hf_repo_id, bos_token_id=bos_token_id, eos_token_id=eos_token_id)\n",
    "config = AutoConfig.from_pretrained(hf_repo_id)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ram used:  2.20 GB, freqs_cis                                         : 100%|â–ˆ| \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded weights in 2251.91 ms, 2.20 GB loaded at 0.98 GB/s\n"
     ]
    }
   ],
   "source": [
    "# model_path = os.path.join(settings.app_dir, f\"models/{hf_repo_id}/model.safetensors\")\n",
    "model_path: Path = Path(settings.app_dir) / f\"models/{hf_repo_id}/model.safetensors\"\n",
    "# tokenizer_path = os.path.join(settings.app_dir, f\"models/{hf_repo_id}/tokenizer.model\")\n",
    "\n",
    "# model = TinygradLlamaForCausalLM.from_pretrained(hf_repo_id) # TODO: this throws PyTorch error\n",
    "model = TinygradLlamaForCausalLM(\n",
    "    config=config,\n",
    "    device=device,\n",
    "    model_path=model_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained( # TODO: make custom tokenizer that converts sentencepiece to tiktoken so it runs in Termux\n",
    "    hf_repo_id,\n",
    "    # bos_token=bos_token,\n",
    "    # clean_up_tokenization_spaces=True,\n",
    "    # eos_token=eos_token,\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokenizer.bos_token_id == model.config.bos_token_id, f\"{tokenizer.bos_token_id} != {model.config.bos_token_id}\"\n",
    "assert tokenizer.eos_token_id == model.config.eos_token_id, f\"{tokenizer.eos_token_id} != {model.config.eos_token_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are an intelligent AI that controls a drone. Given a command or request from the user,\\ncall one of your functions to complete the request. If the request cannot be completed by your available functions, call the reject_request function.\\nIf the request is ambiguous or unclear, reject the request.'},\n",
       " {'role': 'user',\n",
       "  'content': \"Let's get the drone in the air, how high should it go?\"},\n",
       " {'role': 'assistant',\n",
       "  'tool_calls': [{'id': 'call_id',\n",
       "    'type': 'function',\n",
       "    'function': {'name': 'takeoff_drone', 'arguments': '{\"altitude\": 100}'}}]}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations = []\n",
    "\n",
    "with open(\"../../data/drone_training.jsonl\") as f:\n",
    "    for line in f:\n",
    "        conversations.append(json.loads(line))\n",
    "\n",
    "conversation = conversations[0]\n",
    "\n",
    "# print('messages: ', conversation[\"messages\"])\n",
    "\n",
    "# print('parallel_tool_calls: ', conversation[\"parallel_tool_calls\"])\n",
    "\n",
    "# print('tools: ', conversation[\"tools\"])\n",
    "\n",
    "# conversation = conversation[\"messages\"][0:2]\n",
    "# conversation = conversation[\"messages\"]\n",
    "conversation[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# template = \"\"\"{%- for message in messages %}\n",
    "# {%- if message['role'] == 'assistant' %}\n",
    "# {{- '<|assistant|>' + '\\n' }}\n",
    "# {%- if message['content'] %}\n",
    "# {{- message['content'] + eos_token + '\\n' }}\n",
    "# {%- elif message['tool_calls'] %}\n",
    "# {{- 'tool_calls:' + '\\n' }}\n",
    "# {%- for tool_call in message['tool_calls'] %}\n",
    "# {{- tool_call['function']['name'] + ': ' + tool_call['function']['arguments'] }}\n",
    "# {%- endfor %}\n",
    "# {{- eos_token + '\\n' }}\n",
    "# {%- endif %}\n",
    "# {%- elif message['role'] == 'system' %}\n",
    "# {{- '<|system|>' + '\\n' }}\n",
    "# {{- message['content'] }}\n",
    "# {%- if tools %}\n",
    "# {{- '\\n\\nYou are aware of the following tools:\\n' }}\n",
    "# {%- for tool in tools %}\n",
    "# {{- tool.function.name }}: {{ tool.function.parameters | tojson }}\n",
    "# {%- endfor %}\n",
    "# {{- '\\nYou can suggest tool calls by responding in the following format:\\n' }}\n",
    "# {{- 'tool_calls:' + '\\n' }}\n",
    "# {{- 'tool_name: {\\\"arg1\\\": \\\"value1\\\", \\\"arg2\\\": \\\"value2\\\"}' }}\n",
    "# {{- eos_token + '\\n' }}\n",
    "# {%- endif %}\n",
    "# {%- elif message['role'] == 'user' %}\n",
    "# {{- '<|user|>' + '\\n' }}\n",
    "# {{- message['content'] + eos_token + '\\n' }}\n",
    "# {%- endif %}\n",
    "# {%- if loop.last and add_generation_prompt %}\n",
    "# {{- '<|assistant|>' + '\\n' }}\n",
    "# {%- endif %}\n",
    "# {%- endfor %}\"\"\"\n",
    "\n",
    "# open(\"template.jinja\", \"w\").write(template)\n",
    "open(\"template.jinja\", \"w\").write(tokenizer.chat_template)\n",
    "# # open(\"template.jinja\", \"w\").write(function_calling_template)\n",
    "# # open(\"template.jinja\", \"w\").write(llama_3_1_8b_instruct_chat_template)\n",
    "\n",
    "tokenizer.chat_template = open(\"template.jinja\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are an intelligent AI that controls a drone. Given a command or request from the user,\n",
      "call one of your functions to complete the request. If the request cannot be completed by your available functions, call the reject_request function.\n",
      "If the request is ambiguous or unclear, reject the request.</s>\n",
      "<|user|>\n",
      "Let's get the drone in the air, how high should it go?</s>\n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_prompt(add_generation_prompt=False, messages=[], tool_calls=True, tools=[]):\n",
    "    return tokenizer.apply_chat_template(\n",
    "        add_generation_prompt=add_generation_prompt,\n",
    "        conversation=messages,\n",
    "        return_tensors=False,\n",
    "        tokenize=False,\n",
    "        tool_calls=tool_calls,\n",
    "        tools=tools,\n",
    "    )\n",
    "\n",
    "prompt = generate_prompt(\n",
    "    add_generation_prompt=True,\n",
    "    messages=conversation[\"messages\"][0:2],\n",
    "    tools=conversation[\"tools\"],\n",
    ")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup: 0:00:01.548814\n",
      "Elapsed: 0:00:32.776468\n",
      "Elapsed: 0:00:34.327884\n",
      "The drone should be able to fly at a height of at least 10 meters (33 feet) above the ground. This is the minimum height required for safe and effective flight. If the drone is flying too low or too high, it may cause damage to the aircraft or the surrounding environment.</s>\n"
     ]
    }
   ],
   "source": [
    "def predict(prompt: str):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"np\", add_special_tokens=False)\n",
    "    inputs = {k: Tensor(v, device=device) for k, v in inputs.items()}\n",
    "\n",
    "    outputs = model.generate(**inputs, do_sample=True, max_new_tokens=256, temperature=0.0)\n",
    "\n",
    "    # decoded_output = tokenizer.decode(outputs[0][inputs['input_ids'].size(1):], skip_special_tokens=True)\n",
    "    decoded_output = tokenizer.decode(outputs[0][inputs['input_ids'].size(1):])\n",
    "\n",
    "    return decoded_output\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "prediction = predict(prompt)\n",
    "\n",
    "print(f\"Elapsed: {datetime.now()-start}\")\n",
    "\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
