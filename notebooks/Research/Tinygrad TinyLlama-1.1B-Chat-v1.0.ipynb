{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tinygrad TinyLlama-1.1B-Chat-v1.0\n",
    "\n",
    "This example was copied and modified from:\n",
    "- https://github.com/tinygrad/tinygrad/blob/66d0b14a205c33e8a6ae4e9a765f6351b34c2923/examples/llama.py.\n",
    "- https://github.com/tinygrad/tinygrad/blob/66d0b14a205c33e8a6ae4e9a765f6351b34c2923/extra/models/llama.py\n",
    "- https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python3 notebooks/Research/src/tinygrad/examples/llama.py \\\n",
    "    --gen=\"tiny\" \\\n",
    "    --model=\"/home/mjschock/.config/timestep/models/TinyLlama/TinyLlama-1.1B-Chat-v1.0/model.safetensors\" \\\n",
    "    --size=\"1B-Chat\" \\\n",
    "    --temperature=0 \\\n",
    "    --prompt=\"What's the weather in Oakland and San Francisco?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tinygrad and CUDA device.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'app_dir': '/home/mjschock/.config/timestep',\n",
       " 'bearerinfo_func': 'timestep.api.decode_token',\n",
       " 'default_hf_repo_id': 'Mozilla/TinyLlama-1.1B-Chat-v1.0-llamafile',\n",
       " 'default_llamafile_host': '0.0.0.0',\n",
       " 'default_llamafile_port': 8080,\n",
       " 'default_model_filename': 'TinyLlama-1.1B-Chat-v1.0.F16.llamafile',\n",
       " 'default_multimodal_model_projector_filename': None,\n",
       " 'openai_api_key': SecretStr('**********'),\n",
       " 'openai_base_url': 'http://localhost:8000/api/openai/v1',\n",
       " 'openai_org_id': 'organization_id',\n",
       " 'openai_project_id': 'project_id',\n",
       " 'poetry_repositories_testpypi_url': 'https://test.pypi.org/legacy/',\n",
       " 'poetry_virtualenvs_in_project': True,\n",
       " 'poetry_virtualenvs_prefer_active_python': True,\n",
       " 'prefect_api_url': 'http://127.0.0.1:4200/api',\n",
       " 'prefect_logging_level': 'INFO',\n",
       " 'prefect_logging_log_prints': True,\n",
       " 'pyenv_version': '3.10.14',\n",
       " 'verbose': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "from os import getenv\n",
    "\n",
    "from tqdm import tqdm\n",
    "import mlflow\n",
    "import openai\n",
    "import pandas as pd\n",
    "# from sentencepiece import SentencePieceProcessor\n",
    "from tinygrad import Device, Tensor, nn, Variable, GlobalCounters\n",
    "from tinygrad.helpers import Context, Timing, Profiling, DEBUG, JIT, getenv, colored\n",
    "from tinygrad.nn.state import get_state_dict, safe_load, torch_load, load_state_dict, get_parameters, safe_save\n",
    "from transformers import AutoTokenizer, AutoConfig, DataCollatorWithPadding, pipeline, Trainer, TrainingArguments\n",
    "\n",
    "# from notebooks.Research.lib.transformers.src.transformers.models.llama.modeling_tinygrad_llama import TinygradLlamaForCausalLM\n",
    "# from notebooks.Research.tinygrad.train_llama import function_calling_template\n",
    "from timestep.config import settings\n",
    "\n",
    "from src.transformers.models.llama.modeling_tinygrad_llama import TinygradLlamaForCausalLM\n",
    "from src.tinygrad.extra.training import evaluate, train\n",
    "\n",
    "device = Device.DEFAULT\n",
    "print(f'Using Tinygrad and {device} device.')\n",
    "\n",
    "settings.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_repo_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_name_or_path\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 2048,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 5632,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 22,\n",
       "  \"num_key_value_heads\": 4,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.44.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# config = AutoConfig.from_pretrained(hf_repo_id, bos_token_id=bos_token_id, eos_token_id=eos_token_id)\n",
    "config = AutoConfig.from_pretrained(hf_repo_id)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ram used:  2.20 GB, freqs_cis                                         : 100%|â–ˆ| \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded weights in 2646.58 ms, 2.20 GB loaded at 0.83 GB/s\n",
      "202\n"
     ]
    }
   ],
   "source": [
    "# model_path = os.path.join(settings.app_dir, f\"models/{hf_repo_id}/model.safetensors\")\n",
    "model_path: Path = Path(settings.app_dir) / f\"models/{hf_repo_id}/model.safetensors\"\n",
    "# tokenizer_path = os.path.join(settings.app_dir, f\"models/{hf_repo_id}/tokenizer.model\")\n",
    "\n",
    "# model = TinygradLlamaForCausalLM.from_pretrained(hf_repo_id) # TODO: this throws PyTorch error\n",
    "model = TinygradLlamaForCausalLM(\n",
    "    config=config,\n",
    "    device=device,\n",
    "    model_path=model_path,\n",
    ")\n",
    "\n",
    "print(len(nn.state.get_parameters(model.model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained( # TODO: make custom tokenizer that converts sentencepiece to tiktoken so it runs in Termux\n",
    "    hf_repo_id,\n",
    "    # bos_token=bos_token,\n",
    "    # clean_up_tokenization_spaces=True,\n",
    "    # eos_token=eos_token,\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokenizer.bos_token_id == model.config.bos_token_id, f\"{tokenizer.bos_token_id} != {model.config.bos_token_id}\"\n",
    "assert tokenizer.eos_token_id == model.config.eos_token_id, f\"{tokenizer.eos_token_id} != {model.config.eos_token_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are an intelligent AI that controls a drone. Given a command or request from the user,\\ncall one of your functions to complete the request. If the request cannot be completed by your available functions, call the reject_request function.\\nIf the request is ambiguous or unclear, reject the request.'},\n",
       " {'role': 'user',\n",
       "  'content': \"Let's get the drone in the air, how high should it go?\"},\n",
       " {'role': 'assistant',\n",
       "  'tool_calls': [{'id': 'call_id',\n",
       "    'type': 'function',\n",
       "    'function': {'name': 'takeoff_drone', 'arguments': '{\"altitude\": 100}'}}]}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations = []\n",
    "\n",
    "with open(\"../../data/drone_training.jsonl\") as f:\n",
    "    for line in f:\n",
    "        conversations.append(json.loads(line))\n",
    "\n",
    "conversation = conversations[0]\n",
    "\n",
    "# print('messages: ', conversation[\"messages\"])\n",
    "\n",
    "# print('parallel_tool_calls: ', conversation[\"parallel_tool_calls\"])\n",
    "\n",
    "# print('tools: ', conversation[\"tools\"])\n",
    "\n",
    "# conversation = conversation[\"messages\"][0:2]\n",
    "# conversation = conversation[\"messages\"]\n",
    "conversation[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# template = \"\"\"{%- for message in messages %}\n",
    "# {%- if message['role'] == 'assistant' %}\n",
    "# {{- '<|assistant|>' + '\\n' }}\n",
    "# {%- if message['content'] %}\n",
    "# {{- message['content'] + eos_token + '\\n' }}\n",
    "# {%- elif message['tool_calls'] %}\n",
    "# {{- 'tool_calls:' + '\\n' }}\n",
    "# {%- for tool_call in message['tool_calls'] %}\n",
    "# {{- tool_call['function']['name'] + ': ' + tool_call['function']['arguments'] }}\n",
    "# {%- endfor %}\n",
    "# {{- eos_token + '\\n' }}\n",
    "# {%- endif %}\n",
    "# {%- elif message['role'] == 'system' %}\n",
    "# {{- '<|system|>' + '\\n' }}\n",
    "# {{- message['content'] }}\n",
    "# {%- if tools %}\n",
    "# {{- '\\n\\nYou are aware of the following tools:\\n' }}\n",
    "# {%- for tool in tools %}\n",
    "# {{- tool.function.name }}: {{ tool.function.parameters | tojson }}\n",
    "# {%- endfor %}\n",
    "# {{- '\\nYou can suggest tool calls by responding in the following format:\\n' }}\n",
    "# {{- 'tool_calls:' + '\\n' }}\n",
    "# {{- 'tool_name: {\\\"arg1\\\": \\\"value1\\\", \\\"arg2\\\": \\\"value2\\\"}' }}\n",
    "# {{- eos_token + '\\n' }}\n",
    "# {%- endif %}\n",
    "# {%- elif message['role'] == 'user' %}\n",
    "# {{- '<|user|>' + '\\n' }}\n",
    "# {{- message['content'] + eos_token + '\\n' }}\n",
    "# {%- endif %}\n",
    "# {%- if loop.last and add_generation_prompt %}\n",
    "# {{- '<|assistant|>' + '\\n' }}\n",
    "# {%- endif %}\n",
    "# {%- endfor %}\"\"\"\n",
    "\n",
    "# open(\"template.jinja\", \"w\").write(template)\n",
    "open(\"template.jinja\", \"w\").write(tokenizer.chat_template)\n",
    "# # open(\"template.jinja\", \"w\").write(function_calling_template)\n",
    "# # open(\"template.jinja\", \"w\").write(llama_3_1_8b_instruct_chat_template)\n",
    "\n",
    "tokenizer.chat_template = open(\"template.jinja\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are an intelligent AI that controls a drone. Given a command or request from the user,\n",
      "call one of your functions to complete the request. If the request cannot be completed by your available functions, call the reject_request function.\n",
      "If the request is ambiguous or unclear, reject the request.</s>\n",
      "<|user|>\n",
      "Let's get the drone in the air, how high should it go?</s>\n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_prompt(add_generation_prompt=False, messages=[], tool_calls=True, tools=[]):\n",
    "    return tokenizer.apply_chat_template(\n",
    "        add_generation_prompt=add_generation_prompt,\n",
    "        conversation=messages,\n",
    "        return_tensors=False,\n",
    "        tokenize=False,\n",
    "        tool_calls=tool_calls,\n",
    "        tools=tools,\n",
    "    )\n",
    "\n",
    "prompt = generate_prompt(\n",
    "    add_generation_prompt=True,\n",
    "    messages=conversation[\"messages\"][0:2],\n",
    "    tools=conversation[\"tools\"],\n",
    ")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict(prompt: str, max_new_tokens: int = 256):\n",
    "#     inputs = tokenizer(prompt, return_tensors=\"np\", add_special_tokens=False)\n",
    "#     inputs = {k: Tensor(v, device=device) for k, v in inputs.items()}\n",
    "\n",
    "#     outputs = model.generate(**inputs, do_sample=True, max_new_tokens=max_new_tokens, temperature=0.0)\n",
    "\n",
    "#     decoded_output = tokenizer.batch_decode(outputs[0][inputs['input_ids'].size(1):], skip_special_tokens=True)\n",
    "#     # decoded_output = tokenizer.decode(outputs[0][inputs['input_ids'].size(1):], skip_special_tokens=True)\n",
    "#     # decoded_output = tokenizer.decode(outputs[0][inputs['input_ids'].size(1):])\n",
    "#     # assert len(decoded_output) == inputs['input_ids'].size(1), f\"{len(decoded_output)} != {inputs['input_ids'].size(1)}\"\n",
    "#     assert len(decoded_output) == max_new_tokens, f\"{len(decoded_output)} != {max_new_tokens}\"\n",
    "\n",
    "#     return decoded_output\n",
    "\n",
    "# start = datetime.now()\n",
    "\n",
    "# prediction = predict(prompt)\n",
    "\n",
    "# print(f\"Elapsed: {datetime.now()-start}\")\n",
    "\n",
    "# print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# X_test = np.array([prompt])\n",
    "# Y_test = np.array([prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate(\n",
    "#     model=model,\n",
    "#     X_test=X_test,\n",
    "#     Y_test=Y_test,\n",
    "#     num_classes=tokenizer.vocab_size,\n",
    "#     BS=1,\n",
    "#     return_predict=True,\n",
    "#     transform=lambda x: x,\n",
    "#     target_transform=lambda y: y,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from tinygrad.dtype import dtypes\n",
    "\n",
    "# llama = model\n",
    "\n",
    "# timings = {\n",
    "#     \"logits\": [],\n",
    "#     \"sample\": [],\n",
    "#     \"itemize\": [],\n",
    "#     \"append\": [],\n",
    "# }\n",
    "\n",
    "# def greedy_until(prompt:str, until, max_length, temperature):\n",
    "#     toks = tokenizer.encode(prompt)\n",
    "#     # until_toks = tokenizer.encode(until)\n",
    "#     batch_size, seq_len = 1, len(toks)\n",
    "#     dtype = dtypes.float32\n",
    "#     # start_pos = 0\n",
    "\n",
    "#     for start_pos in range(seq_len + max_length):\n",
    "#       start = datetime.now()\n",
    "\n",
    "#       logits = llama.model(Tensor([toks[start_pos:start_pos+1]], device=device, dtype=dtype), start_pos, temperature)\n",
    "#       assert logits.device == device, f\"{logits.device} != {device}\"\n",
    "#       assert logits.dtype == dtype, f\"{logits.dtype} != {dtype}\"\n",
    "#       assert logits.shape == (batch_size, tokenizer.vocab_size), f\"{logits.shape} != {(batch_size, tokenizer.vocab_size)}\"\n",
    "#       # print('logits: ', logits)\n",
    "#       # print('logits.numpy(): ', logits.numpy())\n",
    "#       # softmax = nn.softmax(logits)\n",
    "#       # softmax = logits.softmax(axis=-1)\n",
    "#       # # print('softmax: ', softmax)\n",
    "#       # # print('softmax.numpy(): ', softmax.numpy())\n",
    "#       # assert max(softmax.numpy()[0]) <= 1.0, f\"{max(softmax.numpy()[0])} > 1.0\"\n",
    "#       # assert min(softmax.numpy()[0]) >= 0.0, f\"{min(softmax.numpy()[0])} < 0.0\"\n",
    "#       # # log_softmax = nn.log(softmax)\n",
    "#       # log_softmax = softmax.log() # TODO: log_softmax(axis=-1) instead?\n",
    "#       # # print('log_softmax: ', log_softmax)\n",
    "#       # # print('log_softmax.numpy(): ', log_softmax.numpy())\n",
    "#       # assert max(log_softmax.numpy()[0]) <= 0.0, f\"{max(log_softmax.numpy()[0])} > 0.0\"\n",
    "#       # assert min(log_softmax.numpy()[0]) > -100.0, f\"{min(log_softmax.numpy()[0])} <= -100.0\"\n",
    "\n",
    "#       timings[\"logits\"].append(datetime.now()-start)\n",
    "#       start = datetime.now()\n",
    "\n",
    "#       # tok = log_softmax\n",
    "#       # tok = logits.argmax(axis=-1, keepdim=True).item()\n",
    "#       # tok = logits.argmax(axis=-1).item()\n",
    "#       tok = logits.argmax(axis=-1, keepdim=True)\n",
    "#       assert tok.device == device, f\"{tok.device} != {device}\"\n",
    "#       assert tok.dtype == dtypes.int32, f\"{tok.dtype} != {dtypes.int32}\"\n",
    "#       assert tok.shape == (batch_size, 1), f\"{tok.shape} != {(batch_size, 1)}\"\n",
    "\n",
    "#       timings[\"sample\"].append(datetime.now()-start)\n",
    "#       # start_pos = start_pos + 1\n",
    "\n",
    "#       start = datetime.now()\n",
    "#       # tok = tok.item()\n",
    "#       # tok = tok.data()\n",
    "#       tok = np.frombuffer(tok.data(), dtype=np.int32)[0]\n",
    "#       timings[\"itemize\"].append(datetime.now()-start)\n",
    "\n",
    "#       if start_pos > seq_len - 2:\n",
    "#         start = datetime.now()\n",
    "#         toks.append(tok)\n",
    "#         # toks.append(tok.data()[0])\n",
    "#         timings[\"append\"].append(datetime.now()-start)\n",
    "\n",
    "#         # if tok == tokenizer.eos_token_id:\n",
    "#         # if tok[:1] == tokenizer.eos_token_id:\n",
    "#         if tok == tokenizer.eos_token_id:\n",
    "#           # print(f\"Breaking since token is eos_token_id\")\n",
    "#           break\n",
    "#         #   # print(f\"Breaking since token is eos_token_id\")\n",
    "#         #   break\n",
    "\n",
    "#       # output = tokenizer.decode(toks)\n",
    "\n",
    "#       # print(f\"Elapsed {i}: {datetime.now()-start}\")\n",
    "\n",
    "#       # for s in until:\n",
    "#       #   if output.endswith(s):\n",
    "#       #     print(f'Returning since output ends with {s}')\n",
    "#       #     return output[0:-len(s)]\n",
    "\n",
    "#     output = tokenizer.decode(toks)\n",
    "\n",
    "#     return output\n",
    "\n",
    "# output = greedy_until(prompt, [\"</s>\"], 256, 0.0)\n",
    "# print(output)\n",
    "\n",
    "# for k, v in timings.items():\n",
    "#     print(f\"{k}: {np.mean(v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from tinygrad.dtype import dtypes\n",
    "\n",
    "# llama = model\n",
    "\n",
    "# timings = {\n",
    "#     \"logits\": [],\n",
    "#     \"sample\": [],\n",
    "#     \"itemize\": [],\n",
    "#     \"append\": [],\n",
    "# }\n",
    "\n",
    "# def greedy_until(prompts: list[str], until, max_length, temperature):\n",
    "#     # toks = tokenizer.encode(prompt)\n",
    "#     toks = np.array([tokenizer.encode(prompt) for prompt in prompts])\n",
    "#     # until_toks = tokenizer.encode(until)\n",
    "#     # batch_size, seq_len = 1, len(toks)\n",
    "#     batch_size, seq_len = toks.shape\n",
    "#     dtype = dtypes.float32\n",
    "#     # start_pos = 0\n",
    "\n",
    "#     # padded_toks = np.concatenate((toks, np.zeros((batch_size, max_length), dtype=np.int32)), axis=1)\n",
    "\n",
    "#     for start_pos in range(seq_len + max_length):\n",
    "#       start = datetime.now()\n",
    "\n",
    "#     #   logits = llama.model(Tensor([toks[start_pos:start_pos+1]], device=device, dtype=dtype), start_pos, temperature)\n",
    "#       logits = llama.model(Tensor(toks[:, start_pos:start_pos+1], device=device, dtype=dtype), start_pos, temperature)\n",
    "#     #   logits = llama.model(Tensor(padded_toks[:, start_pos:start_pos+1], device=device, dtype=dtype), start_pos, temperature)\n",
    "#       assert logits.device == device, f\"{logits.device} != {device}\"\n",
    "#       assert logits.dtype == dtype, f\"{logits.dtype} != {dtype}\"\n",
    "#       assert logits.shape == (batch_size, tokenizer.vocab_size), f\"{logits.shape} != {(batch_size, tokenizer.vocab_size)}\"\n",
    "#       # print('logits: ', logits)\n",
    "#       # print('logits.numpy(): ', logits.numpy())\n",
    "#       # softmax = nn.softmax(logits)\n",
    "#       # softmax = logits.softmax(axis=-1)\n",
    "#       # # print('softmax: ', softmax)\n",
    "#       # # print('softmax.numpy(): ', softmax.numpy())\n",
    "#       # assert max(softmax.numpy()[0]) <= 1.0, f\"{max(softmax.numpy()[0])} > 1.0\"\n",
    "#       # assert min(softmax.numpy()[0]) >= 0.0, f\"{min(softmax.numpy()[0])} < 0.0\"\n",
    "#       # # log_softmax = nn.log(softmax)\n",
    "#       # log_softmax = softmax.log() # TODO: log_softmax(axis=-1) instead?\n",
    "#       # # print('log_softmax: ', log_softmax)\n",
    "#       # # print('log_softmax.numpy(): ', log_softmax.numpy())\n",
    "#       # assert max(log_softmax.numpy()[0]) <= 0.0, f\"{max(log_softmax.numpy()[0])} > 0.0\"\n",
    "#       # assert min(log_softmax.numpy()[0]) > -100.0, f\"{min(log_softmax.numpy()[0])} <= -100.0\"\n",
    "\n",
    "#       timings[\"logits\"].append(datetime.now()-start)\n",
    "#       start = datetime.now()\n",
    "\n",
    "#       # tok = log_softmax\n",
    "#       # tok = logits.argmax(axis=-1, keepdim=True).item()\n",
    "#       # tok = logits.argmax(axis=-1).item()\n",
    "#       tok = logits.argmax(axis=-1, keepdim=True)\n",
    "#       assert tok.device == device, f\"{tok.device} != {device}\"\n",
    "#       assert tok.dtype == dtypes.int32, f\"{tok.dtype} != {dtypes.int32}\"\n",
    "#       assert tok.shape == (batch_size, 1), f\"{tok.shape} != {(batch_size, 1)}\"\n",
    "#     #   assert tok.shape == (batch_size,), f\"{tok.shape} != {(batch_size,)}\"\n",
    "\n",
    "#       timings[\"sample\"].append(datetime.now()-start)\n",
    "#       # start_pos = start_pos + 1\n",
    "\n",
    "#       start = datetime.now()\n",
    "#       # tok = tok.item()\n",
    "#       # tok = tok.data()\n",
    "#     #   tok = np.frombuffer(tok.data(), dtype=np.int32)[0]\n",
    "#     #   tok = np.frombuffer(tok.data(), dtype=np.int32)\n",
    "#       tok = tok.numpy()\n",
    "#       assert tok.shape == (batch_size, 1), f\"{tok.shape} != {(batch_size, 1)}\"\n",
    "#     #   assert tok.shape == (batch_size,), f\"{tok.shape} != {(batch_size,)}\"\n",
    "#       timings[\"itemize\"].append(datetime.now()-start)\n",
    "\n",
    "#       if start_pos > seq_len - 2:\n",
    "#         start = datetime.now()\n",
    "#         # toks.append(tok)\n",
    "#         # toks = np.append(toks, tok, axis=0)\n",
    "#         toks = np.column_stack((toks, tok))\n",
    "#         # toks.append(tok.data()[0])\n",
    "#         timings[\"append\"].append(datetime.now()-start)\n",
    "\n",
    "#         # if tok == tokenizer.eos_token_id:\n",
    "#         # if tok[:1] == tokenizer.eos_token_id:\n",
    "#         # if tok == tokenizer.eos_token_id:\n",
    "#         if np.all(tok == tokenizer.eos_token_id):\n",
    "#           # print(f\"Breaking since token is eos_token_id\")\n",
    "#           break\n",
    "#         #   # print(f\"Breaking since token is eos_token_id\")\n",
    "#         #   break\n",
    "\n",
    "#       # output = tokenizer.decode(toks)\n",
    "\n",
    "#       # print(f\"Elapsed {i}: {datetime.now()-start}\")\n",
    "\n",
    "#       # for s in until:\n",
    "#       #   if output.endswith(s):\n",
    "#       #     print(f'Returning since output ends with {s}')\n",
    "#       #     return output[0:-len(s)]\n",
    "\n",
    "#     # output = tokenizer.decode(toks)\n",
    "#     output = [tokenizer.decode(toks[i]) for i in range(batch_size)]\n",
    "\n",
    "#     return output\n",
    "\n",
    "# output = greedy_until([prompt], [\"</s>\"], 256, 0.0)\n",
    "# print(output)\n",
    "\n",
    "# for k, v in timings.items():\n",
    "#     print(f\"{k}: {np.mean(v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: 0:00:00.206311\n",
      "sample: 0:00:00.001985\n",
      "itemize: 0:00:00.000001\n",
      "append: 0:00:00.000224\n",
      "[\"<s> <|system|>\\nYou are an intelligent AI that controls a drone. Given a command or request from the user,\\ncall one of your functions to complete the request. If the request cannot be completed by your available functions, call the reject_request function.\\nIf the request is ambiguous or unclear, reject the request.</s> \\n<|user|>\\nLet's get the drone in the air, how high should it go?</s> \\n<|assistant|>\\n\"]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tinygrad.dtype import dtypes\n",
    "\n",
    "llama = model\n",
    "\n",
    "timings = {\n",
    "    \"logits\": [],\n",
    "    \"sample\": [],\n",
    "    \"itemize\": [],\n",
    "    \"append\": [],\n",
    "}\n",
    "\n",
    "def greedy_until(prompts: list[str], until, max_length, temperature):\n",
    "    # toks = tokenizer.encode(prompt)\n",
    "    toks = np.array([tokenizer.encode(prompt) for prompt in prompts])\n",
    "    # until_toks = tokenizer.encode(until)\n",
    "    # batch_size, seq_len = 1, len(toks)\n",
    "    batch_size, seq_len = toks.shape\n",
    "    dtype = dtypes.float32\n",
    "    # start_pos = 0\n",
    "\n",
    "    # padded_toks = np.concatenate((toks, np.zeros((batch_size, max_length), dtype=np.int32)), axis=1)\n",
    "    # padded_toks = np.\n",
    "    padded_toks_tensor = Tensor(toks, device=device, dtype=dtypes.int32)\n",
    "\n",
    "    for start_pos in range(seq_len + max_length):\n",
    "    #   print('start_pos: ', start_pos)\n",
    "      start = datetime.now()\n",
    "\n",
    "    #   logits = llama.model(Tensor([toks[start_pos:start_pos+1]], device=device, dtype=dtype), start_pos, temperature)\n",
    "    #   logits = llama.model(Tensor(toks[:, start_pos:start_pos+1], device=device, dtype=dtype), start_pos, temperature)\n",
    "      new_var = padded_toks_tensor[:, start_pos:start_pos+1].contiguous()\n",
    "      assert new_var.shape == (batch_size, 1), f\"{new_var.shape} != {(batch_size, 1)}\"\n",
    "    #   print('new_var: ', new_var)\n",
    "      logits = llama.model(new_var, start_pos, temperature)\n",
    "    #   logits = llama.model(Tensor(padded_toks[:, start_pos:start_pos+1], device=device, dtype=dtype), start_pos, temperature)\n",
    "      assert logits.device == device, f\"{logits.device} != {device}\"\n",
    "      assert logits.dtype == dtype, f\"{logits.dtype} != {dtype}\"\n",
    "      assert logits.shape == (batch_size, tokenizer.vocab_size), f\"{logits.shape} != {(batch_size, tokenizer.vocab_size)}\"\n",
    "      # print('logits: ', logits)\n",
    "      # print('logits.numpy(): ', logits.numpy())\n",
    "      # softmax = nn.softmax(logits)\n",
    "      # softmax = logits.softmax(axis=-1)\n",
    "      # # print('softmax: ', softmax)\n",
    "      # # print('softmax.numpy(): ', softmax.numpy())\n",
    "      # assert max(softmax.numpy()[0]) <= 1.0, f\"{max(softmax.numpy()[0])} > 1.0\"\n",
    "      # assert min(softmax.numpy()[0]) >= 0.0, f\"{min(softmax.numpy()[0])} < 0.0\"\n",
    "      # # log_softmax = nn.log(softmax)\n",
    "      # log_softmax = softmax.log() # TODO: log_softmax(axis=-1) instead?\n",
    "      # # print('log_softmax: ', log_softmax)\n",
    "      # # print('log_softmax.numpy(): ', log_softmax.numpy())\n",
    "      # assert max(log_softmax.numpy()[0]) <= 0.0, f\"{max(log_softmax.numpy()[0])} > 0.0\"\n",
    "      # assert min(log_softmax.numpy()[0]) > -100.0, f\"{min(log_softmax.numpy()[0])} <= -100.0\"\n",
    "\n",
    "      timings[\"logits\"].append(datetime.now()-start)\n",
    "      start = datetime.now()\n",
    "\n",
    "      # tok = log_softmax\n",
    "      # tok = logits.argmax(axis=-1, keepdim=True).item()\n",
    "      # tok = logits.argmax(axis=-1).item()\n",
    "      tok = logits.argmax(axis=-1, keepdim=True)\n",
    "      assert tok.device == device, f\"{tok.device} != {device}\"\n",
    "      assert tok.dtype == dtypes.int32, f\"{tok.dtype} != {dtypes.int32}\"\n",
    "      assert tok.shape == (batch_size, 1), f\"{tok.shape} != {(batch_size, 1)}\"\n",
    "    #   assert tok.shape == (batch_size,), f\"{tok.shape} != {(batch_size,)}\"\n",
    "\n",
    "      timings[\"sample\"].append(datetime.now()-start)\n",
    "      # start_pos = start_pos + 1\n",
    "\n",
    "      start = datetime.now()\n",
    "      # tok = tok.item()\n",
    "      # tok = tok.data()\n",
    "    #   tok = np.frombuffer(tok.data(), dtype=np.int32)[0]\n",
    "    #   tok = np.frombuffer(tok.data(), dtype=np.int32)\n",
    "    #   tok = tok.numpy()\n",
    "    #   assert tok.shape == (batch_size, 1), f\"{tok.shape} != {(batch_size, 1)}\"\n",
    "    #   assert tok.shape == (batch_size,), f\"{tok.shape} != {(batch_size,)}\"\n",
    "      timings[\"itemize\"].append(datetime.now()-start)\n",
    "\n",
    "      if start_pos > seq_len - 2:\n",
    "        start = datetime.now()\n",
    "    #     # toks.append(tok)\n",
    "    #     # toks = np.append(toks, tok, axis=0)\n",
    "    #     toks = np.column_stack((toks, tok))\n",
    "    #     # toks.append(tok.data()[0])\n",
    "        padded_toks_tensor = padded_toks_tensor.cat(tok, dim=1)\n",
    "        timings[\"append\"].append(datetime.now()-start)\n",
    "\n",
    "    #     # if tok == tokenizer.eos_token_id:\n",
    "    #     # if tok[:1] == tokenizer.eos_token_id:\n",
    "    #     # if tok == tokenizer.eos_token_id:\n",
    "    #     if np.all(tok == tokenizer.eos_token_id):\n",
    "    #       # print(f\"Breaking since token is eos_token_id\")\n",
    "    #       break\n",
    "    #     #   # print(f\"Breaking since token is eos_token_id\")\n",
    "    #     #   break\n",
    "\n",
    "      # output = tokenizer.decode(toks)\n",
    "\n",
    "      # print(f\"Elapsed {i}: {datetime.now()-start}\")\n",
    "\n",
    "      # for s in until:\n",
    "      #   if output.endswith(s):\n",
    "      #     print(f'Returning since output ends with {s}')\n",
    "      #     return output[0:-len(s)]\n",
    "\n",
    "    # output = tokenizer.decode(toks)\n",
    "    output = [tokenizer.decode(toks[i]) for i in range(batch_size)]\n",
    "\n",
    "    return output\n",
    "\n",
    "output = greedy_until([prompt], [\"</s>\"], 256, 0.0)\n",
    "\n",
    "for k, v in timings.items():\n",
    "    print(f\"{k}: {np.mean(v)}\")\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
