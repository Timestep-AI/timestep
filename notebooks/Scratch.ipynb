{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from openai import OpenAI, Stream\n",
    "from openai.types.chat.chat_completion_chunk import ChatCompletionChunk\n",
    "from openai.types.chat.chat_completion import ChatCompletion\n",
    "from openai.types.completion import Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "    base_url=os.environ.get(\"OPENAI_BASE_URL\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 256\n",
    "temperature = 0\n",
    "user_prompt = \"The meaning to life and the universe is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"I\\'m so proud of you!\"\\nMummy smiled. \"That\\'s so pretty! I\\'m so glad you are very good at it.\"\\nJoe smiled and said, \"Yes, I can make a new place for my friend!\"\\nMommy smiled and said, \"You can do it, but be careful, it\\'s not safe to get too high up in the sky. You have to be careful when you don\\'t want to find something.\"\\nJoe nodded and said, \"I\\'m sorry, I didn\\'t mean to!\"\\nMommy smiled and said, \"It\\'s okay, but you need to be more careful next time. And you can do it again soon!\"\\nSo they both went back to their homes and had a great day.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_completion: ChatCompletion = client.chat.completions.create(\n",
    "    max_tokens=max_tokens,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You an AI assistant. Your top priority is responding to user questions with truthful answers.\"},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    "    model=\"LLaMA_CPP\",\n",
    "    temperature=temperature,\n",
    ")\n",
    "\n",
    "full_reply_content = chat_completion.choices[0].message.content\n",
    "full_reply_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"I\\'m so proud of you!\"\\nMummy smiled. \"That\\'s so pretty! I\\'m so glad you are very good at it.\"\\nJoe smiled and said, \"Yes, I can make a new place for my friend!\"\\nMommy smiled and said, \"You can do it, but be careful, it\\'s not safe to get too high up in the sky. You have to be careful when you don\\'t want to find something.\"\\nJoe nodded and said, \"I\\'m sorry, I didn\\'t mean to!\"\\nMommy smiled and said, \"It\\'s okay, but you need to be more careful next time. And you can do it again soon!\"\\nSo they both went back to their homes and had a great day.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_completion_chunk_stream: Stream[ChatCompletionChunk] = client.chat.completions.create(\n",
    "    max_tokens=max_tokens,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You an AI assistant. Your top priority is responding to user questions with truthful answers.\"},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    "    model=\"LLaMA_CPP\",\n",
    "    stream=True,\n",
    "    stream_options={\"include_usage\": True}, # retrieving token usage for stream response\n",
    "    temperature=temperature,\n",
    ")\n",
    "\n",
    "collected_chunks = []\n",
    "collected_messages = []\n",
    "\n",
    "# iterate through the stream of events\n",
    "for chunk in chat_completion_chunk_stream:\n",
    "    collected_chunks.append(chunk)  # save the event response\n",
    "    # print('chunk: ', chunk)\n",
    "    chunk_message = chunk.choices[0].delta.content  # extract the message\n",
    "    collected_messages.append(chunk_message)  # save the message\n",
    "    # print(f\"choices: {chunk.choices}\\nusage: {chunk.usage}\")\n",
    "    # print(\"****************\")\n",
    "\n",
    "# clean None in collected_messages\n",
    "collected_messages = [m for m in collected_messages if m is not None]\n",
    "full_reply_content = ''.join(collected_messages)\n",
    "full_reply_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' very special. He likes to help his friends, but he did not know how. He says he can do something else.\\nOne day, he sees a big, shiny thing in the sky. It is a little bird. The bird is so big and strong that it could talk. The bird said, \"I will help you find my friend.\"\\nThe bird smiled and said, \"Thank you, bird!\"\\nThe bird gave the bird a big hug. He was very happy. He said, \"Thank you, bird! I am proud of you!\"\\nThe bird said, \"You\\'re welcome, bird. I love your new friend.\" The bird smiled and said, \"I love my new friend too. We can be friends.\"\\nFrom that day on, the bird and the bird were best friends. They played together every day. They were very happy.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion: Completion = client.completions.create(\n",
    "    max_tokens=max_tokens,\n",
    "    model=\"LLaMA_CPP\",\n",
    "    prompt=user_prompt,\n",
    "    temperature=temperature,\n",
    ")\n",
    "\n",
    "full_reply_content = completion.choices[0].text\n",
    "full_reply_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' very special. He likes to help his friends, but he did not know how. He says he can do something else.\\nOne day, he sees a big, shiny thing in the sky. It is a little bird. The bird is so big and strong that it could talk. The bird said, \"I will help you find my friend.\"\\nThe bird smiled and said, \"Thank you, bird!\"\\nThe bird gave the bird a big hug. He was very happy. He said, \"Thank you, bird! I am proud of you!\"\\nThe bird said, \"You\\'re welcome, bird. I love your new friend.\" The bird smiled and said, \"I love my new friend too. We can be friends.\"\\nFrom that day on, the bird and the bird were best friends. They played together every day. They were very happy.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion_stream: Stream[Completion] = client.completions.create(\n",
    "    max_tokens=max_tokens,\n",
    "    model=\"LLaMA_CPP\",\n",
    "    prompt=user_prompt,\n",
    "    stream=True,\n",
    "    stream_options={\"include_usage\": True}, # retrieving token usage for stream response\n",
    "    temperature=temperature,\n",
    ")\n",
    "\n",
    "collected_chunks = []\n",
    "collected_messages = []\n",
    "\n",
    "# iterate through the stream of events\n",
    "for chunk in completion_stream:\n",
    "    collected_chunks.append(chunk)  # save the event response\n",
    "    # print('chunk: ', chunk)\n",
    "    chunk_message = chunk.choices[0].text  # extract the message\n",
    "    collected_messages.append(chunk_message)  # save the message\n",
    "    # print(f\"choices: {chunk.choices}\\nusage: {chunk.usage}\")\n",
    "    # print(\"****************\")\n",
    "\n",
    "# clean None in collected_messages\n",
    "collected_messages = [m for m in collected_messages if m is not None]\n",
    "full_reply_content = ''.join(collected_messages)\n",
    "full_reply_content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
